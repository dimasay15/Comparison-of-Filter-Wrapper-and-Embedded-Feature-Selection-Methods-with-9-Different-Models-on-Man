{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection-Information gain - mutual information In Classification Problem Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information \n",
    "\n",
    "MI Estimate mutual information for a discrete target variable.\n",
    "\n",
    "Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n",
    "\n",
    "The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n",
    "\n",
    "Inshort\n",
    "\n",
    "A quantity called mutual information measures the amount of information one can obtain from one random variable given another.\n",
    "\n",
    "The mutual information between two random variables X and Y can be stated formally as follows:\n",
    "\n",
    "<b>I(X ; Y) = H(X) – H(X | Y)<b>\n",
    "Where I(X ; Y) is the mutual information for X and Y, H(X) is the entropy for X and H(X | Y) is the conditional entropy for X given Y. The result has the units of bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Youtube Videos\n",
    "\n",
    "Entropy: https://www.youtube.com/watch?v=1IQOtJ4NI_0\n",
    "        \n",
    "Information Gain: https://www.youtube.com/watch?v=FuTRucXB9rA\n",
    "        \n",
    "Gini Impurity: https://www.youtube.com/watch?v=5aIFgrrTqOw\n",
    "\n",
    "Statistical test: https://www.youtube.com/watch?v=4-rxTA_5_xA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries buat seleksi fitur\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "#libraries buat klasifikasi\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>999.9</th>\n",
       "      <th>1000.3</th>\n",
       "      <th>1000.7</th>\n",
       "      <th>1001.1</th>\n",
       "      <th>1001.4</th>\n",
       "      <th>1001.8</th>\n",
       "      <th>1002.2</th>\n",
       "      <th>1002.6</th>\n",
       "      <th>1003</th>\n",
       "      <th>1003.4</th>\n",
       "      <th>...</th>\n",
       "      <th>2478.7</th>\n",
       "      <th>2481.1</th>\n",
       "      <th>2483.5</th>\n",
       "      <th>2485.8</th>\n",
       "      <th>2488.2</th>\n",
       "      <th>2490.6</th>\n",
       "      <th>2493</th>\n",
       "      <th>2495.4</th>\n",
       "      <th>2497.8</th>\n",
       "      <th>2500.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.471459</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>0.470934</td>\n",
       "      <td>0.470379</td>\n",
       "      <td>0.470260</td>\n",
       "      <td>0.469880</td>\n",
       "      <td>0.469497</td>\n",
       "      <td>0.469435</td>\n",
       "      <td>0.469454</td>\n",
       "      <td>0.468998</td>\n",
       "      <td>...</td>\n",
       "      <td>1.413537</td>\n",
       "      <td>1.415740</td>\n",
       "      <td>1.417568</td>\n",
       "      <td>1.419698</td>\n",
       "      <td>1.421711</td>\n",
       "      <td>1.423070</td>\n",
       "      <td>1.424394</td>\n",
       "      <td>1.426121</td>\n",
       "      <td>1.427552</td>\n",
       "      <td>1.428625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.433239</td>\n",
       "      <td>0.432622</td>\n",
       "      <td>0.432626</td>\n",
       "      <td>0.432379</td>\n",
       "      <td>0.431620</td>\n",
       "      <td>0.430710</td>\n",
       "      <td>0.430836</td>\n",
       "      <td>0.430847</td>\n",
       "      <td>0.430188</td>\n",
       "      <td>0.429470</td>\n",
       "      <td>...</td>\n",
       "      <td>1.601232</td>\n",
       "      <td>1.602877</td>\n",
       "      <td>1.604524</td>\n",
       "      <td>1.605982</td>\n",
       "      <td>1.606778</td>\n",
       "      <td>1.607837</td>\n",
       "      <td>1.608756</td>\n",
       "      <td>1.609967</td>\n",
       "      <td>1.610900</td>\n",
       "      <td>1.611099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.545045</td>\n",
       "      <td>0.544204</td>\n",
       "      <td>0.543792</td>\n",
       "      <td>0.543596</td>\n",
       "      <td>0.543338</td>\n",
       "      <td>0.542534</td>\n",
       "      <td>0.541493</td>\n",
       "      <td>0.541139</td>\n",
       "      <td>0.541308</td>\n",
       "      <td>0.540831</td>\n",
       "      <td>...</td>\n",
       "      <td>1.524657</td>\n",
       "      <td>1.525973</td>\n",
       "      <td>1.527454</td>\n",
       "      <td>1.529518</td>\n",
       "      <td>1.530097</td>\n",
       "      <td>1.530315</td>\n",
       "      <td>1.530254</td>\n",
       "      <td>1.531191</td>\n",
       "      <td>1.532366</td>\n",
       "      <td>1.533183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.545846</td>\n",
       "      <td>0.544815</td>\n",
       "      <td>0.544524</td>\n",
       "      <td>0.544631</td>\n",
       "      <td>0.544169</td>\n",
       "      <td>0.543143</td>\n",
       "      <td>0.542535</td>\n",
       "      <td>0.542080</td>\n",
       "      <td>0.541842</td>\n",
       "      <td>0.541258</td>\n",
       "      <td>...</td>\n",
       "      <td>1.421962</td>\n",
       "      <td>1.422955</td>\n",
       "      <td>1.423717</td>\n",
       "      <td>1.424639</td>\n",
       "      <td>1.425080</td>\n",
       "      <td>1.425797</td>\n",
       "      <td>1.426503</td>\n",
       "      <td>1.427164</td>\n",
       "      <td>1.427838</td>\n",
       "      <td>1.428271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.381048</td>\n",
       "      <td>0.380483</td>\n",
       "      <td>0.380541</td>\n",
       "      <td>0.380151</td>\n",
       "      <td>0.379599</td>\n",
       "      <td>0.379189</td>\n",
       "      <td>0.379009</td>\n",
       "      <td>0.378722</td>\n",
       "      <td>0.378309</td>\n",
       "      <td>0.377719</td>\n",
       "      <td>...</td>\n",
       "      <td>1.571125</td>\n",
       "      <td>1.572674</td>\n",
       "      <td>1.574303</td>\n",
       "      <td>1.576075</td>\n",
       "      <td>1.577273</td>\n",
       "      <td>1.577980</td>\n",
       "      <td>1.578561</td>\n",
       "      <td>1.579334</td>\n",
       "      <td>1.580042</td>\n",
       "      <td>1.581424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1557 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        999.9    1000.3    1000.7    1001.1    1001.4    1001.8    1002.2  \\\n",
       "16   0.471459  0.471074  0.470934  0.470379  0.470260  0.469880  0.469497   \n",
       "51   0.433239  0.432622  0.432626  0.432379  0.431620  0.430710  0.430836   \n",
       "183  0.545045  0.544204  0.543792  0.543596  0.543338  0.542534  0.541493   \n",
       "145  0.545846  0.544815  0.544524  0.544631  0.544169  0.543143  0.542535   \n",
       "40   0.381048  0.380483  0.380541  0.380151  0.379599  0.379189  0.379009   \n",
       "\n",
       "       1002.6      1003    1003.4  ...    2478.7    2481.1    2483.5  \\\n",
       "16   0.469435  0.469454  0.468998  ...  1.413537  1.415740  1.417568   \n",
       "51   0.430847  0.430188  0.429470  ...  1.601232  1.602877  1.604524   \n",
       "183  0.541139  0.541308  0.540831  ...  1.524657  1.525973  1.527454   \n",
       "145  0.542080  0.541842  0.541258  ...  1.421962  1.422955  1.423717   \n",
       "40   0.378722  0.378309  0.377719  ...  1.571125  1.572674  1.574303   \n",
       "\n",
       "       2485.8    2488.2    2490.6      2493    2495.4    2497.8    2500.2  \n",
       "16   1.419698  1.421711  1.423070  1.424394  1.426121  1.427552  1.428625  \n",
       "51   1.605982  1.606778  1.607837  1.608756  1.609967  1.610900  1.611099  \n",
       "183  1.529518  1.530097  1.530315  1.530254  1.531191  1.532366  1.533183  \n",
       "145  1.424639  1.425080  1.425797  1.426503  1.427164  1.427838  1.428271  \n",
       "40   1.576075  1.577273  1.577980  1.578561  1.579334  1.580042  1.581424  \n",
       "\n",
       "[5 rows x 1557 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dataset\n",
    "df = pd.read_csv('nirsMangga.csv')\n",
    "\n",
    "# separate dataset into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.drop(labels=['No', 'Mango Cultivars', 'Vit C (mg/100g)', 'TA (mg/100g)', 'SSC (oBrix)', 'label'], axis=1),\n",
    "    df['label'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleksi Fitur Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.51 s, sys: 2.09 ms, total: 4.51 s\n",
      "Wall time: 4.51 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.63194121, 0.63677335, 0.63393088, ..., 0.5091611 , 0.49290951,\n",
       "       0.48903907])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# determine the mutual information\n",
    "mutual_info = mutual_info_classif(x_train, y_train)\n",
    "mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['999.9', '1000.3', '1000.7', '1001.1', '1001.4', '1001.8', '1002.2',\n",
       "       '1002.6', '1003', '1003.4',\n",
       "       ...\n",
       "       '2478.7', '2481.1', '2483.5', '2485.8', '2488.2', '2490.6', '2493',\n",
       "       '2495.4', '2497.8', '2500.2'],\n",
       "      dtype='object', length=1557)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Input_Features  Mutual Information Value\n",
      "318         1139.7                  0.709039\n",
      "319         1140.2                  0.703579\n",
      "322         1141.7                  0.702486\n",
      "315         1138.2                  0.700575\n",
      "316         1138.7                  0.699476\n",
      "..             ...                       ...\n",
      "697         1367.5                  0.633102\n",
      "536         1260.4                  0.632364\n",
      "0            999.9                  0.631941\n",
      "569           1281                  0.631650\n",
      "565         1278.5                  0.631549\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create features score, features p_value, and features name\n",
    "features_mi = pd.DataFrame(mutual_info)\n",
    "features = pd.DataFrame(x_train.columns)\n",
    "features_mi = pd.concat([features, features_mi],axis=1)\n",
    "\n",
    "#reset index, supaya tidak ada nilai nan di dataframe yg dibikin\n",
    "features_mi.reset_index(drop=True, inplace=True)\n",
    "features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Assign the column name\n",
    "features_mi.columns = [\"Input_Features\", \"Mutual Information Value\"]\n",
    "\n",
    "# Print features score\n",
    "print(features_mi.nlargest(100,columns=\"Mutual Information Value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1139.7', '1140.2', '1141.7', '1138.2', '1138.7', '1139.2', '1140.7', '1141.2', '1142.2', '1132.2', '1137.7', '1131.7', '1137.2', '1359.6', '1358.9', '1142.7', '1134.2', '1360.3', '1133.7', '1361', '1131.2', '1132.7', '1272.8', '1358.2', '1133.2', '1357.5', '1356.7', '1267.2', '1275.3', '1262.3', '1276', '1272.2', '1270.9', '1143.2', '1267.8', '1136.7', '1273.4', '1266', '1270.3', '1266.6', '1265.4', '1274.7', '1269.1', '1369.6', '1370.4', '1263.5', '1271.6', '1361.7', '1368.9', '1135.7', '1276.6', '1268.5', '1269.7', '1264.7', '1261.7', '1134.7', '1277.2', '1662', '1274.1', '1264.1', '1279.7', '1262.9', '1135.2', '1659.9', '1660.9', '1130.7', '1136.2', '1632.7', '1663.1', '1368.2', '1279.1', '1654.6', '1349.7', '1651.4', '1664.1', '1631.7', '1657.8', '1652.5', '1658.8', '1000.3', '1628.6', '1653.5', '1297.7', '1277.8', '1630.6', '1280.4', '1655.6', '1350.4', '1353.9', '1259.8', '1000.7', '1665.2', '1282.9', '1656.7', '1371.1', '1367.5', '1260.4', '999.9', '1281', '1278.5']\n"
     ]
    }
   ],
   "source": [
    "print(features_mi.nlargest(100,columns=\"Mutual Information Value\").iloc[:,0].values.flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasifikasi Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buat n-fold cv\n",
    "#cross validation 10-fold\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Performa Model Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 20 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 20 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 20 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 20 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 20 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 20 fitur dan 100 trees:0.93\n",
      " \n",
      "CPU times: user 3.77 s, sys: 0 ns, total: 3.77 s\n",
      "Wall time: 9.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#tentukan metode scoring yang digunakan\n",
    "scoring_rfe = {'acc': 'accuracy',\n",
    "               'prec_micro': 'precision_micro',\n",
    "               'rec_micro': 'recall_micro'}\n",
    "\n",
    "#tentukan total fitur dan trees yang digunakan dalam proses klasifikasi ini\n",
    "n_feat = [20]\n",
    "n_trees = [100]\n",
    "\n",
    "for nfeat in n_feat:\n",
    "    for ntrees in n_trees:        \n",
    "        #ambil n fitur input hasil seleksi fitur MI\n",
    "        x_train_selected = x_train[features_mi.nlargest(30,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "        x_test_selected = x_test[features_mi.nlargest(30,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "\n",
    "        #Create a Gaussian Classifier\n",
    "        clf_rfe = RandomForestClassifier(n_estimators=ntrees)\n",
    "        \n",
    "        #Train the model using the training sets\n",
    "        clf_rfe.fit(x_train_selected, y_train)\n",
    "        y_pred_rfe=clf_rfe.predict(x_test_selected)\n",
    "        \n",
    "        #hitung score model dari data train\n",
    "        scores_rfe = cross_validate(clf_rfe, x_train_selected, y_train, scoring=scoring_rfe, cv=cv, return_train_score=True)\n",
    "        \n",
    "        print(\"Akurasi model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_acc'].mean(), 2)))\n",
    "        print(\"Akurasi model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.accuracy_score(y_test, y_pred_rfe), 2)))\n",
    "        print(\"Precision model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_prec_micro'].mean(), 2)))\n",
    "        print(\"Precision model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.precision_score(y_test, y_pred_rfe, average='micro'), 2)))\n",
    "        print(\"Recall model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_rec_micro'].mean(), 2)))\n",
    "        print(\"Recall model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.recall_score(y_test, y_pred_rfe, average='micro'), 2)))\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 20 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 20 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 20 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 20 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 20 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 20 fitur dan 150 trees:0.93\n",
      " \n",
      "CPU times: user 5.53 s, sys: 42.7 ms, total: 5.57 s\n",
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#tentukan metode scoring yang digunakan\n",
    "scoring_rfe = {'acc': 'accuracy',\n",
    "               'prec_micro': 'precision_micro',\n",
    "               'rec_micro': 'recall_micro'}\n",
    "\n",
    "#tentukan total fitur dan trees yang digunakan dalam proses klasifikasi ini\n",
    "n_feat = [20]\n",
    "n_trees = [150]\n",
    "\n",
    "for nfeat in n_feat:\n",
    "    for ntrees in n_trees:        \n",
    "        #ambil n fitur input hasil seleksi fitur MI\n",
    "        x_train_selected = x_train[features_mi.nlargest(30,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "        x_test_selected = x_test[features_mi.nlargest(30,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "\n",
    "        #Create a Gaussian Classifier\n",
    "        clf_rfe = RandomForestClassifier(n_estimators=ntrees)\n",
    "        \n",
    "        #Train the model using the training sets\n",
    "        clf_rfe.fit(x_train_selected, y_train)\n",
    "        y_pred_rfe=clf_rfe.predict(x_test_selected)\n",
    "        \n",
    "        #hitung score model dari data train\n",
    "        scores_rfe = cross_validate(clf_rfe, x_train_selected, y_train, scoring=scoring_rfe, cv=cv, return_train_score=True)\n",
    "        \n",
    "        print(\"Akurasi model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_acc'].mean(), 2)))\n",
    "        print(\"Akurasi model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.accuracy_score(y_test, y_pred_rfe), 2)))\n",
    "        print(\"Precision model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_prec_micro'].mean(), 2)))\n",
    "        print(\"Precision model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.precision_score(y_test, y_pred_rfe, average='micro'), 2)))\n",
    "        print(\"Recall model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_rec_micro'].mean(), 2)))\n",
    "        print(\"Recall model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.recall_score(y_test, y_pred_rfe, average='micro'), 2)))\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 20 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 20 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 20 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 20 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 20 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 20 fitur dan 200 trees:0.93\n",
      " \n",
      "CPU times: user 7.39 s, sys: 72.6 ms, total: 7.46 s\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#tentukan metode scoring yang digunakan\n",
    "scoring_rfe = {'acc': 'accuracy',\n",
    "               'prec_micro': 'precision_micro',\n",
    "               'rec_micro': 'recall_micro'}\n",
    "\n",
    "#tentukan total fitur dan trees yang digunakan dalam proses klasifikasi ini\n",
    "n_feat = [20]\n",
    "n_trees = [200]\n",
    "\n",
    "for nfeat in n_feat:\n",
    "    for ntrees in n_trees:        \n",
    "        #ambil n fitur input hasil seleksi fitur MI\n",
    "        x_train_selected = x_train[features_mi.nlargest(30,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "        x_test_selected = x_test[features_mi.nlargest(30,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "\n",
    "        #Create a Gaussian Classifier\n",
    "        clf_rfe = RandomForestClassifier(n_estimators=ntrees)\n",
    "        \n",
    "        #Train the model using the training sets\n",
    "        clf_rfe.fit(x_train_selected, y_train)\n",
    "        y_pred_rfe=clf_rfe.predict(x_test_selected)\n",
    "        \n",
    "        #hitung score model dari data train\n",
    "        scores_rfe = cross_validate(clf_rfe, x_train_selected, y_train, scoring=scoring_rfe, cv=cv, return_train_score=True)\n",
    "        \n",
    "        print(\"Akurasi model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_acc'].mean(), 2)))\n",
    "        print(\"Akurasi model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.accuracy_score(y_test, y_pred_rfe), 2)))\n",
    "        print(\"Precision model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_prec_micro'].mean(), 2)))\n",
    "        print(\"Precision model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.precision_score(y_test, y_pred_rfe, average='micro'), 2)))\n",
    "        print(\"Recall model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_rec_micro'].mean(), 2)))\n",
    "        print(\"Recall model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.recall_score(y_test, y_pred_rfe, average='micro'), 2)))\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasifikasi Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk train dan test set, serta cv-nya ambil dari yang Random Forest Classifier, sama aja "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Performa Model Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "akurasi model SVM data Train dengan 1 fitur: 0.71\n",
      "akurasi model SVM data Test dengan 1 fitur: 0.64\n",
      "Precision model SVM data Train dengan 1 fitur:0.71\n",
      "Precision model SVM data Test dengan 1 fitur:0.64\n",
      "Recall model SVM data Train dengan 1 fitur:0.71\n",
      "Recall model SVM data Test dengan 1 fitur:0.64\n",
      " \n",
      "Total waktu:  0.14014220237731934\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 2 fitur: 0.71\n",
      "akurasi model SVM data Test dengan 2 fitur: 0.64\n",
      "Precision model SVM data Train dengan 2 fitur:0.71\n",
      "Precision model SVM data Test dengan 2 fitur:0.64\n",
      "Recall model SVM data Train dengan 2 fitur:0.71\n",
      "Recall model SVM data Test dengan 2 fitur:0.64\n",
      " \n",
      "Total waktu:  0.1002352237701416\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 3 fitur: 0.71\n",
      "akurasi model SVM data Test dengan 3 fitur: 0.64\n",
      "Precision model SVM data Train dengan 3 fitur:0.71\n",
      "Precision model SVM data Test dengan 3 fitur:0.64\n",
      "Recall model SVM data Train dengan 3 fitur:0.71\n",
      "Recall model SVM data Test dengan 3 fitur:0.64\n",
      " \n",
      "Total waktu:  0.09466981887817383\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 4 fitur: 0.71\n",
      "akurasi model SVM data Test dengan 4 fitur: 0.64\n",
      "Precision model SVM data Train dengan 4 fitur:0.71\n",
      "Precision model SVM data Test dengan 4 fitur:0.64\n",
      "Recall model SVM data Train dengan 4 fitur:0.71\n",
      "Recall model SVM data Test dengan 4 fitur:0.64\n",
      " \n",
      "Total waktu:  0.09574627876281738\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 5 fitur: 0.71\n",
      "akurasi model SVM data Test dengan 5 fitur: 0.64\n",
      "Precision model SVM data Train dengan 5 fitur:0.71\n",
      "Precision model SVM data Test dengan 5 fitur:0.64\n",
      "Recall model SVM data Train dengan 5 fitur:0.71\n",
      "Recall model SVM data Test dengan 5 fitur:0.64\n",
      " \n",
      "Total waktu:  0.10071825981140137\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 6 fitur: 0.71\n",
      "akurasi model SVM data Test dengan 6 fitur: 0.64\n",
      "Precision model SVM data Train dengan 6 fitur:0.71\n",
      "Precision model SVM data Test dengan 6 fitur:0.64\n",
      "Recall model SVM data Train dengan 6 fitur:0.71\n",
      "Recall model SVM data Test dengan 6 fitur:0.64\n",
      " \n",
      "Total waktu:  0.11421704292297363\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 7 fitur: 0.71\n",
      "akurasi model SVM data Test dengan 7 fitur: 0.64\n",
      "Precision model SVM data Train dengan 7 fitur:0.71\n",
      "Precision model SVM data Test dengan 7 fitur:0.64\n",
      "Recall model SVM data Train dengan 7 fitur:0.71\n",
      "Recall model SVM data Test dengan 7 fitur:0.64\n",
      " \n",
      "Total waktu:  0.11496782302856445\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 8 fitur: 0.71\n",
      "akurasi model SVM data Test dengan 8 fitur: 0.64\n",
      "Precision model SVM data Train dengan 8 fitur:0.71\n",
      "Precision model SVM data Test dengan 8 fitur:0.64\n",
      "Recall model SVM data Train dengan 8 fitur:0.71\n",
      "Recall model SVM data Test dengan 8 fitur:0.64\n",
      " \n",
      "Total waktu:  0.11274170875549316\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 9 fitur: 0.71\n",
      "akurasi model SVM data Test dengan 9 fitur: 0.64\n",
      "Precision model SVM data Train dengan 9 fitur:0.71\n",
      "Precision model SVM data Test dengan 9 fitur:0.64\n",
      "Recall model SVM data Train dengan 9 fitur:0.71\n",
      "Recall model SVM data Test dengan 9 fitur:0.64\n",
      " \n",
      "Total waktu:  0.11387181282043457\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 10 fitur: 0.71\n",
      "akurasi model SVM data Test dengan 10 fitur: 0.64\n",
      "Precision model SVM data Train dengan 10 fitur:0.71\n",
      "Precision model SVM data Test dengan 10 fitur:0.64\n",
      "Recall model SVM data Train dengan 10 fitur:0.71\n",
      "Recall model SVM data Test dengan 10 fitur:0.64\n",
      " \n",
      "Total waktu:  0.1136314868927002\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 11 fitur: 0.71\n",
      "akurasi model SVM data Test dengan 11 fitur: 0.64\n",
      "Precision model SVM data Train dengan 11 fitur:0.71\n",
      "Precision model SVM data Test dengan 11 fitur:0.64\n",
      "Recall model SVM data Train dengan 11 fitur:0.71\n",
      "Recall model SVM data Test dengan 11 fitur:0.64\n",
      " \n",
      "Total waktu:  0.11650586128234863\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 12 fitur: 0.71\n",
      "akurasi model SVM data Test dengan 12 fitur: 0.64\n",
      "Precision model SVM data Train dengan 12 fitur:0.71\n",
      "Precision model SVM data Test dengan 12 fitur:0.64\n",
      "Recall model SVM data Train dengan 12 fitur:0.71\n",
      "Recall model SVM data Test dengan 12 fitur:0.64\n",
      " \n",
      "Total waktu:  0.1159203052520752\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 13 fitur: 0.71\n",
      "akurasi model SVM data Test dengan 13 fitur: 0.64\n",
      "Precision model SVM data Train dengan 13 fitur:0.71\n",
      "Precision model SVM data Test dengan 13 fitur:0.64\n",
      "Recall model SVM data Train dengan 13 fitur:0.71\n",
      "Recall model SVM data Test dengan 13 fitur:0.64\n",
      " \n",
      "Total waktu:  0.11860036849975586\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 14 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 14 fitur: 0.64\n",
      "Precision model SVM data Train dengan 14 fitur:0.72\n",
      "Precision model SVM data Test dengan 14 fitur:0.64\n",
      "Recall model SVM data Train dengan 14 fitur:0.72\n",
      "Recall model SVM data Test dengan 14 fitur:0.64\n",
      " \n",
      "Total waktu:  0.11623716354370117\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 15 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 15 fitur: 0.64\n",
      "Precision model SVM data Train dengan 15 fitur:0.73\n",
      "Precision model SVM data Test dengan 15 fitur:0.64\n",
      "Recall model SVM data Train dengan 15 fitur:0.73\n",
      "Recall model SVM data Test dengan 15 fitur:0.64\n",
      " \n",
      "Total waktu:  0.10853409767150879\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 16 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 16 fitur: 0.64\n",
      "Precision model SVM data Train dengan 16 fitur:0.73\n",
      "Precision model SVM data Test dengan 16 fitur:0.64\n",
      "Recall model SVM data Train dengan 16 fitur:0.73\n",
      "Recall model SVM data Test dengan 16 fitur:0.64\n",
      " \n",
      "Total waktu:  0.09851741790771484\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 17 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 17 fitur: 0.64\n",
      "Precision model SVM data Train dengan 17 fitur:0.73\n",
      "Precision model SVM data Test dengan 17 fitur:0.64\n",
      "Recall model SVM data Train dengan 17 fitur:0.73\n",
      "Recall model SVM data Test dengan 17 fitur:0.64\n",
      " \n",
      "Total waktu:  0.10370111465454102\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 18 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 18 fitur: 0.64\n",
      "Precision model SVM data Train dengan 18 fitur:0.73\n",
      "Precision model SVM data Test dengan 18 fitur:0.64\n",
      "Recall model SVM data Train dengan 18 fitur:0.73\n",
      "Recall model SVM data Test dengan 18 fitur:0.64\n",
      " \n",
      "Total waktu:  0.10351943969726562\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 19 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 19 fitur: 0.64\n",
      "Precision model SVM data Train dengan 19 fitur:0.73\n",
      "Precision model SVM data Test dengan 19 fitur:0.64\n",
      "Recall model SVM data Train dengan 19 fitur:0.73\n",
      "Recall model SVM data Test dengan 19 fitur:0.64\n",
      " \n",
      "Total waktu:  0.10640597343444824\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 20 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 20 fitur: 0.61\n",
      "Precision model SVM data Train dengan 20 fitur:0.73\n",
      "Precision model SVM data Test dengan 20 fitur:0.61\n",
      "Recall model SVM data Train dengan 20 fitur:0.73\n",
      "Recall model SVM data Test dengan 20 fitur:0.61\n",
      " \n",
      "Total waktu:  0.10596776008605957\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 21 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 21 fitur: 0.61\n",
      "Precision model SVM data Train dengan 21 fitur:0.73\n",
      "Precision model SVM data Test dengan 21 fitur:0.61\n",
      "Recall model SVM data Train dengan 21 fitur:0.73\n",
      "Recall model SVM data Test dengan 21 fitur:0.61\n",
      " \n",
      "Total waktu:  0.10944318771362305\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 22 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 22 fitur: 0.62\n",
      "Precision model SVM data Train dengan 22 fitur:0.73\n",
      "Precision model SVM data Test dengan 22 fitur:0.62\n",
      "Recall model SVM data Train dengan 22 fitur:0.73\n",
      "Recall model SVM data Test dengan 22 fitur:0.62\n",
      " \n",
      "Total waktu:  0.1085805892944336\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akurasi model SVM data Train dengan 23 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 23 fitur: 0.62\n",
      "Precision model SVM data Train dengan 23 fitur:0.73\n",
      "Precision model SVM data Test dengan 23 fitur:0.62\n",
      "Recall model SVM data Train dengan 23 fitur:0.73\n",
      "Recall model SVM data Test dengan 23 fitur:0.62\n",
      " \n",
      "Total waktu:  0.10444259643554688\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 24 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 24 fitur: 0.61\n",
      "Precision model SVM data Train dengan 24 fitur:0.72\n",
      "Precision model SVM data Test dengan 24 fitur:0.61\n",
      "Recall model SVM data Train dengan 24 fitur:0.72\n",
      "Recall model SVM data Test dengan 24 fitur:0.61\n",
      " \n",
      "Total waktu:  0.1006317138671875\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 25 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 25 fitur: 0.61\n",
      "Precision model SVM data Train dengan 25 fitur:0.73\n",
      "Precision model SVM data Test dengan 25 fitur:0.61\n",
      "Recall model SVM data Train dengan 25 fitur:0.73\n",
      "Recall model SVM data Test dengan 25 fitur:0.61\n",
      " \n",
      "Total waktu:  0.10882282257080078\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 26 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 26 fitur: 0.61\n",
      "Precision model SVM data Train dengan 26 fitur:0.72\n",
      "Precision model SVM data Test dengan 26 fitur:0.61\n",
      "Recall model SVM data Train dengan 26 fitur:0.72\n",
      "Recall model SVM data Test dengan 26 fitur:0.61\n",
      " \n",
      "Total waktu:  0.10080385208129883\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 27 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 27 fitur: 0.61\n",
      "Precision model SVM data Train dengan 27 fitur:0.73\n",
      "Precision model SVM data Test dengan 27 fitur:0.61\n",
      "Recall model SVM data Train dengan 27 fitur:0.73\n",
      "Recall model SVM data Test dengan 27 fitur:0.61\n",
      " \n",
      "Total waktu:  0.10174846649169922\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 28 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 28 fitur: 0.61\n",
      "Precision model SVM data Train dengan 28 fitur:0.72\n",
      "Precision model SVM data Test dengan 28 fitur:0.61\n",
      "Recall model SVM data Train dengan 28 fitur:0.72\n",
      "Recall model SVM data Test dengan 28 fitur:0.61\n",
      " \n",
      "Total waktu:  0.1007997989654541\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 29 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 29 fitur: 0.61\n",
      "Precision model SVM data Train dengan 29 fitur:0.73\n",
      "Precision model SVM data Test dengan 29 fitur:0.61\n",
      "Recall model SVM data Train dengan 29 fitur:0.73\n",
      "Recall model SVM data Test dengan 29 fitur:0.61\n",
      " \n",
      "Total waktu:  0.10290026664733887\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 30 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 30 fitur: 0.61\n",
      "Precision model SVM data Train dengan 30 fitur:0.73\n",
      "Precision model SVM data Test dengan 30 fitur:0.61\n",
      "Recall model SVM data Train dengan 30 fitur:0.73\n",
      "Recall model SVM data Test dengan 30 fitur:0.61\n",
      " \n",
      "Total waktu:  0.10736393928527832\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 31 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 31 fitur: 0.61\n",
      "Precision model SVM data Train dengan 31 fitur:0.73\n",
      "Precision model SVM data Test dengan 31 fitur:0.61\n",
      "Recall model SVM data Train dengan 31 fitur:0.73\n",
      "Recall model SVM data Test dengan 31 fitur:0.61\n",
      " \n",
      "Total waktu:  0.10836505889892578\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 32 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 32 fitur: 0.61\n",
      "Precision model SVM data Train dengan 32 fitur:0.72\n",
      "Precision model SVM data Test dengan 32 fitur:0.61\n",
      "Recall model SVM data Train dengan 32 fitur:0.72\n",
      "Recall model SVM data Test dengan 32 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11063408851623535\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 33 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 33 fitur: 0.61\n",
      "Precision model SVM data Train dengan 33 fitur:0.72\n",
      "Precision model SVM data Test dengan 33 fitur:0.61\n",
      "Recall model SVM data Train dengan 33 fitur:0.72\n",
      "Recall model SVM data Test dengan 33 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11518192291259766\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 34 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 34 fitur: 0.61\n",
      "Precision model SVM data Train dengan 34 fitur:0.72\n",
      "Precision model SVM data Test dengan 34 fitur:0.61\n",
      "Recall model SVM data Train dengan 34 fitur:0.72\n",
      "Recall model SVM data Test dengan 34 fitur:0.61\n",
      " \n",
      "Total waktu:  0.12065768241882324\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 35 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 35 fitur: 0.61\n",
      "Precision model SVM data Train dengan 35 fitur:0.72\n",
      "Precision model SVM data Test dengan 35 fitur:0.61\n",
      "Recall model SVM data Train dengan 35 fitur:0.72\n",
      "Recall model SVM data Test dengan 35 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11534929275512695\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 36 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 36 fitur: 0.61\n",
      "Precision model SVM data Train dengan 36 fitur:0.72\n",
      "Precision model SVM data Test dengan 36 fitur:0.61\n",
      "Recall model SVM data Train dengan 36 fitur:0.72\n",
      "Recall model SVM data Test dengan 36 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11454415321350098\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 37 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 37 fitur: 0.61\n",
      "Precision model SVM data Train dengan 37 fitur:0.73\n",
      "Precision model SVM data Test dengan 37 fitur:0.61\n",
      "Recall model SVM data Train dengan 37 fitur:0.73\n",
      "Recall model SVM data Test dengan 37 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11558675765991211\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 38 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 38 fitur: 0.61\n",
      "Precision model SVM data Train dengan 38 fitur:0.72\n",
      "Precision model SVM data Test dengan 38 fitur:0.61\n",
      "Recall model SVM data Train dengan 38 fitur:0.72\n",
      "Recall model SVM data Test dengan 38 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11757445335388184\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 39 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 39 fitur: 0.61\n",
      "Precision model SVM data Train dengan 39 fitur:0.72\n",
      "Precision model SVM data Test dengan 39 fitur:0.61\n",
      "Recall model SVM data Train dengan 39 fitur:0.72\n",
      "Recall model SVM data Test dengan 39 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11994695663452148\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 40 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 40 fitur: 0.61\n",
      "Precision model SVM data Train dengan 40 fitur:0.73\n",
      "Precision model SVM data Test dengan 40 fitur:0.61\n",
      "Recall model SVM data Train dengan 40 fitur:0.73\n",
      "Recall model SVM data Test dengan 40 fitur:0.61\n",
      " \n",
      "Total waktu:  0.1163625717163086\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 41 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 41 fitur: 0.61\n",
      "Precision model SVM data Train dengan 41 fitur:0.72\n",
      "Precision model SVM data Test dengan 41 fitur:0.61\n",
      "Recall model SVM data Train dengan 41 fitur:0.72\n",
      "Recall model SVM data Test dengan 41 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11869287490844727\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 42 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 42 fitur: 0.61\n",
      "Precision model SVM data Train dengan 42 fitur:0.72\n",
      "Precision model SVM data Test dengan 42 fitur:0.61\n",
      "Recall model SVM data Train dengan 42 fitur:0.72\n",
      "Recall model SVM data Test dengan 42 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11954092979431152\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 43 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 43 fitur: 0.61\n",
      "Precision model SVM data Train dengan 43 fitur:0.73\n",
      "Precision model SVM data Test dengan 43 fitur:0.61\n",
      "Recall model SVM data Train dengan 43 fitur:0.73\n",
      "Recall model SVM data Test dengan 43 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11831212043762207\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 44 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 44 fitur: 0.61\n",
      "Precision model SVM data Train dengan 44 fitur:0.72\n",
      "Precision model SVM data Test dengan 44 fitur:0.61\n",
      "Recall model SVM data Train dengan 44 fitur:0.72\n",
      "Recall model SVM data Test dengan 44 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11142182350158691\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akurasi model SVM data Train dengan 45 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 45 fitur: 0.61\n",
      "Precision model SVM data Train dengan 45 fitur:0.72\n",
      "Precision model SVM data Test dengan 45 fitur:0.61\n",
      "Recall model SVM data Train dengan 45 fitur:0.72\n",
      "Recall model SVM data Test dengan 45 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11353492736816406\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 46 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 46 fitur: 0.61\n",
      "Precision model SVM data Train dengan 46 fitur:0.72\n",
      "Precision model SVM data Test dengan 46 fitur:0.61\n",
      "Recall model SVM data Train dengan 46 fitur:0.72\n",
      "Recall model SVM data Test dengan 46 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11639857292175293\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 47 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 47 fitur: 0.61\n",
      "Precision model SVM data Train dengan 47 fitur:0.73\n",
      "Precision model SVM data Test dengan 47 fitur:0.61\n",
      "Recall model SVM data Train dengan 47 fitur:0.73\n",
      "Recall model SVM data Test dengan 47 fitur:0.61\n",
      " \n",
      "Total waktu:  0.1167142391204834\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 48 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 48 fitur: 0.61\n",
      "Precision model SVM data Train dengan 48 fitur:0.72\n",
      "Precision model SVM data Test dengan 48 fitur:0.61\n",
      "Recall model SVM data Train dengan 48 fitur:0.72\n",
      "Recall model SVM data Test dengan 48 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11165070533752441\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 49 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 49 fitur: 0.61\n",
      "Precision model SVM data Train dengan 49 fitur:0.73\n",
      "Precision model SVM data Test dengan 49 fitur:0.61\n",
      "Recall model SVM data Train dengan 49 fitur:0.73\n",
      "Recall model SVM data Test dengan 49 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11971521377563477\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 50 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 50 fitur: 0.61\n",
      "Precision model SVM data Train dengan 50 fitur:0.73\n",
      "Precision model SVM data Test dengan 50 fitur:0.61\n",
      "Recall model SVM data Train dengan 50 fitur:0.73\n",
      "Recall model SVM data Test dengan 50 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11286306381225586\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 51 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 51 fitur: 0.61\n",
      "Precision model SVM data Train dengan 51 fitur:0.73\n",
      "Precision model SVM data Test dengan 51 fitur:0.61\n",
      "Recall model SVM data Train dengan 51 fitur:0.73\n",
      "Recall model SVM data Test dengan 51 fitur:0.61\n",
      " \n",
      "Total waktu:  0.12146759033203125\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 52 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 52 fitur: 0.61\n",
      "Precision model SVM data Train dengan 52 fitur:0.73\n",
      "Precision model SVM data Test dengan 52 fitur:0.61\n",
      "Recall model SVM data Train dengan 52 fitur:0.73\n",
      "Recall model SVM data Test dengan 52 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11249303817749023\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 53 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 53 fitur: 0.61\n",
      "Precision model SVM data Train dengan 53 fitur:0.72\n",
      "Precision model SVM data Test dengan 53 fitur:0.61\n",
      "Recall model SVM data Train dengan 53 fitur:0.72\n",
      "Recall model SVM data Test dengan 53 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11115193367004395\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 54 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 54 fitur: 0.61\n",
      "Precision model SVM data Train dengan 54 fitur:0.72\n",
      "Precision model SVM data Test dengan 54 fitur:0.61\n",
      "Recall model SVM data Train dengan 54 fitur:0.72\n",
      "Recall model SVM data Test dengan 54 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11071038246154785\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 55 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 55 fitur: 0.61\n",
      "Precision model SVM data Train dengan 55 fitur:0.72\n",
      "Precision model SVM data Test dengan 55 fitur:0.61\n",
      "Recall model SVM data Train dengan 55 fitur:0.72\n",
      "Recall model SVM data Test dengan 55 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11321043968200684\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 56 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 56 fitur: 0.61\n",
      "Precision model SVM data Train dengan 56 fitur:0.72\n",
      "Precision model SVM data Test dengan 56 fitur:0.61\n",
      "Recall model SVM data Train dengan 56 fitur:0.72\n",
      "Recall model SVM data Test dengan 56 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11679458618164062\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 57 fitur: 0.72\n",
      "akurasi model SVM data Test dengan 57 fitur: 0.61\n",
      "Precision model SVM data Train dengan 57 fitur:0.72\n",
      "Precision model SVM data Test dengan 57 fitur:0.61\n",
      "Recall model SVM data Train dengan 57 fitur:0.72\n",
      "Recall model SVM data Test dengan 57 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11678957939147949\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 58 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 58 fitur: 0.61\n",
      "Precision model SVM data Train dengan 58 fitur:0.73\n",
      "Precision model SVM data Test dengan 58 fitur:0.61\n",
      "Recall model SVM data Train dengan 58 fitur:0.73\n",
      "Recall model SVM data Test dengan 58 fitur:0.61\n",
      " \n",
      "Total waktu:  0.1145625114440918\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 59 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 59 fitur: 0.61\n",
      "Precision model SVM data Train dengan 59 fitur:0.73\n",
      "Precision model SVM data Test dengan 59 fitur:0.61\n",
      "Recall model SVM data Train dengan 59 fitur:0.73\n",
      "Recall model SVM data Test dengan 59 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11666369438171387\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 60 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 60 fitur: 0.61\n",
      "Precision model SVM data Train dengan 60 fitur:0.73\n",
      "Precision model SVM data Test dengan 60 fitur:0.61\n",
      "Recall model SVM data Train dengan 60 fitur:0.73\n",
      "Recall model SVM data Test dengan 60 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11876916885375977\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 61 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 61 fitur: 0.61\n",
      "Precision model SVM data Train dengan 61 fitur:0.73\n",
      "Precision model SVM data Test dengan 61 fitur:0.61\n",
      "Recall model SVM data Train dengan 61 fitur:0.73\n",
      "Recall model SVM data Test dengan 61 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11652565002441406\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 62 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 62 fitur: 0.61\n",
      "Precision model SVM data Train dengan 62 fitur:0.73\n",
      "Precision model SVM data Test dengan 62 fitur:0.61\n",
      "Recall model SVM data Train dengan 62 fitur:0.73\n",
      "Recall model SVM data Test dengan 62 fitur:0.61\n",
      " \n",
      "Total waktu:  0.1144564151763916\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 63 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 63 fitur: 0.61\n",
      "Precision model SVM data Train dengan 63 fitur:0.73\n",
      "Precision model SVM data Test dengan 63 fitur:0.61\n",
      "Recall model SVM data Train dengan 63 fitur:0.73\n",
      "Recall model SVM data Test dengan 63 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11705636978149414\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 64 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 64 fitur: 0.61\n",
      "Precision model SVM data Train dengan 64 fitur:0.73\n",
      "Precision model SVM data Test dengan 64 fitur:0.61\n",
      "Recall model SVM data Train dengan 64 fitur:0.73\n",
      "Recall model SVM data Test dengan 64 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11519646644592285\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 65 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 65 fitur: 0.61\n",
      "Precision model SVM data Train dengan 65 fitur:0.74\n",
      "Precision model SVM data Test dengan 65 fitur:0.61\n",
      "Recall model SVM data Train dengan 65 fitur:0.74\n",
      "Recall model SVM data Test dengan 65 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11734247207641602\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 66 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 66 fitur: 0.61\n",
      "Precision model SVM data Train dengan 66 fitur:0.73\n",
      "Precision model SVM data Test dengan 66 fitur:0.61\n",
      "Recall model SVM data Train dengan 66 fitur:0.73\n",
      "Recall model SVM data Test dengan 66 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11280179023742676\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akurasi model SVM data Train dengan 67 fitur: 0.73\n",
      "akurasi model SVM data Test dengan 67 fitur: 0.61\n",
      "Precision model SVM data Train dengan 67 fitur:0.73\n",
      "Precision model SVM data Test dengan 67 fitur:0.61\n",
      "Recall model SVM data Train dengan 67 fitur:0.73\n",
      "Recall model SVM data Test dengan 67 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11548066139221191\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 68 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 68 fitur: 0.61\n",
      "Precision model SVM data Train dengan 68 fitur:0.74\n",
      "Precision model SVM data Test dengan 68 fitur:0.61\n",
      "Recall model SVM data Train dengan 68 fitur:0.74\n",
      "Recall model SVM data Test dengan 68 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11843681335449219\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 69 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 69 fitur: 0.61\n",
      "Precision model SVM data Train dengan 69 fitur:0.74\n",
      "Precision model SVM data Test dengan 69 fitur:0.61\n",
      "Recall model SVM data Train dengan 69 fitur:0.74\n",
      "Recall model SVM data Test dengan 69 fitur:0.61\n",
      " \n",
      "Total waktu:  0.12084078788757324\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 70 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 70 fitur: 0.61\n",
      "Precision model SVM data Train dengan 70 fitur:0.74\n",
      "Precision model SVM data Test dengan 70 fitur:0.61\n",
      "Recall model SVM data Train dengan 70 fitur:0.74\n",
      "Recall model SVM data Test dengan 70 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11575794219970703\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 71 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 71 fitur: 0.61\n",
      "Precision model SVM data Train dengan 71 fitur:0.74\n",
      "Precision model SVM data Test dengan 71 fitur:0.61\n",
      "Recall model SVM data Train dengan 71 fitur:0.74\n",
      "Recall model SVM data Test dengan 71 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11699914932250977\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 72 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 72 fitur: 0.61\n",
      "Precision model SVM data Train dengan 72 fitur:0.74\n",
      "Precision model SVM data Test dengan 72 fitur:0.61\n",
      "Recall model SVM data Train dengan 72 fitur:0.74\n",
      "Recall model SVM data Test dengan 72 fitur:0.61\n",
      " \n",
      "Total waktu:  0.12186169624328613\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 73 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 73 fitur: 0.61\n",
      "Precision model SVM data Train dengan 73 fitur:0.74\n",
      "Precision model SVM data Test dengan 73 fitur:0.61\n",
      "Recall model SVM data Train dengan 73 fitur:0.74\n",
      "Recall model SVM data Test dengan 73 fitur:0.61\n",
      " \n",
      "Total waktu:  0.12261629104614258\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 74 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 74 fitur: 0.61\n",
      "Precision model SVM data Train dengan 74 fitur:0.74\n",
      "Precision model SVM data Test dengan 74 fitur:0.61\n",
      "Recall model SVM data Train dengan 74 fitur:0.74\n",
      "Recall model SVM data Test dengan 74 fitur:0.61\n",
      " \n",
      "Total waktu:  0.11809444427490234\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 75 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 75 fitur: 0.62\n",
      "Precision model SVM data Train dengan 75 fitur:0.74\n",
      "Precision model SVM data Test dengan 75 fitur:0.62\n",
      "Recall model SVM data Train dengan 75 fitur:0.74\n",
      "Recall model SVM data Test dengan 75 fitur:0.62\n",
      " \n",
      "Total waktu:  0.11917829513549805\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 76 fitur: 0.75\n",
      "akurasi model SVM data Test dengan 76 fitur: 0.62\n",
      "Precision model SVM data Train dengan 76 fitur:0.75\n",
      "Precision model SVM data Test dengan 76 fitur:0.62\n",
      "Recall model SVM data Train dengan 76 fitur:0.75\n",
      "Recall model SVM data Test dengan 76 fitur:0.62\n",
      " \n",
      "Total waktu:  0.11766552925109863\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 77 fitur: 0.75\n",
      "akurasi model SVM data Test dengan 77 fitur: 0.62\n",
      "Precision model SVM data Train dengan 77 fitur:0.75\n",
      "Precision model SVM data Test dengan 77 fitur:0.62\n",
      "Recall model SVM data Train dengan 77 fitur:0.75\n",
      "Recall model SVM data Test dengan 77 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12333893775939941\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 78 fitur: 0.75\n",
      "akurasi model SVM data Test dengan 78 fitur: 0.62\n",
      "Precision model SVM data Train dengan 78 fitur:0.75\n",
      "Precision model SVM data Test dengan 78 fitur:0.62\n",
      "Recall model SVM data Train dengan 78 fitur:0.75\n",
      "Recall model SVM data Test dengan 78 fitur:0.62\n",
      " \n",
      "Total waktu:  0.11936688423156738\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 79 fitur: 0.75\n",
      "akurasi model SVM data Test dengan 79 fitur: 0.62\n",
      "Precision model SVM data Train dengan 79 fitur:0.75\n",
      "Precision model SVM data Test dengan 79 fitur:0.62\n",
      "Recall model SVM data Train dengan 79 fitur:0.75\n",
      "Recall model SVM data Test dengan 79 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12165284156799316\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 80 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 80 fitur: 0.62\n",
      "Precision model SVM data Train dengan 80 fitur:0.74\n",
      "Precision model SVM data Test dengan 80 fitur:0.62\n",
      "Recall model SVM data Train dengan 80 fitur:0.74\n",
      "Recall model SVM data Test dengan 80 fitur:0.62\n",
      " \n",
      "Total waktu:  0.1148223876953125\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 81 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 81 fitur: 0.62\n",
      "Precision model SVM data Train dengan 81 fitur:0.74\n",
      "Precision model SVM data Test dengan 81 fitur:0.62\n",
      "Recall model SVM data Train dengan 81 fitur:0.74\n",
      "Recall model SVM data Test dengan 81 fitur:0.62\n",
      " \n",
      "Total waktu:  0.11964845657348633\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 82 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 82 fitur: 0.62\n",
      "Precision model SVM data Train dengan 82 fitur:0.74\n",
      "Precision model SVM data Test dengan 82 fitur:0.62\n",
      "Recall model SVM data Train dengan 82 fitur:0.74\n",
      "Recall model SVM data Test dengan 82 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12291765213012695\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 83 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 83 fitur: 0.62\n",
      "Precision model SVM data Train dengan 83 fitur:0.74\n",
      "Precision model SVM data Test dengan 83 fitur:0.62\n",
      "Recall model SVM data Train dengan 83 fitur:0.74\n",
      "Recall model SVM data Test dengan 83 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12378048896789551\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 84 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 84 fitur: 0.62\n",
      "Precision model SVM data Train dengan 84 fitur:0.74\n",
      "Precision model SVM data Test dengan 84 fitur:0.62\n",
      "Recall model SVM data Train dengan 84 fitur:0.74\n",
      "Recall model SVM data Test dengan 84 fitur:0.62\n",
      " \n",
      "Total waktu:  0.1182253360748291\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 85 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 85 fitur: 0.62\n",
      "Precision model SVM data Train dengan 85 fitur:0.74\n",
      "Precision model SVM data Test dengan 85 fitur:0.62\n",
      "Recall model SVM data Train dengan 85 fitur:0.74\n",
      "Recall model SVM data Test dengan 85 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12177348136901855\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 86 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 86 fitur: 0.62\n",
      "Precision model SVM data Train dengan 86 fitur:0.74\n",
      "Precision model SVM data Test dengan 86 fitur:0.62\n",
      "Recall model SVM data Train dengan 86 fitur:0.74\n",
      "Recall model SVM data Test dengan 86 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12172126770019531\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 87 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 87 fitur: 0.62\n",
      "Precision model SVM data Train dengan 87 fitur:0.74\n",
      "Precision model SVM data Test dengan 87 fitur:0.62\n",
      "Recall model SVM data Train dengan 87 fitur:0.74\n",
      "Recall model SVM data Test dengan 87 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12004399299621582\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 88 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 88 fitur: 0.62\n",
      "Precision model SVM data Train dengan 88 fitur:0.74\n",
      "Precision model SVM data Test dengan 88 fitur:0.62\n",
      "Recall model SVM data Train dengan 88 fitur:0.74\n",
      "Recall model SVM data Test dengan 88 fitur:0.62\n",
      " \n",
      "Total waktu:  0.11957764625549316\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akurasi model SVM data Train dengan 89 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 89 fitur: 0.62\n",
      "Precision model SVM data Train dengan 89 fitur:0.74\n",
      "Precision model SVM data Test dengan 89 fitur:0.62\n",
      "Recall model SVM data Train dengan 89 fitur:0.74\n",
      "Recall model SVM data Test dengan 89 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12302136421203613\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 90 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 90 fitur: 0.62\n",
      "Precision model SVM data Train dengan 90 fitur:0.74\n",
      "Precision model SVM data Test dengan 90 fitur:0.62\n",
      "Recall model SVM data Train dengan 90 fitur:0.74\n",
      "Recall model SVM data Test dengan 90 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12421798706054688\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 91 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 91 fitur: 0.62\n",
      "Precision model SVM data Train dengan 91 fitur:0.74\n",
      "Precision model SVM data Test dengan 91 fitur:0.62\n",
      "Recall model SVM data Train dengan 91 fitur:0.74\n",
      "Recall model SVM data Test dengan 91 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12324070930480957\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 92 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 92 fitur: 0.62\n",
      "Precision model SVM data Train dengan 92 fitur:0.74\n",
      "Precision model SVM data Test dengan 92 fitur:0.62\n",
      "Recall model SVM data Train dengan 92 fitur:0.74\n",
      "Recall model SVM data Test dengan 92 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12258219718933105\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 93 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 93 fitur: 0.62\n",
      "Precision model SVM data Train dengan 93 fitur:0.74\n",
      "Precision model SVM data Test dengan 93 fitur:0.62\n",
      "Recall model SVM data Train dengan 93 fitur:0.74\n",
      "Recall model SVM data Test dengan 93 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12772345542907715\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 94 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 94 fitur: 0.62\n",
      "Precision model SVM data Train dengan 94 fitur:0.74\n",
      "Precision model SVM data Test dengan 94 fitur:0.62\n",
      "Recall model SVM data Train dengan 94 fitur:0.74\n",
      "Recall model SVM data Test dengan 94 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12497115135192871\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 95 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 95 fitur: 0.62\n",
      "Precision model SVM data Train dengan 95 fitur:0.74\n",
      "Precision model SVM data Test dengan 95 fitur:0.62\n",
      "Recall model SVM data Train dengan 95 fitur:0.74\n",
      "Recall model SVM data Test dengan 95 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12646961212158203\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 96 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 96 fitur: 0.62\n",
      "Precision model SVM data Train dengan 96 fitur:0.74\n",
      "Precision model SVM data Test dengan 96 fitur:0.62\n",
      "Recall model SVM data Train dengan 96 fitur:0.74\n",
      "Recall model SVM data Test dengan 96 fitur:0.62\n",
      " \n",
      "Total waktu:  0.11745405197143555\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 97 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 97 fitur: 0.62\n",
      "Precision model SVM data Train dengan 97 fitur:0.74\n",
      "Precision model SVM data Test dengan 97 fitur:0.62\n",
      "Recall model SVM data Train dengan 97 fitur:0.74\n",
      "Recall model SVM data Test dengan 97 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12218403816223145\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 98 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 98 fitur: 0.62\n",
      "Precision model SVM data Train dengan 98 fitur:0.74\n",
      "Precision model SVM data Test dengan 98 fitur:0.62\n",
      "Recall model SVM data Train dengan 98 fitur:0.74\n",
      "Recall model SVM data Test dengan 98 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12385940551757812\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 99 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 99 fitur: 0.62\n",
      "Precision model SVM data Train dengan 99 fitur:0.74\n",
      "Precision model SVM data Test dengan 99 fitur:0.62\n",
      "Recall model SVM data Train dengan 99 fitur:0.74\n",
      "Recall model SVM data Test dengan 99 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12975549697875977\n",
      "==================================================\n",
      "akurasi model SVM data Train dengan 100 fitur: 0.74\n",
      "akurasi model SVM data Test dengan 100 fitur: 0.62\n",
      "Precision model SVM data Train dengan 100 fitur:0.74\n",
      "Precision model SVM data Test dengan 100 fitur:0.62\n",
      "Recall model SVM data Train dengan 100 fitur:0.74\n",
      "Recall model SVM data Test dengan 100 fitur:0.62\n",
      " \n",
      "Total waktu:  0.12384176254272461\n",
      "====Nilai Performa Tertinggi====\n",
      "Nilai akurasi model tertinggi: 0.62\n",
      "Nilai presisi model tertinggi: 0.62\n",
      "Nilai Recall model tertinggi: 0.62\n",
      "CPU times: user 11.6 s, sys: 382 ms, total: 11.9 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#tentukan metode scoring yang digunakan\n",
    "scoring_svm = {'acc': 'accuracy',\n",
    "               'prec_micro': 'precision_micro',\n",
    "               'rec_micro': 'recall_micro'}\n",
    "\n",
    "#tentukan total fitur yang digunakan dalam proses klasifikasi SVM ini\n",
    "n_feat = range(1, 101)\n",
    "\n",
    "for nfeat in n_feat:\n",
    "\n",
    "        print(\"==================================================\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #ambil n fitur input hasil seleksi fitur MI\n",
    "        x_train_selected = x_train[features_mi.nlargest(100,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "        x_test_selected = x_test[features_mi.nlargest(100,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "\n",
    "\n",
    "        #Create a Support Vector Classifier\n",
    "        clf_svm = svm.SVC()\n",
    "\n",
    "        #Train the model using the training sets\n",
    "        clf_svm.fit(x_train_selected, y_train)\n",
    "        y_pred_svm = clf_svm.predict(x_test_selected)\n",
    "\n",
    "        #hitung score model dari data train\n",
    "        scores_svm = cross_validate(clf_svm, x_train_selected, y_train, scoring=scoring_svm, cv=cv, return_train_score=True)\n",
    "\n",
    "        print(\"akurasi model SVM data Train dengan \" + str(nfeat) + \" fitur: \" \n",
    "              + str(round(scores_svm['train_acc'].mean(), 2)))\n",
    "        print(\"akurasi model SVM data Test dengan \" + str(nfeat) + \" fitur: \" \n",
    "              + str(round(metrics.accuracy_score(y_test, y_pred_svm), 2)))\n",
    "        print(\"Precision model SVM data Train dengan \" + str(nfeat) + \" fitur:\"\n",
    "              + str(round(scores_svm['train_prec_micro'].mean(), 2)))\n",
    "        print(\"Precision model SVM data Test dengan \" + str(nfeat) + \" fitur:\"\n",
    "              + str(round(metrics.precision_score(y_test, y_pred_svm, average='micro'), 2)))\n",
    "        print(\"Recall model SVM data Train dengan \" + str(nfeat) + \" fitur:\"\n",
    "              + str(round(scores_svm['train_rec_micro'].mean(), 2)))\n",
    "        print(\"Recall model SVM data Test dengan \" + str(nfeat) + \" fitur:\"\n",
    "              + str(round(metrics.recall_score(y_test, y_pred_svm, average='micro'), 2)))\n",
    "        print(\" \")\n",
    "\n",
    "        \n",
    "        max_acc = 0\n",
    "        max_prec = 0\n",
    "        max_rec = 0\n",
    "        \n",
    "        if(round(metrics.accuracy_score(y_test, y_pred_svm), 2) > max_acc):\n",
    "            max_acc = round(metrics.accuracy_score(y_test, y_pred_svm), 2)\n",
    "        \n",
    "        if(round(metrics.precision_score(y_test, y_pred_svm, average='micro'), 2) > max_prec):\n",
    "            max_prec = round(metrics.precision_score(y_test, y_pred_svm, average='micro'), 2)\n",
    "            \n",
    "        if(round(metrics.recall_score(y_test, y_pred_svm, average='micro'), 2) > max_rec):\n",
    "            max_rec = round(metrics.recall_score(y_test, y_pred_svm, average='micro'), 2)\n",
    "            \n",
    "        end_time = time.time()\n",
    "        print(\"Total waktu: \", end_time - start_time)\n",
    "\n",
    "print(\"====Nilai Performa Tertinggi====\")\n",
    "print(\"Nilai akurasi model tertinggi: \" + str(max_acc))\n",
    "print(\"Nilai presisi model tertinggi: \" + str(max_prec))\n",
    "print(\"Nilai Recall model tertinggi: \" + str(max_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tes Baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aa = features_mi.sort_values(\"Mutual Information Value\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1139.7', '1140.2', '1141.7', '1138.2', '1138.7', '1139.2', '1140.7', '1141.2', '1142.2', '1132.2', '1137.7', '1131.7', '1137.2', '1359.6', '1358.9', '1142.7', '1134.2', '1360.3', '1133.7', '1361', '1131.2', '1132.7', '1272.8', '1358.2', '1133.2', '1357.5', '1356.7', '1267.2', '1275.3', '1262.3', '1276', '1272.2', '1270.9', '1143.2', '1267.8', '1136.7', '1273.4', '1270.3', '1266', '1266.6', '1265.4', '1274.7', '1269.1', '1369.6', '1370.4', '1263.5', '1271.6', '1361.7', '1368.9', '1135.7', '1276.6', '1268.5', '1269.7', '1264.7', '1261.7', '1134.7', '1277.2', '1662', '1274.1', '1264.1', '1279.7', '1262.9', '1135.2', '1659.9', '1660.9', '1130.7', '1136.2', '1632.7', '1663.1', '1368.2', '1279.1', '1654.6', '1349.7', '1651.4', '1664.1', '1631.7', '1657.8', '1652.5', '1658.8', '1000.3', '1628.6', '1653.5', '1297.7', '1277.8', '1630.6', '1280.4', '1655.6', '1350.4', '1353.9', '1259.8', '1000.7', '1665.2', '1282.9', '1656.7', '1371.1', '1367.5', '1260.4', '999.9', '1281', '1278.5', '1294.4', '1650.4', '1281.6', '1282.3', '1354.6', '1356', '1626.6', '1629.6', '1339.2', '1001.8', '1666.3', '1669.5', '1627.6', '1004.5', '1001.4', '1297', '1672.7', '1668.4', '1283.5', '1635.8', '1649.3', '1694.6', '1690.2', '1001.1', '1366.8', '1667.4', '1363.9', '1693.5', '1373.3', '1172.7', '1296.4', '1348.3', '1330.3', '1002.2', '1352.5', '1670.6', '1355.3', '1362.4', '1130.2', '1172.1', '1634.8', '1633.7', '1004.2', '1673.8', '1298.3', '1671.7', '1625.5', '1366', '1317.4', '1178', '1261.1', '1316.1', '1349', '1337.8', '1258.6', '1295.7', '1701.3', '1178.5', '1346.9', '1299', '1674.9', '1143.7', '1681.4', '1692.4', '1329.6', '1318.1', '1004.9', '1678.1', '1686.9', '1374', '1691.3', '1365.3', '1347.6', '1682.5', '1321.5', '1331', '1003.4', '1284.8', '1364.6', '1351.1', '1683.6', '1177.4', '1648.3', '1173.2', '1338.5', '1363.2', '1679.2', '1293.8', '1688', '1645.1', '1680.3', '1677.1', '1316.8', '1003.8', '1322.8', '1695.7', '1322.1', '1003', '1259.2', '1617.4', '1685.8', '1619.4', '1288.6', '1646.2', '1689.1', '1676', '1284.2', '1151.3', '1144.2', '1684.7', '1844', '1372.5', '1353.2', '1200.9', '1845.4', '1636.8', '1171.1', '1622.5', '1611.4', '1285.4', '1831', '1295.1', '1207', '1699', '1175.8', '1647.2', '1351.8', '1614.4', '1173.7', '1203.7', '1612.4', '1829.7', '1299.6', '1841.4', '1328.9', '1144.7', '1167.4', '1613.4', '1642', '1129.7', '1621.5', '1007.7', '1150.8', '1620.5', '1638.9', '1846.7', '1129.2', '1332.3', '1151.8', '1637.9', '1643.1', '1171.6', '1288', '1623.5', '1198.1', '1323.5', '1371.8', '1197.6', '1256.2', '1618.4', '1339.9', '1615.4', '1842.7', '1154.4', '1174.2', '1624.5', '1341.3', '1194.8', '1340.6', '1175.3', '1644.1', '1832.3', '1833.6', '1002.6', '1258', '1318.8', '1327.6', '1128.7', '1212.1', '1289.3', '1147.7', '1702.4', '1255.6', '1697.9', '1153.4', '1203.1', '1186.1', '1213.3', '1286.7', '1333', '1287.4', '1207.6', '1346.2', '1007.3', '1212.7', '1320.8', '1610.4', '1335.8', '1336.5', '1700.2', '1335.1', '1184.4', '1150.3', '1333.7', '1342', '1641', '1256.8', '1194.3', '1639.9', '1169.5', '1149.3', '1696.8', '1005.7', '1286.1', '1008.5', '1200.3', '1328.2', '1183.9', '1006.5', '1616.4', '1609.4', '1006.9', '1293.1', '1300.9', '1337.1', '1186.6', '1192.6', '1342.7', '1170', '1128.3', '1603.4', '1840.1', '1314.8', '1214.4', '1215', '1604.4', '1560.9', '1167.9', '1324.9', '1188.2', '1848', '1187.1', '1315.4', '1127.8', '1334.4', '1005.3', '1006.1', '1374.7', '1291.2', '1201.5', '1849.3', '1818.2', '1331.7', '1169', '1159', '1208.7', '1838.8', '1008.1', '1161.1', '1204.2', '1344.8', '1179.1', '1563.8', '1170.5', '1202.6', '1560', '1193.2', '1211', '1257.4', '1248.9', '1395.4', '1824.6', '1145.2', '1565.7', '1290.6', '1189.3', '1168.4', '1564.7', '1828.4', '1853.3', '1187.7', '1809.3', '1810.6', '1154.9', '1205.4', '1213.8', '1289.9', '1198.7', '1811.8', '1854.6', '1205.9', '1199.8', '1837.5', '1185', '1822', '1181.7', '1836.2', '1202', '1208.2', '1255', '1182.8', '1209.9', '1557.2', '1185.5', '1291.8', '1204.8', '1183.4', '1182.3', '1195.4', '1345.5', '1608.4', '1834.9', '1344.1', '1010.4', '1714.8', '1249.5', '1823.3', '1816.9', '1825.9', '1147.2', '1562.8', '1189.9', '1326.9', '1566.6', '1343.4', '1319.5', '1191', '1158.5', '1394.7', '1820.7', '1153.9', '1292.5', '1180.1', '1176.9', '1308.1', '1206.5', '1188.8', '1211.6', '1174.8', '1152.3', '1008.8', '1558.1', '1606.4', '1326.2', '1827.2', '1209.3', '1703.5', '1191.5', '1300.3', '1254.3', '1567.6', '1324.2', '1196.5', '1307.5', '1320.1', '1155.9', '1166.8', '1314.1', '1306.2', '1325.5', '1306.8', '1819.5', '1561.9', '1199.2', '1553.5', '1180.7', '1808', '1607.4', '1554.4', '1704.6', '1148.2', '1794.3', '1219', '1195.9', '1181.2', '1855.9', '1163.7', '1559.1', '1305.5', '1197', '1713.6', '1166.3', '1791.8', '1793', '1190.4', '1193.7', '1600.5', '1152.8', '1179.6', '1158', '1145.7', '1148.8', '1192.1', '1813.1', '1605.4', '1165.8', '1157.5', '1803', '1160.1', '1402.2', '1790.6', '1155.4', '1210.4', '1568.5', '1804.3', '1220.1', '1217.8', '1304.8', '1160.6', '1795.5', '1235.2', '1127.3', '1706.9', '1012.4', '1602.4', '1010.8', '1010', '1252.5', '1250.1', '1814.4', '1375.5', '1253.1', '1244.1', '1301.6', '1302.2', '1215.5', '1772.2', '1796.8', '1149.8', '1396.2', '1163.2', '1815.6', '1852', '1397.7', '1597.5', '1247.1', '1009.2', '1400', '1801.8', '1216.1', '1125.3', '1251.9', '1552.5', '1250.7', '1219.5', '1806.8', '1159.5', '1156.4', '1251.3', '1712.5', '1599.5', '1313.4', '1555.3', '1126.8', '1786.9', '1308.8', '1126.3', '1705.7', '1789.3', '1125.8', '1009.6', '1248.3', '1850.6', '1857.3', '1310.1', '1556.3', '1788.1', '1247.7', '1569.5', '1778.3', '1598.5', '1376.2', '1124.8', '1858.6', '1011.6', '1768.6', '1601.4', '1715.9', '1777.1', '1711.4', '1785.6', '1775.8', '1771', '1309.5', '1784.4', '1157', '1547', '1492.7', '1547.9', '1762.6', '1216.7', '1161.6', '1012', '1243.5', '1302.9', '1491.8', '1761.4', '1774.6', '1242.9', '1312.1', '1176.4', '1783.2', '1490.9', '1218.4', '1011.2', '1720.5', '1742.4', '1708', '1253.7', '1710.2', '1238.2', '1304.2', '1238.8', '1221.3', '1805.5', '1781.9', '1234.6', '1717', '1721.6', '1767.4', '1165.3', '1391', '1734.3', '1388.7', '1595.5', '1763.8', '1549.8', '1769.8', '1718.2', '1544.2', '1232.3', '1779.5', '1745.9', '1391.7', '1231.1', '1722.7', '1124.3', '1028', '1231.7', '1747.1', '1780.7', '1548.8', '1709.1', '1578', '1399.2', '1594.5', '1593.6', '1773.4', '1719.3', '1303.5', '1311.4', '1748.3', '1217.2', '1545.1', '1799.3', '1310.8', '1221.8', '1220.7', '1146.2', '1570.4', '1244.7', '1312.8', '1551.6', '1162.1', '1596.5', '1030.5', '1753', '1233.5', '1241.1', '1038.3', '1390.2', '1388', '1393.9', '1234', '1240.5', '1798', '1859.9', '1408.3', '1224.1', '1392.4', '1733.1', '1800.5', '1232.9', '1503', '1246.5', '1765', '1242.3', '1396.9', '1743.6', '1415.2', '1740.1', '1235.8', '1030.9', '1546.1', '1529.6', '1744.8', '1536', '1028.5', '1530.5', '1239.9', '1245.9', '1751.8', '1730.8', '1023.2', '1029.7', '1577.1', '1580.9', '1493.5', '1582.9', '1723.9', '1732', '1037.9', '1029.3', '1503.9', '1239.4', '1222.4', '1230.5', '1550.7', '1241.7', '1738.9', '1223.6', '1737.8', '1164.8', '1012.8', '1741.3', '1030.1', '1237.6', '1027.6', '1015.2', '1494.4', '1572.3', '1532.3', '1164.2', '1592.6', '1025.6', '1576.1', '1014.4', '1579', '1036.7', '1146.7', '1015.6', '1574.2', '1122.9', '1535.1', '1573.3', '1736.6', '1026', '1581.9', '1533.3', '1229.9', '1245.3', '1580', '1735.4', '1236.4', '1041.3', '1729.6', '1376.9', '1393.2', '1583.8', '1766.2', '1121.9', '1571.4', '1223', '1032.1', '1120', '1162.7', '1401.5', '1040.8', '1014', '1403', '1123.4', '1409.1', '1506.5', '1534.2', '1725', '1536.9', '1726.2', '1028.9', '1228.8', '1575.2', '1515.3', '1123.9', '1531.4', '1013.6', '1227.6', '1026.4', '1023.6', '1754.2', '1013.2', '1874.7', '1400.7', '1121.4', '1584.8', '1414.5', '1014.8', '1122.4', '1760.2', '1861.3', '1542.4', '1405.3', '1862.6', '1527.8', '1749.5', '1036.3', '1119.5', '1237', '1035.4', '1588.7', '1541.5', '1074.5', '1407.6', '1037.1', '1589.7', '1032.6', '1590.6', '1228.2', '1031.3', '1037.5', '1750.7', '1040.4', '1404.5', '1229.4', '1227', '1118.5', '1119', '1543.3', '1033', '1490.1', '1035.9', '1389.5', '1517.1', '1528.7', '1085.7', '1755.4', '1586.7', '1115.2', '1387.2', '1120.9', '1117.1', '1591.6', '1018.8', '1878.8', '1027.2', '1756.6', '1403.8', '1540.5', '1120.5', '1022.8', '1034.6', '1024.8', '1505.7', '1022', '1085.3', '1409.9', '1114.2', '1024.4', '1039.2', '1025.2', '1413.7', '1031.7', '1035', '1587.7', '1585.8', '1538.7', '1224.7', '1226.5', '1083.5', '1016', '1537.8', '1398.5', '1018.4', '1026.8', '1115.6', '1116.6', '1086.2', '1083', '1759', '1757.8', '1087.1', '1225.3', '1081.2', '1020.4', '1504.8', '1020', '1084.4', '1516.2', '1046.3', '1016.4', '1727.3', '1033.8', '1074', '1033.4', '1017.2', '1086.6', '1024', '1083.9', '1022.4', '1042.9', '1046.7', '1084.8', '1495.2', '1038.8', '1071.8', '1225.9', '1117.6', '1076.7', '2488.2', '1526.9', '1728.5', '1075.4', '1089.8', '1034.2', '1116.1', '1113.7', '1406.8', '1075.8', '1045.9', '1502.2', '1406', '1378.4', '1518', '1016.8', '1377.6', '1539.6', '1410.6', '1040', '1020.8', '1089.4', '1079.9', '1073.6', '1039.6', '1082.1', '1386.5', '1081.7', '1412.9', '1072.3', '1079.4', '1078.1', '1041.7', '1507.4', '1017.6', '1018', '1091.7', '1045', '1525.1', '1074.9', '1092.1', '1066.1', '1872', '1079', '1088.9', '1991.3', '1501.3', '1082.6', '1863.9', '1416', '1070.1', '1021.2', '1070.5', '1066.5', '1118', '1080.8', '1080.3', '1526', '1112.8', '1076.3', '1077.2', '1019.6', '1078.5', '1114.7', '1042.1', '1063.5', '1069.6', '1067', '2485.8', '1073.2', '1088.5', '1019.2', '1043.4', '1067.4', '1087.6', '1069.2', '1042.5', '1060', '1518.9', '1063.9', '1090.3', '1064.3', '1090.8', '1065.7', '1070.9', '1091.2', '1513.6', '1524.2', '1048.4', '1021.6', '1063', '1476.5', '1097.7', '1104.7', '1064.8', '1523.3', '1488.4', '1071.4', '2495.4', '1048', '1088', '1068.3', '1077.6', '1067.8', '1072.7', '2493', '2139.2', '1379.1', '1045.5', '2429.9', '1113.2', '1049.7', '1877.4', '1873.4', '1477.3', '1093.5', '1103.8', '1068.7', '1048.8', '1055.7', '1056.1', '1047.1', '2490.6', '1098.2', '1065.2', '1514.4', '1050.1', '2483.5', '1062.2', '1097.2', '1497', '1092.6', '1059.6', '1061.3', '1044.2', '1508.3', '1510.9', '1057.8', '1058.3', '1057.4', '1496.1', '2137.5', '1519.8', '1050.5', '2481.1', '1093.1', '1094.9', '1512.7', '1051', '1098.6', '1058.7', '1054', '1060.9', '1105.2', '1381.3', '1989.8', '1112.3', '1096.8', '1520.7', '1057', '2432.2', '1500.4', '1111.3', '1096.3', '1379.8', '1047.6', '1043.8', '1054.8', '1044.6', '1053.5', '1049.3', '1108.5', '1094.4', '1511.8', '1422.2', '1095.4', '1522.5', '1489.2', '1053.1', '1102.4', '1104.2', '1059.1', '1865.3', '1060.4', '1094', '1095.8', '1061.7', '1109.9', '1062.6', '1099.1', '1056.5', '1868', '1055.2', '1052.7', '2434.5', '2466.9', '1051.8', '1054.4', '1107.5', '1110.4', '1052.2', '2497.8', '1101.9', '1099.5', '2423.1', '1509.2', '2148.1', '1106.1', '2478.7', '1876.1', '2141', '1111.8', '1100.9', '1478.2', '1110.9', '1423', '2427.7', '1101.4', '1103.3', '1100', '1866.6', '1109.4', '1102.8', '2500.2', '1107.1', '1992.9', '1475.7', '2144.5', '1100.5', '1479.9', '1412.2', '1510', '1385.7', '1105.6', '1051.4', '2476.3', '1521.6', '2146.3', '2474', '1108', '1106.6', '1985.2', '1497.8', '2469.3', '1499.6', '1109', '2286.4', '1424.6', '2002.1', '1988.3', '1498.7', '2282.3', '1899.4', '1418.3', '1479', '1485', '2142.8', '1487.5', '2126.9', '2256.5', '2471.6', '2439.1', '1480.7', '2464.6', '2149.9', '2284.3', '2357', '1986.8', '1416.8', '1999', '2354.9', '1994.4', '1473.1', '1380.6', '2425.4', '1411.4', '1896.7', '2418.6', '2135.7', '2033.5', '2151.6', '1484.1', '2111.3', '2113.1', '2153.4', '1421.5', '1383.5', '1893.9', '2036.7', '1419.1', '1472.3', '1423.8', '1426.1', '2125.2', '1417.6', '2219.8', '2201', '2107.9', '1382.1', '1425.4', '2016.1', '2009.9', '2258.5', '2416.3', '2157', '1474.8', '2260.4', '1474', '2106.2', '2166', '2420.9', '2352.8', '2099.4', '2101.1', '2216', '2035.1', '2212.2', '1482.4', '1995.9', '2436.8', '2225.5', '2120', '2109.6', '2104.5', '2158.8', '2114.8', '2217.9', '2278.3', '2227.4', '1997.5', '2202.8', '2199.1', '1880.2', '2155.2', '2204.7', '2210.3', '2038.3', '2214.1', '1483.3', '1470.6', '2206.6', '2233.2', '2019.3', '1382.8', '1485.8', '2017.7', '2197.2', '2221.7', '2132.2', '2025.6', '2167.8', '2133.9', '2000.6', '2223.6', '2020.8', '1870.7', '1983.7', '2013', '2121.7', '2231.3', '2102.8', '1419.9', '2056.1', '2028.7', '2462.2', '2003.7', '1486.7', '2288.4', '2229.3', '2208.5', '2350.6', '1450.1', '2043.1', '2024', '2457.6', '1884.3', '2359.2', '1869.3', '2022.4', '2441.4', '2052.8', '2237', '2164.2', '1469', '1426.9', '1468.1', '2027.2', '2014.6', '2344.2', '2049.6', '2044.7', '1952.4', '2348.5', '2128.7', '1420.7', '1385', '1481.6', '1449.3', '2031.9', '2011.4', '2290.4', '2051.2', '2118.2', '2195.4', '2096', '2041.5', '2252.6', '2054.5', '2452.9', '1898', '2414.1', '2008.3', '2097.7', '2342.1', '2235.1', '2455.2', '1459.9', '1895.3', '2162.4', '1892.5', '2130.4', '2005.2', '1982.2', '1446', '2340', '1427.7', '1448.5', '1956.8', '2266.4', '1471.5', '2250.6', '2274.3', '2459.9', '1953.8', '2300.6', '1891.1', '1469.8', '2337.9', '1467.3', '2006.8', '1464', '1446.8', '1384.3', '1457.4', '1905', '2123.5', '2280.3', '2116.5', '2298.5', '1958.3', '2188', '1903.6', '2046.4', '1463.2', '2443.7', '2039.9', '1442', '2346.4', '2264.4', '1442.8', '1461.5', '2411.8', '2169.7', '1455.8', '2193.5', '1443.6', '1919.1', '2246.7', '2048', '1881.5', '1440.4', '1976.2', '1456.6', '2030.3', '2361.3', '1447.6', '1955.3', '1460.7', '2363.5', '2262.4', '2270.3', '1927.7', '1453.3', '1454.1', '2276.3', '1462.3', '1465.6', '1980.7', '2254.6', '1882.9', '1439.6', '1431.7', '2244.8', '1950.9', '1934.9', '1445.2', '1441.2', '2057.7', '2065.9', '2067.6', '1451.7', '1455', '2329.5', '1940.7', '2160.6', '1939.2', '2446', '2335.8', '2294.5', '1450.9', '1902.2', '1464.8', '2272.3', '1914.9', '1458.2', '1433.2', '2094.3', '2333.7', '2296.5', '2239', '1466.5', '2171.5', '2302.6', '2186.1', '1459.1', '1910.6', '2306.7', '1428.5', '2072.5', '1434', '1924.8', '1900.8', '2304.7', '2314.9', '1943.6', '1920.5', '1429.3', '1964.2', '1912', '2319.1', '1885.6', '1906.4', '2189.8', '2292.4', '2268.4', '1430.1', '2069.2', '1452.5', '1432.4', '2059.4', '2409.6', '1959.7', '2089.2', '1937.8', '1923.4', '2191.7', '2074.2', '2331.6', '2450.6', '1444.4', '1933.4', '2090.9', '1889.7', '2173.3', '1437.2', '2448.3', '2064.3', '1932', '1961.2', '2092.6', '1438.8', '2242.8', '2061', '2248.7', '1916.3', '1430.9', '2184.3', '1434.8', '1926.3', '2070.9', '2308.8', '2327.4', '1435.6', '1907.8', '2176.9', '2062.6', '1936.3', '1438', '2175.1', '1929.1', '1917.7', '1979.2', '2317', '1436.4', '1974.7', '1945', '2178.8', '1973.2', '2087.5', '2325.3', '1930.6', '1977.7', '1922', '2240.9', '1942.1', '1962.7', '2321.2', '2312.9', '2365.6', '2323.2', '2370', '2367.8', '2407.4', '1949.4', '1913.5', '2082.5', '1948', '2180.6', '1888.4', '2084.2', '1965.7', '2077.5', '1971.7', '1970.2', '1968.7', '1909.2', '2396.2', '2085.9', '1946.5', '1967.2', '2380.8', '2372.1', '2075.8', '2391.8', '2385.2', '2387.4', '2383', '2389.6', '2310.8', '2378.7', '2374.3', '2376.5', '2079.2', '2405.1', '2398.5', '2080.8', '2182.4', '2400.7', '2394', '2402.9', '1887']\n"
     ]
    }
   ],
   "source": [
    "print(list(aa['Input_Features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab = range(1, 101)\n",
    "len(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1139.7\n",
      "16   0.492063\n",
      "51   0.464010\n",
      "183  0.586312\n",
      "145  0.577506\n",
      "40   0.402640\n",
      "..        ...\n",
      "103  0.543538\n",
      "67   0.473749\n",
      "117  0.503392\n",
      "47   0.459436\n",
      "172  0.499040\n",
      "\n",
      "[130 rows x 1 columns]\n",
      "       1139.7    1140.2\n",
      "16   0.492063  0.493875\n",
      "51   0.464010  0.466825\n",
      "183  0.586312  0.589698\n",
      "145  0.577506  0.580543\n",
      "40   0.402640  0.405209\n",
      "..        ...       ...\n",
      "103  0.543538  0.546104\n",
      "67   0.473749  0.476239\n",
      "117  0.503392  0.506553\n",
      "47   0.459436  0.462290\n",
      "172  0.499040  0.501932\n",
      "\n",
      "[130 rows x 2 columns]\n",
      "       1139.7    1140.2    1141.7\n",
      "16   0.492063  0.493875  0.500545\n",
      "51   0.464010  0.466825  0.475396\n",
      "183  0.586312  0.589698  0.600224\n",
      "145  0.577506  0.580543  0.589776\n",
      "40   0.402640  0.405209  0.413083\n",
      "..        ...       ...       ...\n",
      "103  0.543538  0.546104  0.554676\n",
      "67   0.473749  0.476239  0.484654\n",
      "117  0.503392  0.506553  0.516428\n",
      "47   0.459436  0.462290  0.471228\n",
      "172  0.499040  0.501932  0.510931\n",
      "\n",
      "[130 rows x 3 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2\n",
      "16   0.492063  0.493875  0.500545  0.485824\n",
      "51   0.464010  0.466825  0.475396  0.454894\n",
      "183  0.586312  0.589698  0.600224  0.575629\n",
      "145  0.577506  0.580543  0.589776  0.568155\n",
      "40   0.402640  0.405209  0.413083  0.395286\n",
      "..        ...       ...       ...       ...\n",
      "103  0.543538  0.546104  0.554676  0.535210\n",
      "67   0.473749  0.476239  0.484654  0.465488\n",
      "117  0.503392  0.506553  0.516428  0.493610\n",
      "47   0.459436  0.462290  0.471228  0.450392\n",
      "172  0.499040  0.501932  0.510931  0.489965\n",
      "\n",
      "[130 rows x 4 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906\n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810\n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215\n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318\n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766\n",
      "..        ...       ...       ...       ...       ...\n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105\n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295\n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839\n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445\n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841\n",
      "\n",
      "[130 rows x 5 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890\n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916\n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786\n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446\n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044\n",
      "..        ...       ...       ...       ...       ...       ...\n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964\n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059\n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089\n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400\n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920\n",
      "\n",
      "[130 rows x 6 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089\n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729\n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261\n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619\n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849\n",
      "..        ...       ...       ...       ...       ...       ...       ...\n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027\n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081\n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868\n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182\n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904\n",
      "\n",
      "[130 rows x 7 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2  \n",
      "16   0.498279  \n",
      "51   0.472565  \n",
      "183  0.596755  \n",
      "145  0.586613  \n",
      "40   0.410373  \n",
      "..        ...  \n",
      "103  0.551875  \n",
      "67   0.481982  \n",
      "117  0.513116  \n",
      "47   0.468146  \n",
      "172  0.507965  \n",
      "\n",
      "[130 rows x 8 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2  \n",
      "16   0.498279  0.502916  \n",
      "51   0.472565  0.478432  \n",
      "183  0.596755  0.603650  \n",
      "145  0.586613  0.592968  \n",
      "40   0.410373  0.415739  \n",
      "..        ...       ...  \n",
      "103  0.551875  0.557337  \n",
      "67   0.481982  0.487460  \n",
      "117  0.513116  0.519892  \n",
      "47   0.468146  0.474372  \n",
      "172  0.507965  0.513851  \n",
      "\n",
      "[130 rows x 9 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  \n",
      "16   0.498279  0.502916  0.464507  \n",
      "51   0.472565  0.478432  0.426489  \n",
      "183  0.596755  0.603650  0.540704  \n",
      "145  0.586613  0.592968  0.538299  \n",
      "40   0.410373  0.415739  0.370542  \n",
      "..        ...       ...       ...  \n",
      "103  0.551875  0.557337  0.508815  \n",
      "67   0.481982  0.487460  0.439093  \n",
      "117  0.513116  0.519892  0.462708  \n",
      "47   0.468146  0.474372  0.421441  \n",
      "172  0.507965  0.513851  0.460731  \n",
      "\n",
      "[130 rows x 10 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2    1137.7  \n",
      "16   0.498279  0.502916  0.464507  0.483576  \n",
      "51   0.472565  0.478432  0.426489  0.452144  \n",
      "183  0.596755  0.603650  0.540704  0.572300  \n",
      "145  0.586613  0.592968  0.538299  0.565456  \n",
      "40   0.410373  0.415739  0.370542  0.392791  \n",
      "..        ...       ...       ...       ...  \n",
      "103  0.551875  0.557337  0.508815  0.532858  \n",
      "67   0.481982  0.487460  0.439093  0.462973  \n",
      "117  0.513116  0.519892  0.462708  0.490728  \n",
      "47   0.468146  0.474372  0.421441  0.447612  \n",
      "172  0.507965  0.513851  0.460731  0.487190  \n",
      "\n",
      "[130 rows x 11 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2    1137.7    1131.7  \n",
      "16   0.498279  0.502916  0.464507  0.483576  0.463249  \n",
      "51   0.472565  0.478432  0.426489  0.452144  0.424577  \n",
      "183  0.596755  0.603650  0.540704  0.572300  0.538428  \n",
      "145  0.586613  0.592968  0.538299  0.565456  0.536392  \n",
      "40   0.410373  0.415739  0.370542  0.392791  0.368772  \n",
      "..        ...       ...       ...       ...       ...  \n",
      "103  0.551875  0.557337  0.508815  0.532858  0.507043  \n",
      "67   0.481982  0.487460  0.439093  0.462973  0.437461  \n",
      "117  0.513116  0.519892  0.462708  0.490728  0.460551  \n",
      "47   0.468146  0.474372  0.421441  0.447612  0.419607  \n",
      "172  0.507965  0.513851  0.460731  0.487190  0.458694  \n",
      "\n",
      "[130 rows x 12 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2    1137.7    1131.7    1137.2  \n",
      "16   0.498279  0.502916  0.464507  0.483576  0.463249  0.481456  \n",
      "51   0.472565  0.478432  0.426489  0.452144  0.424577  0.449299  \n",
      "183  0.596755  0.603650  0.540704  0.572300  0.538428  0.569114  \n",
      "145  0.586613  0.592968  0.538299  0.565456  0.536392  0.562662  \n",
      "40   0.410373  0.415739  0.370542  0.392791  0.368772  0.390255  \n",
      "..        ...       ...       ...       ...       ...       ...  \n",
      "103  0.551875  0.557337  0.508815  0.532858  0.507043  0.530465  \n",
      "67   0.481982  0.487460  0.439093  0.462973  0.437461  0.460415  \n",
      "117  0.513116  0.519892  0.462708  0.490728  0.460551  0.487577  \n",
      "47   0.468146  0.474372  0.421441  0.447612  0.419607  0.444806  \n",
      "172  0.507965  0.513851  0.460731  0.487190  0.458694  0.484417  \n",
      "\n",
      "[130 rows x 13 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2    1137.7    1131.7    1137.2    1359.6  \n",
      "16   0.498279  0.502916  0.464507  0.483576  0.463249  0.481456  0.690210  \n",
      "51   0.472565  0.478432  0.426489  0.452144  0.424577  0.449299  0.753362  \n",
      "183  0.596755  0.603650  0.540704  0.572300  0.538428  0.569114  0.889088  \n",
      "145  0.586613  0.592968  0.538299  0.565456  0.536392  0.562662  0.851647  \n",
      "40   0.410373  0.415739  0.370542  0.392791  0.368772  0.390255  0.655231  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.551875  0.557337  0.508815  0.532858  0.507043  0.530465  0.800671  \n",
      "67   0.481982  0.487460  0.439093  0.462973  0.437461  0.460415  0.738887  \n",
      "117  0.513116  0.519892  0.462708  0.490728  0.460551  0.487577  0.838513  \n",
      "47   0.468146  0.474372  0.421441  0.447612  0.419607  0.444806  0.751807  \n",
      "172  0.507965  0.513851  0.460731  0.487190  0.458694  0.484417  0.770272  \n",
      "\n",
      "[130 rows x 14 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2    1137.7    1131.7    1137.2    1359.6  \\\n",
      "16   0.498279  0.502916  0.464507  0.483576  0.463249  0.481456  0.690210   \n",
      "51   0.472565  0.478432  0.426489  0.452144  0.424577  0.449299  0.753362   \n",
      "183  0.596755  0.603650  0.540704  0.572300  0.538428  0.569114  0.889088   \n",
      "145  0.586613  0.592968  0.538299  0.565456  0.536392  0.562662  0.851647   \n",
      "40   0.410373  0.415739  0.370542  0.392791  0.368772  0.390255  0.655231   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  0.532858  0.507043  0.530465  0.800671   \n",
      "67   0.481982  0.487460  0.439093  0.462973  0.437461  0.460415  0.738887   \n",
      "117  0.513116  0.519892  0.462708  0.490728  0.460551  0.487577  0.838513   \n",
      "47   0.468146  0.474372  0.421441  0.447612  0.419607  0.444806  0.751807   \n",
      "172  0.507965  0.513851  0.460731  0.487190  0.458694  0.484417  0.770272   \n",
      "\n",
      "       1358.9  \n",
      "16   0.689055  \n",
      "51   0.751373  \n",
      "183  0.887233  \n",
      "145  0.849926  \n",
      "40   0.653478  \n",
      "..        ...  \n",
      "103  0.798952  \n",
      "67   0.737085  \n",
      "117  0.836401  \n",
      "47   0.749825  \n",
      "172  0.768535  \n",
      "\n",
      "[130 rows x 15 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2    1137.7    1131.7    1137.2    1359.6  \\\n",
      "16   0.498279  0.502916  0.464507  0.483576  0.463249  0.481456  0.690210   \n",
      "51   0.472565  0.478432  0.426489  0.452144  0.424577  0.449299  0.753362   \n",
      "183  0.596755  0.603650  0.540704  0.572300  0.538428  0.569114  0.889088   \n",
      "145  0.586613  0.592968  0.538299  0.565456  0.536392  0.562662  0.851647   \n",
      "40   0.410373  0.415739  0.370542  0.392791  0.368772  0.390255  0.655231   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  0.532858  0.507043  0.530465  0.800671   \n",
      "67   0.481982  0.487460  0.439093  0.462973  0.437461  0.460415  0.738887   \n",
      "117  0.513116  0.519892  0.462708  0.490728  0.460551  0.487577  0.838513   \n",
      "47   0.468146  0.474372  0.421441  0.447612  0.419607  0.444806  0.751807   \n",
      "172  0.507965  0.513851  0.460731  0.487190  0.458694  0.484417  0.770272   \n",
      "\n",
      "       1358.9    1142.7  \n",
      "16   0.689055  0.504970  \n",
      "51   0.751373  0.481487  \n",
      "183  0.887233  0.607286  \n",
      "145  0.849926  0.595914  \n",
      "40   0.653478  0.418227  \n",
      "..        ...       ...  \n",
      "103  0.798952  0.560019  \n",
      "67   0.737085  0.490125  \n",
      "117  0.836401  0.523318  \n",
      "47   0.749825  0.477323  \n",
      "172  0.768535  0.516882  \n",
      "\n",
      "[130 rows x 16 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2    1137.7    1131.7    1137.2    1359.6  \\\n",
      "16   0.498279  0.502916  0.464507  0.483576  0.463249  0.481456  0.690210   \n",
      "51   0.472565  0.478432  0.426489  0.452144  0.424577  0.449299  0.753362   \n",
      "183  0.596755  0.603650  0.540704  0.572300  0.538428  0.569114  0.889088   \n",
      "145  0.586613  0.592968  0.538299  0.565456  0.536392  0.562662  0.851647   \n",
      "40   0.410373  0.415739  0.370542  0.392791  0.368772  0.390255  0.655231   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  0.532858  0.507043  0.530465  0.800671   \n",
      "67   0.481982  0.487460  0.439093  0.462973  0.437461  0.460415  0.738887   \n",
      "117  0.513116  0.519892  0.462708  0.490728  0.460551  0.487577  0.838513   \n",
      "47   0.468146  0.474372  0.421441  0.447612  0.419607  0.444806  0.751807   \n",
      "172  0.507965  0.513851  0.460731  0.487190  0.458694  0.484417  0.770272   \n",
      "\n",
      "       1358.9    1142.7    1134.2  \n",
      "16   0.689055  0.504970  0.470589  \n",
      "51   0.751373  0.481487  0.434599  \n",
      "183  0.887233  0.607286  0.550558  \n",
      "145  0.849926  0.595914  0.546982  \n",
      "40   0.653478  0.418227  0.377520  \n",
      "..        ...       ...       ...  \n",
      "103  0.798952  0.560019  0.516258  \n",
      "67   0.737085  0.490125  0.446635  \n",
      "117  0.836401  0.523318  0.471256  \n",
      "47   0.749825  0.477323  0.429586  \n",
      "172  0.768535  0.516882  0.468952  \n",
      "\n",
      "[130 rows x 17 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2    1137.7    1131.7    1137.2    1359.6  \\\n",
      "16   0.498279  0.502916  0.464507  0.483576  0.463249  0.481456  0.690210   \n",
      "51   0.472565  0.478432  0.426489  0.452144  0.424577  0.449299  0.753362   \n",
      "183  0.596755  0.603650  0.540704  0.572300  0.538428  0.569114  0.889088   \n",
      "145  0.586613  0.592968  0.538299  0.565456  0.536392  0.562662  0.851647   \n",
      "40   0.410373  0.415739  0.370542  0.392791  0.368772  0.390255  0.655231   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  0.532858  0.507043  0.530465  0.800671   \n",
      "67   0.481982  0.487460  0.439093  0.462973  0.437461  0.460415  0.738887   \n",
      "117  0.513116  0.519892  0.462708  0.490728  0.460551  0.487577  0.838513   \n",
      "47   0.468146  0.474372  0.421441  0.447612  0.419607  0.444806  0.751807   \n",
      "172  0.507965  0.513851  0.460731  0.487190  0.458694  0.484417  0.770272   \n",
      "\n",
      "       1358.9    1142.7    1134.2    1360.3  \n",
      "16   0.689055  0.504970  0.470589  0.691726  \n",
      "51   0.751373  0.481487  0.434599  0.755340  \n",
      "183  0.887233  0.607286  0.550558  0.891154  \n",
      "145  0.849926  0.595914  0.546982  0.853496  \n",
      "40   0.653478  0.418227  0.377520  0.657074  \n",
      "..        ...       ...       ...       ...  \n",
      "103  0.798952  0.560019  0.516258  0.802255  \n",
      "67   0.737085  0.490125  0.446635  0.740859  \n",
      "117  0.836401  0.523318  0.471256  0.840817  \n",
      "47   0.749825  0.477323  0.429586  0.753832  \n",
      "172  0.768535  0.516882  0.468952  0.772146  \n",
      "\n",
      "[130 rows x 18 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2    1137.7    1131.7    1137.2    1359.6  \\\n",
      "16   0.498279  0.502916  0.464507  0.483576  0.463249  0.481456  0.690210   \n",
      "51   0.472565  0.478432  0.426489  0.452144  0.424577  0.449299  0.753362   \n",
      "183  0.596755  0.603650  0.540704  0.572300  0.538428  0.569114  0.889088   \n",
      "145  0.586613  0.592968  0.538299  0.565456  0.536392  0.562662  0.851647   \n",
      "40   0.410373  0.415739  0.370542  0.392791  0.368772  0.390255  0.655231   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  0.532858  0.507043  0.530465  0.800671   \n",
      "67   0.481982  0.487460  0.439093  0.462973  0.437461  0.460415  0.738887   \n",
      "117  0.513116  0.519892  0.462708  0.490728  0.460551  0.487577  0.838513   \n",
      "47   0.468146  0.474372  0.421441  0.447612  0.419607  0.444806  0.751807   \n",
      "172  0.507965  0.513851  0.460731  0.487190  0.458694  0.484417  0.770272   \n",
      "\n",
      "       1358.9    1142.7    1134.2    1360.3    1133.7  \n",
      "16   0.689055  0.504970  0.470589  0.691726  0.469017  \n",
      "51   0.751373  0.481487  0.434599  0.755340  0.432378  \n",
      "183  0.887233  0.607286  0.550558  0.891154  0.548097  \n",
      "145  0.849926  0.595914  0.546982  0.853496  0.544723  \n",
      "40   0.653478  0.418227  0.377520  0.657074  0.375719  \n",
      "..        ...       ...       ...       ...       ...  \n",
      "103  0.798952  0.560019  0.516258  0.802255  0.514539  \n",
      "67   0.737085  0.490125  0.446635  0.740859  0.444700  \n",
      "117  0.836401  0.523318  0.471256  0.840817  0.468948  \n",
      "47   0.749825  0.477323  0.429586  0.753832  0.427497  \n",
      "172  0.768535  0.516882  0.468952  0.772146  0.466991  \n",
      "\n",
      "[130 rows x 19 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2    1137.7    1131.7    1137.2    1359.6  \\\n",
      "16   0.498279  0.502916  0.464507  0.483576  0.463249  0.481456  0.690210   \n",
      "51   0.472565  0.478432  0.426489  0.452144  0.424577  0.449299  0.753362   \n",
      "183  0.596755  0.603650  0.540704  0.572300  0.538428  0.569114  0.889088   \n",
      "145  0.586613  0.592968  0.538299  0.565456  0.536392  0.562662  0.851647   \n",
      "40   0.410373  0.415739  0.370542  0.392791  0.368772  0.390255  0.655231   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  0.532858  0.507043  0.530465  0.800671   \n",
      "67   0.481982  0.487460  0.439093  0.462973  0.437461  0.460415  0.738887   \n",
      "117  0.513116  0.519892  0.462708  0.490728  0.460551  0.487577  0.838513   \n",
      "47   0.468146  0.474372  0.421441  0.447612  0.419607  0.444806  0.751807   \n",
      "172  0.507965  0.513851  0.460731  0.487190  0.458694  0.484417  0.770272   \n",
      "\n",
      "       1358.9    1142.7    1134.2    1360.3    1133.7      1361  \n",
      "16   0.689055  0.504970  0.470589  0.691726  0.469017  0.692994  \n",
      "51   0.751373  0.481487  0.434599  0.755340  0.432378  0.757196  \n",
      "183  0.887233  0.607286  0.550558  0.891154  0.548097  0.892685  \n",
      "145  0.849926  0.595914  0.546982  0.853496  0.544723  0.855082  \n",
      "40   0.653478  0.418227  0.377520  0.657074  0.375719  0.658768  \n",
      "..        ...       ...       ...       ...       ...       ...  \n",
      "103  0.798952  0.560019  0.516258  0.802255  0.514539  0.803588  \n",
      "67   0.737085  0.490125  0.446635  0.740859  0.444700  0.742642  \n",
      "117  0.836401  0.523318  0.471256  0.840817  0.468948  0.843046  \n",
      "47   0.749825  0.477323  0.429586  0.753832  0.427497  0.755748  \n",
      "172  0.768535  0.516882  0.468952  0.772146  0.466991  0.773772  \n",
      "\n",
      "[130 rows x 20 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1131.7    1137.2    1359.6  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.463249  0.481456  0.690210   \n",
      "51   0.472565  0.478432  0.426489  ...  0.424577  0.449299  0.753362   \n",
      "183  0.596755  0.603650  0.540704  ...  0.538428  0.569114  0.889088   \n",
      "145  0.586613  0.592968  0.538299  ...  0.536392  0.562662  0.851647   \n",
      "40   0.410373  0.415739  0.370542  ...  0.368772  0.390255  0.655231   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.507043  0.530465  0.800671   \n",
      "67   0.481982  0.487460  0.439093  ...  0.437461  0.460415  0.738887   \n",
      "117  0.513116  0.519892  0.462708  ...  0.460551  0.487577  0.838513   \n",
      "47   0.468146  0.474372  0.421441  ...  0.419607  0.444806  0.751807   \n",
      "172  0.507965  0.513851  0.460731  ...  0.458694  0.484417  0.770272   \n",
      "\n",
      "       1358.9    1142.7    1134.2    1360.3    1133.7      1361    1131.2  \n",
      "16   0.689055  0.504970  0.470589  0.691726  0.469017  0.692994  0.462041  \n",
      "51   0.751373  0.481487  0.434599  0.755340  0.432378  0.757196  0.422574  \n",
      "183  0.887233  0.607286  0.550558  0.891154  0.548097  0.892685  0.536244  \n",
      "145  0.849926  0.595914  0.546982  0.853496  0.544723  0.855082  0.534536  \n",
      "40   0.653478  0.418227  0.377520  0.657074  0.375719  0.658768  0.367193  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.798952  0.560019  0.516258  0.802255  0.514539  0.803588  0.505490  \n",
      "67   0.737085  0.490125  0.446635  0.740859  0.444700  0.742642  0.435731  \n",
      "117  0.836401  0.523318  0.471256  0.840817  0.468948  0.843046  0.458521  \n",
      "47   0.749825  0.477323  0.429586  0.753832  0.427497  0.755748  0.417804  \n",
      "172  0.768535  0.516882  0.468952  0.772146  0.466991  0.773772  0.456907  \n",
      "\n",
      "[130 rows x 21 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1137.2    1359.6    1358.9  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.481456  0.690210  0.689055   \n",
      "51   0.472565  0.478432  0.426489  ...  0.449299  0.753362  0.751373   \n",
      "183  0.596755  0.603650  0.540704  ...  0.569114  0.889088  0.887233   \n",
      "145  0.586613  0.592968  0.538299  ...  0.562662  0.851647  0.849926   \n",
      "40   0.410373  0.415739  0.370542  ...  0.390255  0.655231  0.653478   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.530465  0.800671  0.798952   \n",
      "67   0.481982  0.487460  0.439093  ...  0.460415  0.738887  0.737085   \n",
      "117  0.513116  0.519892  0.462708  ...  0.487577  0.838513  0.836401   \n",
      "47   0.468146  0.474372  0.421441  ...  0.444806  0.751807  0.749825   \n",
      "172  0.507965  0.513851  0.460731  ...  0.484417  0.770272  0.768535   \n",
      "\n",
      "       1142.7    1134.2    1360.3    1133.7      1361    1131.2    1132.7  \n",
      "16   0.504970  0.470589  0.691726  0.469017  0.692994  0.462041  0.465845  \n",
      "51   0.481487  0.434599  0.755340  0.432378  0.757196  0.422574  0.428395  \n",
      "183  0.607286  0.550558  0.891154  0.548097  0.892685  0.536244  0.543212  \n",
      "145  0.595914  0.546982  0.853496  0.544723  0.855082  0.534536  0.540435  \n",
      "40   0.418227  0.377520  0.657074  0.375719  0.658768  0.367193  0.372102  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.560019  0.516258  0.802255  0.514539  0.803588  0.505490  0.510800  \n",
      "67   0.490125  0.446635  0.740859  0.444700  0.742642  0.435731  0.440891  \n",
      "117  0.523318  0.471256  0.840817  0.468948  0.843046  0.458521  0.464717  \n",
      "47   0.477323  0.429586  0.753832  0.427497  0.755748  0.417804  0.423373  \n",
      "172  0.516882  0.468952  0.772146  0.466991  0.773772  0.456907  0.462986  \n",
      "\n",
      "[130 rows x 22 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1359.6    1358.9    1142.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.690210  0.689055  0.504970   \n",
      "51   0.472565  0.478432  0.426489  ...  0.753362  0.751373  0.481487   \n",
      "183  0.596755  0.603650  0.540704  ...  0.889088  0.887233  0.607286   \n",
      "145  0.586613  0.592968  0.538299  ...  0.851647  0.849926  0.595914   \n",
      "40   0.410373  0.415739  0.370542  ...  0.655231  0.653478  0.418227   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.800671  0.798952  0.560019   \n",
      "67   0.481982  0.487460  0.439093  ...  0.738887  0.737085  0.490125   \n",
      "117  0.513116  0.519892  0.462708  ...  0.838513  0.836401  0.523318   \n",
      "47   0.468146  0.474372  0.421441  ...  0.751807  0.749825  0.477323   \n",
      "172  0.507965  0.513851  0.460731  ...  0.770272  0.768535  0.516882   \n",
      "\n",
      "       1134.2    1360.3    1133.7      1361    1131.2    1132.7    1272.8  \n",
      "16   0.470589  0.691726  0.469017  0.692994  0.462041  0.465845  0.550064  \n",
      "51   0.434599  0.755340  0.432378  0.757196  0.422574  0.428395  0.547437  \n",
      "183  0.550558  0.891154  0.548097  0.892685  0.536244  0.543212  0.682313  \n",
      "145  0.546982  0.853496  0.544723  0.855082  0.534536  0.540435  0.660714  \n",
      "40   0.377520  0.657074  0.375719  0.658768  0.367193  0.372102  0.474118  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.516258  0.802255  0.514539  0.803588  0.505490  0.510800  0.622584  \n",
      "67   0.446635  0.740859  0.444700  0.742642  0.435731  0.440891  0.549084  \n",
      "117  0.471256  0.840817  0.468948  0.843046  0.458521  0.464717  0.598433  \n",
      "47   0.429586  0.753832  0.427497  0.755748  0.417804  0.423373  0.542507  \n",
      "172  0.468952  0.772146  0.466991  0.773772  0.456907  0.462986  0.579796  \n",
      "\n",
      "[130 rows x 23 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1358.9    1142.7    1134.2  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.689055  0.504970  0.470589   \n",
      "51   0.472565  0.478432  0.426489  ...  0.751373  0.481487  0.434599   \n",
      "183  0.596755  0.603650  0.540704  ...  0.887233  0.607286  0.550558   \n",
      "145  0.586613  0.592968  0.538299  ...  0.849926  0.595914  0.546982   \n",
      "40   0.410373  0.415739  0.370542  ...  0.653478  0.418227  0.377520   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.798952  0.560019  0.516258   \n",
      "67   0.481982  0.487460  0.439093  ...  0.737085  0.490125  0.446635   \n",
      "117  0.513116  0.519892  0.462708  ...  0.836401  0.523318  0.471256   \n",
      "47   0.468146  0.474372  0.421441  ...  0.749825  0.477323  0.429586   \n",
      "172  0.507965  0.513851  0.460731  ...  0.768535  0.516882  0.468952   \n",
      "\n",
      "       1360.3    1133.7      1361    1131.2    1132.7    1272.8    1358.2  \n",
      "16   0.691726  0.469017  0.692994  0.462041  0.465845  0.550064  0.687939  \n",
      "51   0.755340  0.432378  0.757196  0.422574  0.428395  0.547437  0.749504  \n",
      "183  0.891154  0.548097  0.892685  0.536244  0.543212  0.682313  0.885778  \n",
      "145  0.853496  0.544723  0.855082  0.534536  0.540435  0.660714  0.848379  \n",
      "40   0.657074  0.375719  0.658768  0.367193  0.372102  0.474118  0.651788  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.802255  0.514539  0.803588  0.505490  0.510800  0.622584  0.797466  \n",
      "67   0.740859  0.444700  0.742642  0.435731  0.440891  0.549084  0.735379  \n",
      "117  0.840817  0.468948  0.843046  0.458521  0.464717  0.598433  0.834153  \n",
      "47   0.753832  0.427497  0.755748  0.417804  0.423373  0.542507  0.747888  \n",
      "172  0.772146  0.466991  0.773772  0.456907  0.462986  0.579796  0.767031  \n",
      "\n",
      "[130 rows x 24 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1142.7    1134.2    1360.3  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.504970  0.470589  0.691726   \n",
      "51   0.472565  0.478432  0.426489  ...  0.481487  0.434599  0.755340   \n",
      "183  0.596755  0.603650  0.540704  ...  0.607286  0.550558  0.891154   \n",
      "145  0.586613  0.592968  0.538299  ...  0.595914  0.546982  0.853496   \n",
      "40   0.410373  0.415739  0.370542  ...  0.418227  0.377520  0.657074   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.560019  0.516258  0.802255   \n",
      "67   0.481982  0.487460  0.439093  ...  0.490125  0.446635  0.740859   \n",
      "117  0.513116  0.519892  0.462708  ...  0.523318  0.471256  0.840817   \n",
      "47   0.468146  0.474372  0.421441  ...  0.477323  0.429586  0.753832   \n",
      "172  0.507965  0.513851  0.460731  ...  0.516882  0.468952  0.772146   \n",
      "\n",
      "       1133.7      1361    1131.2    1132.7    1272.8    1358.2    1133.2  \n",
      "16   0.469017  0.692994  0.462041  0.465845  0.550064  0.687939  0.467234  \n",
      "51   0.432378  0.757196  0.422574  0.428395  0.547437  0.749504  0.430296  \n",
      "183  0.548097  0.892685  0.536244  0.543212  0.682313  0.885778  0.545634  \n",
      "145  0.544723  0.855082  0.534536  0.540435  0.660714  0.848379  0.542453  \n",
      "40   0.375719  0.658768  0.367193  0.372102  0.474118  0.651788  0.373659  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.514539  0.803588  0.505490  0.510800  0.622584  0.797466  0.512619  \n",
      "67   0.444700  0.742642  0.435731  0.440891  0.549084  0.735379  0.442750  \n",
      "117  0.468948  0.843046  0.458521  0.464717  0.598433  0.834153  0.466721  \n",
      "47   0.427497  0.755748  0.417804  0.423373  0.542507  0.747888  0.425381  \n",
      "172  0.466991  0.773772  0.456907  0.462986  0.579796  0.767031  0.464960  \n",
      "\n",
      "[130 rows x 25 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1134.2    1360.3    1133.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.470589  0.691726  0.469017   \n",
      "51   0.472565  0.478432  0.426489  ...  0.434599  0.755340  0.432378   \n",
      "183  0.596755  0.603650  0.540704  ...  0.550558  0.891154  0.548097   \n",
      "145  0.586613  0.592968  0.538299  ...  0.546982  0.853496  0.544723   \n",
      "40   0.410373  0.415739  0.370542  ...  0.377520  0.657074  0.375719   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.516258  0.802255  0.514539   \n",
      "67   0.481982  0.487460  0.439093  ...  0.446635  0.740859  0.444700   \n",
      "117  0.513116  0.519892  0.462708  ...  0.471256  0.840817  0.468948   \n",
      "47   0.468146  0.474372  0.421441  ...  0.429586  0.753832  0.427497   \n",
      "172  0.507965  0.513851  0.460731  ...  0.468952  0.772146  0.466991   \n",
      "\n",
      "         1361    1131.2    1132.7    1272.8    1358.2    1133.2    1357.5  \n",
      "16   0.692994  0.462041  0.465845  0.550064  0.687939  0.467234  0.686376  \n",
      "51   0.757196  0.422574  0.428395  0.547437  0.749504  0.430296  0.747396  \n",
      "183  0.892685  0.536244  0.543212  0.682313  0.885778  0.545634  0.883820  \n",
      "145  0.855082  0.534536  0.540435  0.660714  0.848379  0.542453  0.846582  \n",
      "40   0.658768  0.367193  0.372102  0.474118  0.651788  0.373659  0.649921  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.803588  0.505490  0.510800  0.622584  0.797466  0.512619  0.795769  \n",
      "67   0.742642  0.435731  0.440891  0.549084  0.735379  0.442750  0.733556  \n",
      "117  0.843046  0.458521  0.464717  0.598433  0.834153  0.466721  0.831688  \n",
      "47   0.755748  0.417804  0.423373  0.542507  0.747888  0.425381  0.745809  \n",
      "172  0.773772  0.456907  0.462986  0.579796  0.767031  0.464960  0.765109  \n",
      "\n",
      "[130 rows x 26 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1360.3    1133.7      1361  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.691726  0.469017  0.692994   \n",
      "51   0.472565  0.478432  0.426489  ...  0.755340  0.432378  0.757196   \n",
      "183  0.596755  0.603650  0.540704  ...  0.891154  0.548097  0.892685   \n",
      "145  0.586613  0.592968  0.538299  ...  0.853496  0.544723  0.855082   \n",
      "40   0.410373  0.415739  0.370542  ...  0.657074  0.375719  0.658768   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.802255  0.514539  0.803588   \n",
      "67   0.481982  0.487460  0.439093  ...  0.740859  0.444700  0.742642   \n",
      "117  0.513116  0.519892  0.462708  ...  0.840817  0.468948  0.843046   \n",
      "47   0.468146  0.474372  0.421441  ...  0.753832  0.427497  0.755748   \n",
      "172  0.507965  0.513851  0.460731  ...  0.772146  0.466991  0.773772   \n",
      "\n",
      "       1131.2    1132.7    1272.8    1358.2    1133.2    1357.5    1356.7  \n",
      "16   0.462041  0.465845  0.550064  0.687939  0.467234  0.686376  0.685177  \n",
      "51   0.422574  0.428395  0.547437  0.749504  0.430296  0.747396  0.745675  \n",
      "183  0.536244  0.543212  0.682313  0.885778  0.545634  0.883820  0.882231  \n",
      "145  0.534536  0.540435  0.660714  0.848379  0.542453  0.846582  0.844985  \n",
      "40   0.367193  0.372102  0.474118  0.651788  0.373659  0.649921  0.648267  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.505490  0.510800  0.622584  0.797466  0.512619  0.795769  0.794281  \n",
      "67   0.435731  0.440891  0.549084  0.735379  0.442750  0.733556  0.731986  \n",
      "117  0.458521  0.464717  0.598433  0.834153  0.466721  0.831688  0.829669  \n",
      "47   0.417804  0.423373  0.542507  0.747888  0.425381  0.745809  0.743894  \n",
      "172  0.456907  0.462986  0.579796  0.767031  0.464960  0.765109  0.763586  \n",
      "\n",
      "[130 rows x 27 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1133.7      1361    1131.2  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.469017  0.692994  0.462041   \n",
      "51   0.472565  0.478432  0.426489  ...  0.432378  0.757196  0.422574   \n",
      "183  0.596755  0.603650  0.540704  ...  0.548097  0.892685  0.536244   \n",
      "145  0.586613  0.592968  0.538299  ...  0.544723  0.855082  0.534536   \n",
      "40   0.410373  0.415739  0.370542  ...  0.375719  0.658768  0.367193   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.514539  0.803588  0.505490   \n",
      "67   0.481982  0.487460  0.439093  ...  0.444700  0.742642  0.435731   \n",
      "117  0.513116  0.519892  0.462708  ...  0.468948  0.843046  0.458521   \n",
      "47   0.468146  0.474372  0.421441  ...  0.427497  0.755748  0.417804   \n",
      "172  0.507965  0.513851  0.460731  ...  0.466991  0.773772  0.456907   \n",
      "\n",
      "       1132.7    1272.8    1358.2    1133.2    1357.5    1356.7    1267.2  \n",
      "16   0.465845  0.550064  0.687939  0.467234  0.686376  0.685177  0.549601  \n",
      "51   0.428395  0.547437  0.749504  0.430296  0.747396  0.745675  0.546862  \n",
      "183  0.543212  0.682313  0.885778  0.545634  0.883820  0.882231  0.681063  \n",
      "145  0.540435  0.660714  0.848379  0.542453  0.846582  0.844985  0.660047  \n",
      "40   0.372102  0.474118  0.651788  0.373659  0.649921  0.648267  0.473666  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.510800  0.622584  0.797466  0.512619  0.795769  0.794281  0.621668  \n",
      "67   0.440891  0.549084  0.735379  0.442750  0.733556  0.731986  0.548640  \n",
      "117  0.464717  0.598433  0.834153  0.466721  0.831688  0.829669  0.597575  \n",
      "47   0.423373  0.542507  0.747888  0.425381  0.745809  0.743894  0.541778  \n",
      "172  0.462986  0.579796  0.767031  0.464960  0.765109  0.763586  0.578969  \n",
      "\n",
      "[130 rows x 28 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...      1361    1131.2    1132.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.692994  0.462041  0.465845   \n",
      "51   0.472565  0.478432  0.426489  ...  0.757196  0.422574  0.428395   \n",
      "183  0.596755  0.603650  0.540704  ...  0.892685  0.536244  0.543212   \n",
      "145  0.586613  0.592968  0.538299  ...  0.855082  0.534536  0.540435   \n",
      "40   0.410373  0.415739  0.370542  ...  0.658768  0.367193  0.372102   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.803588  0.505490  0.510800   \n",
      "67   0.481982  0.487460  0.439093  ...  0.742642  0.435731  0.440891   \n",
      "117  0.513116  0.519892  0.462708  ...  0.843046  0.458521  0.464717   \n",
      "47   0.468146  0.474372  0.421441  ...  0.755748  0.417804  0.423373   \n",
      "172  0.507965  0.513851  0.460731  ...  0.773772  0.456907  0.462986   \n",
      "\n",
      "       1272.8    1358.2    1133.2    1357.5    1356.7    1267.2    1275.3  \n",
      "16   0.550064  0.687939  0.467234  0.686376  0.685177  0.549601  0.550721  \n",
      "51   0.547437  0.749504  0.430296  0.747396  0.745675  0.546862  0.548119  \n",
      "183  0.682313  0.885778  0.545634  0.883820  0.882231  0.681063  0.683063  \n",
      "145  0.660714  0.848379  0.542453  0.846582  0.844985  0.660047  0.661462  \n",
      "40   0.474118  0.651788  0.373659  0.649921  0.648267  0.473666  0.474701  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.622584  0.797466  0.512619  0.795769  0.794281  0.621668  0.623270  \n",
      "67   0.549084  0.735379  0.442750  0.733556  0.731986  0.548640  0.549730  \n",
      "117  0.598433  0.834153  0.466721  0.831688  0.829669  0.597575  0.599102  \n",
      "47   0.542507  0.747888  0.425381  0.745809  0.743894  0.541778  0.543146  \n",
      "172  0.579796  0.767031  0.464960  0.765109  0.763586  0.578969  0.580606  \n",
      "\n",
      "[130 rows x 29 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1131.2    1132.7    1272.8  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.462041  0.465845  0.550064   \n",
      "51   0.472565  0.478432  0.426489  ...  0.422574  0.428395  0.547437   \n",
      "183  0.596755  0.603650  0.540704  ...  0.536244  0.543212  0.682313   \n",
      "145  0.586613  0.592968  0.538299  ...  0.534536  0.540435  0.660714   \n",
      "40   0.410373  0.415739  0.370542  ...  0.367193  0.372102  0.474118   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.505490  0.510800  0.622584   \n",
      "67   0.481982  0.487460  0.439093  ...  0.435731  0.440891  0.549084   \n",
      "117  0.513116  0.519892  0.462708  ...  0.458521  0.464717  0.598433   \n",
      "47   0.468146  0.474372  0.421441  ...  0.417804  0.423373  0.542507   \n",
      "172  0.507965  0.513851  0.460731  ...  0.456907  0.462986  0.579796   \n",
      "\n",
      "       1358.2    1133.2    1357.5    1356.7    1267.2    1275.3    1262.3  \n",
      "16   0.687939  0.467234  0.686376  0.685177  0.549601  0.550721  0.549940  \n",
      "51   0.749504  0.430296  0.747396  0.745675  0.546862  0.548119  0.547310  \n",
      "183  0.885778  0.545634  0.883820  0.882231  0.681063  0.683063  0.681510  \n",
      "145  0.848379  0.542453  0.846582  0.844985  0.660047  0.661462  0.660473  \n",
      "40   0.651788  0.373659  0.649921  0.648267  0.473666  0.474701  0.474148  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.797466  0.512619  0.795769  0.794281  0.621668  0.623270  0.622102  \n",
      "67   0.735379  0.442750  0.733556  0.731986  0.548640  0.549730  0.549093  \n",
      "117  0.834153  0.466721  0.831688  0.829669  0.597575  0.599102  0.598198  \n",
      "47   0.747888  0.425381  0.745809  0.743894  0.541778  0.543146  0.542238  \n",
      "172  0.767031  0.464960  0.765109  0.763586  0.578969  0.580606  0.579222  \n",
      "\n",
      "[130 rows x 30 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1132.7    1272.8    1358.2  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.465845  0.550064  0.687939   \n",
      "51   0.472565  0.478432  0.426489  ...  0.428395  0.547437  0.749504   \n",
      "183  0.596755  0.603650  0.540704  ...  0.543212  0.682313  0.885778   \n",
      "145  0.586613  0.592968  0.538299  ...  0.540435  0.660714  0.848379   \n",
      "40   0.410373  0.415739  0.370542  ...  0.372102  0.474118  0.651788   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.510800  0.622584  0.797466   \n",
      "67   0.481982  0.487460  0.439093  ...  0.440891  0.549084  0.735379   \n",
      "117  0.513116  0.519892  0.462708  ...  0.464717  0.598433  0.834153   \n",
      "47   0.468146  0.474372  0.421441  ...  0.423373  0.542507  0.747888   \n",
      "172  0.507965  0.513851  0.460731  ...  0.462986  0.579796  0.767031   \n",
      "\n",
      "       1133.2    1357.5    1356.7    1267.2    1275.3    1262.3      1276  \n",
      "16   0.467234  0.686376  0.685177  0.549601  0.550721  0.549940  0.550826  \n",
      "51   0.430296  0.747396  0.745675  0.546862  0.548119  0.547310  0.548370  \n",
      "183  0.545634  0.883820  0.882231  0.681063  0.683063  0.681510  0.683452  \n",
      "145  0.542453  0.846582  0.844985  0.660047  0.661462  0.660473  0.661692  \n",
      "40   0.373659  0.649921  0.648267  0.473666  0.474701  0.474148  0.474844  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.512619  0.795769  0.794281  0.621668  0.623270  0.622102  0.623471  \n",
      "67   0.442750  0.733556  0.731986  0.548640  0.549730  0.549093  0.549975  \n",
      "117  0.466721  0.831688  0.829669  0.597575  0.599102  0.598198  0.599481  \n",
      "47   0.425381  0.745809  0.743894  0.541778  0.543146  0.542238  0.543433  \n",
      "172  0.464960  0.765109  0.763586  0.578969  0.580606  0.579222  0.580780  \n",
      "\n",
      "[130 rows x 31 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1272.8    1358.2    1133.2  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.550064  0.687939  0.467234   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547437  0.749504  0.430296   \n",
      "183  0.596755  0.603650  0.540704  ...  0.682313  0.885778  0.545634   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660714  0.848379  0.542453   \n",
      "40   0.410373  0.415739  0.370542  ...  0.474118  0.651788  0.373659   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.622584  0.797466  0.512619   \n",
      "67   0.481982  0.487460  0.439093  ...  0.549084  0.735379  0.442750   \n",
      "117  0.513116  0.519892  0.462708  ...  0.598433  0.834153  0.466721   \n",
      "47   0.468146  0.474372  0.421441  ...  0.542507  0.747888  0.425381   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579796  0.767031  0.464960   \n",
      "\n",
      "       1357.5    1356.7    1267.2    1275.3    1262.3      1276    1272.2  \n",
      "16   0.686376  0.685177  0.549601  0.550721  0.549940  0.550826  0.550069  \n",
      "51   0.747396  0.745675  0.546862  0.548119  0.547310  0.548370  0.547489  \n",
      "183  0.883820  0.882231  0.681063  0.683063  0.681510  0.683452  0.682088  \n",
      "145  0.846582  0.844985  0.660047  0.661462  0.660473  0.661692  0.660699  \n",
      "40   0.649921  0.648267  0.473666  0.474701  0.474148  0.474844  0.474133  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.795769  0.794281  0.621668  0.623270  0.622102  0.623471  0.622367  \n",
      "67   0.733556  0.731986  0.548640  0.549730  0.549093  0.549975  0.549142  \n",
      "117  0.831688  0.829669  0.597575  0.599102  0.598198  0.599481  0.598428  \n",
      "47   0.745809  0.743894  0.541778  0.543146  0.542238  0.543433  0.542597  \n",
      "172  0.765109  0.763586  0.578969  0.580606  0.579222  0.580780  0.579687  \n",
      "\n",
      "[130 rows x 32 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1358.2    1133.2    1357.5  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.687939  0.467234  0.686376   \n",
      "51   0.472565  0.478432  0.426489  ...  0.749504  0.430296  0.747396   \n",
      "183  0.596755  0.603650  0.540704  ...  0.885778  0.545634  0.883820   \n",
      "145  0.586613  0.592968  0.538299  ...  0.848379  0.542453  0.846582   \n",
      "40   0.410373  0.415739  0.370542  ...  0.651788  0.373659  0.649921   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.797466  0.512619  0.795769   \n",
      "67   0.481982  0.487460  0.439093  ...  0.735379  0.442750  0.733556   \n",
      "117  0.513116  0.519892  0.462708  ...  0.834153  0.466721  0.831688   \n",
      "47   0.468146  0.474372  0.421441  ...  0.747888  0.425381  0.745809   \n",
      "172  0.507965  0.513851  0.460731  ...  0.767031  0.464960  0.765109   \n",
      "\n",
      "       1356.7    1267.2    1275.3    1262.3      1276    1272.2    1270.9  \n",
      "16   0.685177  0.549601  0.550721  0.549940  0.550826  0.550069  0.549837  \n",
      "51   0.745675  0.546862  0.548119  0.547310  0.548370  0.547489  0.547223  \n",
      "183  0.882231  0.681063  0.683063  0.681510  0.683452  0.682088  0.681674  \n",
      "145  0.844985  0.660047  0.661462  0.660473  0.661692  0.660699  0.660331  \n",
      "40   0.648267  0.473666  0.474701  0.474148  0.474844  0.474133  0.473772  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.794281  0.621668  0.623270  0.622102  0.623471  0.622367  0.622002  \n",
      "67   0.731986  0.548640  0.549730  0.549093  0.549975  0.549142  0.548855  \n",
      "117  0.829669  0.597575  0.599102  0.598198  0.599481  0.598428  0.597986  \n",
      "47   0.743894  0.541778  0.543146  0.542238  0.543433  0.542597  0.542089  \n",
      "172  0.763586  0.578969  0.580606  0.579222  0.580780  0.579687  0.579379  \n",
      "\n",
      "[130 rows x 33 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1133.2    1357.5    1356.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.467234  0.686376  0.685177   \n",
      "51   0.472565  0.478432  0.426489  ...  0.430296  0.747396  0.745675   \n",
      "183  0.596755  0.603650  0.540704  ...  0.545634  0.883820  0.882231   \n",
      "145  0.586613  0.592968  0.538299  ...  0.542453  0.846582  0.844985   \n",
      "40   0.410373  0.415739  0.370542  ...  0.373659  0.649921  0.648267   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.512619  0.795769  0.794281   \n",
      "67   0.481982  0.487460  0.439093  ...  0.442750  0.733556  0.731986   \n",
      "117  0.513116  0.519892  0.462708  ...  0.466721  0.831688  0.829669   \n",
      "47   0.468146  0.474372  0.421441  ...  0.425381  0.745809  0.743894   \n",
      "172  0.507965  0.513851  0.460731  ...  0.464960  0.765109  0.763586   \n",
      "\n",
      "       1267.2    1275.3    1262.3      1276    1272.2    1270.9    1143.2  \n",
      "16   0.549601  0.550721  0.549940  0.550826  0.550069  0.549837  0.506871  \n",
      "51   0.546862  0.548119  0.547310  0.548370  0.547489  0.547223  0.484188  \n",
      "183  0.681063  0.683063  0.681510  0.683452  0.682088  0.681674  0.610750  \n",
      "145  0.660047  0.661462  0.660473  0.661692  0.660699  0.660331  0.598958  \n",
      "40   0.473666  0.474701  0.474148  0.474844  0.474133  0.473772  0.420590  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.621668  0.623270  0.622102  0.623471  0.622367  0.622002  0.562694  \n",
      "67   0.548640  0.549730  0.549093  0.549975  0.549142  0.548855  0.492731  \n",
      "117  0.597575  0.599102  0.598198  0.599481  0.598428  0.597986  0.526516  \n",
      "47   0.541778  0.543146  0.542238  0.543433  0.542597  0.542089  0.480225  \n",
      "172  0.578969  0.580606  0.579222  0.580780  0.579687  0.579379  0.519830  \n",
      "\n",
      "[130 rows x 34 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1357.5    1356.7    1267.2  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.686376  0.685177  0.549601   \n",
      "51   0.472565  0.478432  0.426489  ...  0.747396  0.745675  0.546862   \n",
      "183  0.596755  0.603650  0.540704  ...  0.883820  0.882231  0.681063   \n",
      "145  0.586613  0.592968  0.538299  ...  0.846582  0.844985  0.660047   \n",
      "40   0.410373  0.415739  0.370542  ...  0.649921  0.648267  0.473666   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.795769  0.794281  0.621668   \n",
      "67   0.481982  0.487460  0.439093  ...  0.733556  0.731986  0.548640   \n",
      "117  0.513116  0.519892  0.462708  ...  0.831688  0.829669  0.597575   \n",
      "47   0.468146  0.474372  0.421441  ...  0.745809  0.743894  0.541778   \n",
      "172  0.507965  0.513851  0.460731  ...  0.765109  0.763586  0.578969   \n",
      "\n",
      "       1275.3    1262.3      1276    1272.2    1270.9    1143.2    1267.8  \n",
      "16   0.550721  0.549940  0.550826  0.550069  0.549837  0.506871  0.549593  \n",
      "51   0.548119  0.547310  0.548370  0.547489  0.547223  0.484188  0.547010  \n",
      "183  0.683063  0.681510  0.683452  0.682088  0.681674  0.610750  0.681319  \n",
      "145  0.661462  0.660473  0.661692  0.660699  0.660331  0.598958  0.660124  \n",
      "40   0.474701  0.474148  0.474844  0.474133  0.473772  0.420590  0.473700  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.623270  0.622102  0.623471  0.622367  0.622002  0.562694  0.621794  \n",
      "67   0.549730  0.549093  0.549975  0.549142  0.548855  0.492731  0.548684  \n",
      "117  0.599102  0.598198  0.599481  0.598428  0.597986  0.526516  0.597815  \n",
      "47   0.543146  0.542238  0.543433  0.542597  0.542089  0.480225  0.541924  \n",
      "172  0.580606  0.579222  0.580780  0.579687  0.579379  0.519830  0.579034  \n",
      "\n",
      "[130 rows x 35 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1356.7    1267.2    1275.3  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.685177  0.549601  0.550721   \n",
      "51   0.472565  0.478432  0.426489  ...  0.745675  0.546862  0.548119   \n",
      "183  0.596755  0.603650  0.540704  ...  0.882231  0.681063  0.683063   \n",
      "145  0.586613  0.592968  0.538299  ...  0.844985  0.660047  0.661462   \n",
      "40   0.410373  0.415739  0.370542  ...  0.648267  0.473666  0.474701   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.794281  0.621668  0.623270   \n",
      "67   0.481982  0.487460  0.439093  ...  0.731986  0.548640  0.549730   \n",
      "117  0.513116  0.519892  0.462708  ...  0.829669  0.597575  0.599102   \n",
      "47   0.468146  0.474372  0.421441  ...  0.743894  0.541778  0.543146   \n",
      "172  0.507965  0.513851  0.460731  ...  0.763586  0.578969  0.580606   \n",
      "\n",
      "       1262.3      1276    1272.2    1270.9    1143.2    1267.8    1136.7  \n",
      "16   0.549940  0.550826  0.550069  0.549837  0.506871  0.549593  0.479435  \n",
      "51   0.547310  0.548370  0.547489  0.547223  0.484188  0.547010  0.446696  \n",
      "183  0.681510  0.683452  0.682088  0.681674  0.610750  0.681319  0.565836  \n",
      "145  0.660473  0.661692  0.660699  0.660331  0.598958  0.660124  0.559817  \n",
      "40   0.474148  0.474844  0.474133  0.473772  0.420590  0.473700  0.387905  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.622102  0.623471  0.622367  0.622002  0.562694  0.621794  0.527988  \n",
      "67   0.549093  0.549975  0.549142  0.548855  0.492731  0.548684  0.457875  \n",
      "117  0.598198  0.599481  0.598428  0.597986  0.526516  0.597815  0.484530  \n",
      "47   0.542238  0.543433  0.542597  0.542089  0.480225  0.541924  0.442135  \n",
      "172  0.579222  0.580780  0.579687  0.579379  0.519830  0.579034  0.481677  \n",
      "\n",
      "[130 rows x 36 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1267.2    1275.3    1262.3  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549601  0.550721  0.549940   \n",
      "51   0.472565  0.478432  0.426489  ...  0.546862  0.548119  0.547310   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681063  0.683063  0.681510   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660047  0.661462  0.660473   \n",
      "40   0.410373  0.415739  0.370542  ...  0.473666  0.474701  0.474148   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.621668  0.623270  0.622102   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548640  0.549730  0.549093   \n",
      "117  0.513116  0.519892  0.462708  ...  0.597575  0.599102  0.598198   \n",
      "47   0.468146  0.474372  0.421441  ...  0.541778  0.543146  0.542238   \n",
      "172  0.507965  0.513851  0.460731  ...  0.578969  0.580606  0.579222   \n",
      "\n",
      "         1276    1272.2    1270.9    1143.2    1267.8    1136.7    1273.4  \n",
      "16   0.550826  0.550069  0.549837  0.506871  0.549593  0.479435  0.550204  \n",
      "51   0.548370  0.547489  0.547223  0.484188  0.547010  0.446696  0.547685  \n",
      "183  0.683452  0.682088  0.681674  0.610750  0.681319  0.565836  0.682402  \n",
      "145  0.661692  0.660699  0.660331  0.598958  0.660124  0.559817  0.660835  \n",
      "40   0.474844  0.474133  0.473772  0.420590  0.473700  0.387905  0.474088  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.623471  0.622367  0.622002  0.562694  0.621794  0.527988  0.622692  \n",
      "67   0.549975  0.549142  0.548855  0.492731  0.548684  0.457875  0.549229  \n",
      "117  0.599481  0.598428  0.597986  0.526516  0.597815  0.484530  0.598622  \n",
      "47   0.543433  0.542597  0.542089  0.480225  0.541924  0.442135  0.542551  \n",
      "172  0.580780  0.579687  0.579379  0.519830  0.579034  0.481677  0.579872  \n",
      "\n",
      "[130 rows x 37 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1275.3    1262.3      1276  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.550721  0.549940  0.550826   \n",
      "51   0.472565  0.478432  0.426489  ...  0.548119  0.547310  0.548370   \n",
      "183  0.596755  0.603650  0.540704  ...  0.683063  0.681510  0.683452   \n",
      "145  0.586613  0.592968  0.538299  ...  0.661462  0.660473  0.661692   \n",
      "40   0.410373  0.415739  0.370542  ...  0.474701  0.474148  0.474844   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.623270  0.622102  0.623471   \n",
      "67   0.481982  0.487460  0.439093  ...  0.549730  0.549093  0.549975   \n",
      "117  0.513116  0.519892  0.462708  ...  0.599102  0.598198  0.599481   \n",
      "47   0.468146  0.474372  0.421441  ...  0.543146  0.542238  0.543433   \n",
      "172  0.507965  0.513851  0.460731  ...  0.580606  0.579222  0.580780   \n",
      "\n",
      "       1272.2    1270.9    1143.2    1267.8    1136.7    1273.4      1266  \n",
      "16   0.550069  0.549837  0.506871  0.549593  0.479435  0.550204  0.549714  \n",
      "51   0.547489  0.547223  0.484188  0.547010  0.446696  0.547685  0.546949  \n",
      "183  0.682088  0.681674  0.610750  0.681319  0.565836  0.682402  0.681339  \n",
      "145  0.660699  0.660331  0.598958  0.660124  0.559817  0.660835  0.660298  \n",
      "40   0.474133  0.473772  0.420590  0.473700  0.387905  0.474088  0.473720  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.622367  0.622002  0.562694  0.621794  0.527988  0.622692  0.621922  \n",
      "67   0.549142  0.548855  0.492731  0.548684  0.457875  0.549229  0.548689  \n",
      "117  0.598428  0.597986  0.526516  0.597815  0.484530  0.598622  0.597846  \n",
      "47   0.542597  0.542089  0.480225  0.541924  0.442135  0.542551  0.541904  \n",
      "172  0.579687  0.579379  0.519830  0.579034  0.481677  0.579872  0.579144  \n",
      "\n",
      "[130 rows x 38 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1262.3      1276    1272.2  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549940  0.550826  0.550069   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547310  0.548370  0.547489   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681510  0.683452  0.682088   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660473  0.661692  0.660699   \n",
      "40   0.410373  0.415739  0.370542  ...  0.474148  0.474844  0.474133   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.622102  0.623471  0.622367   \n",
      "67   0.481982  0.487460  0.439093  ...  0.549093  0.549975  0.549142   \n",
      "117  0.513116  0.519892  0.462708  ...  0.598198  0.599481  0.598428   \n",
      "47   0.468146  0.474372  0.421441  ...  0.542238  0.543433  0.542597   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579222  0.580780  0.579687   \n",
      "\n",
      "       1270.9    1143.2    1267.8    1136.7    1273.4      1266    1270.3  \n",
      "16   0.549837  0.506871  0.549593  0.479435  0.550204  0.549714  0.549856  \n",
      "51   0.547223  0.484188  0.547010  0.446696  0.547685  0.546949  0.547168  \n",
      "183  0.681674  0.610750  0.681319  0.565836  0.682402  0.681339  0.681728  \n",
      "145  0.660331  0.598958  0.660124  0.559817  0.660835  0.660298  0.660342  \n",
      "40   0.473772  0.420590  0.473700  0.387905  0.474088  0.473720  0.473737  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.622002  0.562694  0.621794  0.527988  0.622692  0.621922  0.622036  \n",
      "67   0.548855  0.492731  0.548684  0.457875  0.549229  0.548689  0.548802  \n",
      "117  0.597986  0.526516  0.597815  0.484530  0.598622  0.597846  0.597966  \n",
      "47   0.542089  0.480225  0.541924  0.442135  0.542551  0.541904  0.542047  \n",
      "172  0.579379  0.519830  0.579034  0.481677  0.579872  0.579144  0.579241  \n",
      "\n",
      "[130 rows x 39 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...      1276    1272.2    1270.9  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.550826  0.550069  0.549837   \n",
      "51   0.472565  0.478432  0.426489  ...  0.548370  0.547489  0.547223   \n",
      "183  0.596755  0.603650  0.540704  ...  0.683452  0.682088  0.681674   \n",
      "145  0.586613  0.592968  0.538299  ...  0.661692  0.660699  0.660331   \n",
      "40   0.410373  0.415739  0.370542  ...  0.474844  0.474133  0.473772   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.623471  0.622367  0.622002   \n",
      "67   0.481982  0.487460  0.439093  ...  0.549975  0.549142  0.548855   \n",
      "117  0.513116  0.519892  0.462708  ...  0.599481  0.598428  0.597986   \n",
      "47   0.468146  0.474372  0.421441  ...  0.543433  0.542597  0.542089   \n",
      "172  0.507965  0.513851  0.460731  ...  0.580780  0.579687  0.579379   \n",
      "\n",
      "       1143.2    1267.8    1136.7    1273.4      1266    1270.3    1266.6  \n",
      "16   0.506871  0.549593  0.479435  0.550204  0.549714  0.549856  0.549688  \n",
      "51   0.484188  0.547010  0.446696  0.547685  0.546949  0.547168  0.546906  \n",
      "183  0.610750  0.681319  0.565836  0.682402  0.681339  0.681728  0.681204  \n",
      "145  0.598958  0.660124  0.559817  0.660835  0.660298  0.660342  0.660230  \n",
      "40   0.420590  0.473700  0.387905  0.474088  0.473720  0.473737  0.473768  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.562694  0.621794  0.527988  0.622692  0.621922  0.622036  0.621869  \n",
      "67   0.492731  0.548684  0.457875  0.549229  0.548689  0.548802  0.548729  \n",
      "117  0.526516  0.597815  0.484530  0.598622  0.597846  0.597966  0.597723  \n",
      "47   0.480225  0.541924  0.442135  0.542551  0.541904  0.542047  0.541895  \n",
      "172  0.519830  0.579034  0.481677  0.579872  0.579144  0.579241  0.579167  \n",
      "\n",
      "[130 rows x 40 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1272.2    1270.9    1143.2  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.550069  0.549837  0.506871   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547489  0.547223  0.484188   \n",
      "183  0.596755  0.603650  0.540704  ...  0.682088  0.681674  0.610750   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660699  0.660331  0.598958   \n",
      "40   0.410373  0.415739  0.370542  ...  0.474133  0.473772  0.420590   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.622367  0.622002  0.562694   \n",
      "67   0.481982  0.487460  0.439093  ...  0.549142  0.548855  0.492731   \n",
      "117  0.513116  0.519892  0.462708  ...  0.598428  0.597986  0.526516   \n",
      "47   0.468146  0.474372  0.421441  ...  0.542597  0.542089  0.480225   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579687  0.579379  0.519830   \n",
      "\n",
      "       1267.8    1136.7    1273.4      1266    1270.3    1266.6    1265.4  \n",
      "16   0.549593  0.479435  0.550204  0.549714  0.549856  0.549688  0.549712  \n",
      "51   0.547010  0.446696  0.547685  0.546949  0.547168  0.546906  0.546984  \n",
      "183  0.681319  0.565836  0.682402  0.681339  0.681728  0.681204  0.681295  \n",
      "145  0.660124  0.559817  0.660835  0.660298  0.660342  0.660230  0.660226  \n",
      "40   0.473700  0.387905  0.474088  0.473720  0.473737  0.473768  0.473781  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.621794  0.527988  0.622692  0.621922  0.622036  0.621869  0.621790  \n",
      "67   0.548684  0.457875  0.549229  0.548689  0.548802  0.548729  0.548642  \n",
      "117  0.597815  0.484530  0.598622  0.597846  0.597966  0.597723  0.597817  \n",
      "47   0.541924  0.442135  0.542551  0.541904  0.542047  0.541895  0.541967  \n",
      "172  0.579034  0.481677  0.579872  0.579144  0.579241  0.579167  0.579043  \n",
      "\n",
      "[130 rows x 41 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1270.9    1143.2    1267.8  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549837  0.506871  0.549593   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547223  0.484188  0.547010   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681674  0.610750  0.681319   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660331  0.598958  0.660124   \n",
      "40   0.410373  0.415739  0.370542  ...  0.473772  0.420590  0.473700   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.622002  0.562694  0.621794   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548855  0.492731  0.548684   \n",
      "117  0.513116  0.519892  0.462708  ...  0.597986  0.526516  0.597815   \n",
      "47   0.468146  0.474372  0.421441  ...  0.542089  0.480225  0.541924   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579379  0.519830  0.579034   \n",
      "\n",
      "       1136.7    1273.4      1266    1270.3    1266.6    1265.4    1274.7  \n",
      "16   0.479435  0.550204  0.549714  0.549856  0.549688  0.549712  0.550611  \n",
      "51   0.446696  0.547685  0.546949  0.547168  0.546906  0.546984  0.547844  \n",
      "183  0.565836  0.682402  0.681339  0.681728  0.681204  0.681295  0.682764  \n",
      "145  0.559817  0.660835  0.660298  0.660342  0.660230  0.660226  0.661221  \n",
      "40   0.387905  0.474088  0.473720  0.473737  0.473768  0.473781  0.474471  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.527988  0.622692  0.621922  0.622036  0.621869  0.621790  0.623076  \n",
      "67   0.457875  0.549229  0.548689  0.548802  0.548729  0.548642  0.549506  \n",
      "117  0.484530  0.598622  0.597846  0.597966  0.597723  0.597817  0.598829  \n",
      "47   0.442135  0.542551  0.541904  0.542047  0.541895  0.541967  0.542778  \n",
      "172  0.481677  0.579872  0.579144  0.579241  0.579167  0.579043  0.580233  \n",
      "\n",
      "[130 rows x 42 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1143.2    1267.8    1136.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.506871  0.549593  0.479435   \n",
      "51   0.472565  0.478432  0.426489  ...  0.484188  0.547010  0.446696   \n",
      "183  0.596755  0.603650  0.540704  ...  0.610750  0.681319  0.565836   \n",
      "145  0.586613  0.592968  0.538299  ...  0.598958  0.660124  0.559817   \n",
      "40   0.410373  0.415739  0.370542  ...  0.420590  0.473700  0.387905   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.562694  0.621794  0.527988   \n",
      "67   0.481982  0.487460  0.439093  ...  0.492731  0.548684  0.457875   \n",
      "117  0.513116  0.519892  0.462708  ...  0.526516  0.597815  0.484530   \n",
      "47   0.468146  0.474372  0.421441  ...  0.480225  0.541924  0.442135   \n",
      "172  0.507965  0.513851  0.460731  ...  0.519830  0.579034  0.481677   \n",
      "\n",
      "       1273.4      1266    1270.3    1266.6    1265.4    1274.7    1269.1  \n",
      "16   0.550204  0.549714  0.549856  0.549688  0.549712  0.550611  0.549698  \n",
      "51   0.547685  0.546949  0.547168  0.546906  0.546984  0.547844  0.547007  \n",
      "183  0.682402  0.681339  0.681728  0.681204  0.681295  0.682764  0.681515  \n",
      "145  0.660835  0.660298  0.660342  0.660230  0.660226  0.661221  0.660256  \n",
      "40   0.474088  0.473720  0.473737  0.473768  0.473781  0.474471  0.473737  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.622692  0.621922  0.622036  0.621869  0.621790  0.623076  0.621776  \n",
      "67   0.549229  0.548689  0.548802  0.548729  0.548642  0.549506  0.548525  \n",
      "117  0.598622  0.597846  0.597966  0.597723  0.597817  0.598829  0.597721  \n",
      "47   0.542551  0.541904  0.542047  0.541895  0.541967  0.542778  0.541820  \n",
      "172  0.579872  0.579144  0.579241  0.579167  0.579043  0.580233  0.579034  \n",
      "\n",
      "[130 rows x 43 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1267.8    1136.7    1273.4  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549593  0.479435  0.550204   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547010  0.446696  0.547685   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681319  0.565836  0.682402   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660124  0.559817  0.660835   \n",
      "40   0.410373  0.415739  0.370542  ...  0.473700  0.387905  0.474088   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.621794  0.527988  0.622692   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548684  0.457875  0.549229   \n",
      "117  0.513116  0.519892  0.462708  ...  0.597815  0.484530  0.598622   \n",
      "47   0.468146  0.474372  0.421441  ...  0.541924  0.442135  0.542551   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579034  0.481677  0.579872   \n",
      "\n",
      "         1266    1270.3    1266.6    1265.4    1274.7    1269.1    1369.6  \n",
      "16   0.549714  0.549856  0.549688  0.549712  0.550611  0.549698  0.713986  \n",
      "51   0.546949  0.547168  0.546906  0.546984  0.547844  0.547007  0.788922  \n",
      "183  0.681339  0.681728  0.681204  0.681295  0.682764  0.681515  0.920578  \n",
      "145  0.660298  0.660342  0.660230  0.660226  0.661221  0.660256  0.881742  \n",
      "40   0.473720  0.473737  0.473768  0.473781  0.474471  0.473737  0.686927  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.621922  0.622036  0.621869  0.621790  0.623076  0.621776  0.829722  \n",
      "67   0.548689  0.548802  0.548729  0.548642  0.549506  0.548525  0.771319  \n",
      "117  0.597846  0.597966  0.597723  0.597817  0.598829  0.597721  0.878702  \n",
      "47   0.541904  0.542047  0.541895  0.541967  0.542778  0.541820  0.787893  \n",
      "172  0.579144  0.579241  0.579167  0.579043  0.580233  0.579034  0.801684  \n",
      "\n",
      "[130 rows x 44 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1136.7    1273.4      1266  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.479435  0.550204  0.549714   \n",
      "51   0.472565  0.478432  0.426489  ...  0.446696  0.547685  0.546949   \n",
      "183  0.596755  0.603650  0.540704  ...  0.565836  0.682402  0.681339   \n",
      "145  0.586613  0.592968  0.538299  ...  0.559817  0.660835  0.660298   \n",
      "40   0.410373  0.415739  0.370542  ...  0.387905  0.474088  0.473720   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.527988  0.622692  0.621922   \n",
      "67   0.481982  0.487460  0.439093  ...  0.457875  0.549229  0.548689   \n",
      "117  0.513116  0.519892  0.462708  ...  0.484530  0.598622  0.597846   \n",
      "47   0.468146  0.474372  0.421441  ...  0.442135  0.542551  0.541904   \n",
      "172  0.507965  0.513851  0.460731  ...  0.481677  0.579872  0.579144   \n",
      "\n",
      "       1270.3    1266.6    1265.4    1274.7    1269.1    1369.6    1370.4  \n",
      "16   0.549856  0.549688  0.549712  0.550611  0.549698  0.713986  0.715977  \n",
      "51   0.547168  0.546906  0.546984  0.547844  0.547007  0.788922  0.792114  \n",
      "183  0.681728  0.681204  0.681295  0.682764  0.681515  0.920578  0.923226  \n",
      "145  0.660342  0.660230  0.660226  0.661221  0.660256  0.881742  0.884380  \n",
      "40   0.473737  0.473768  0.473781  0.474471  0.473737  0.686927  0.689682  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.622036  0.621869  0.621790  0.623076  0.621776  0.829722  0.832299  \n",
      "67   0.548802  0.548729  0.548642  0.549506  0.548525  0.771319  0.774248  \n",
      "117  0.597966  0.597723  0.597817  0.598829  0.597721  0.878702  0.882351  \n",
      "47   0.542047  0.541895  0.541967  0.542778  0.541820  0.787893  0.791090  \n",
      "172  0.579241  0.579167  0.579043  0.580233  0.579034  0.801684  0.804446  \n",
      "\n",
      "[130 rows x 45 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1273.4      1266    1270.3  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.550204  0.549714  0.549856   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547685  0.546949  0.547168   \n",
      "183  0.596755  0.603650  0.540704  ...  0.682402  0.681339  0.681728   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660835  0.660298  0.660342   \n",
      "40   0.410373  0.415739  0.370542  ...  0.474088  0.473720  0.473737   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.622692  0.621922  0.622036   \n",
      "67   0.481982  0.487460  0.439093  ...  0.549229  0.548689  0.548802   \n",
      "117  0.513116  0.519892  0.462708  ...  0.598622  0.597846  0.597966   \n",
      "47   0.468146  0.474372  0.421441  ...  0.542551  0.541904  0.542047   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579872  0.579144  0.579241   \n",
      "\n",
      "       1266.6    1265.4    1274.7    1269.1    1369.6    1370.4    1263.5  \n",
      "16   0.549688  0.549712  0.550611  0.549698  0.713986  0.715977  0.549830  \n",
      "51   0.546906  0.546984  0.547844  0.547007  0.788922  0.792114  0.547065  \n",
      "183  0.681204  0.681295  0.682764  0.681515  0.920578  0.923226  0.681209  \n",
      "145  0.660230  0.660226  0.661221  0.660256  0.881742  0.884380  0.660099  \n",
      "40   0.473768  0.473781  0.474471  0.473737  0.686927  0.689682  0.473992  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.621869  0.621790  0.623076  0.621776  0.829722  0.832299  0.621702  \n",
      "67   0.548729  0.548642  0.549506  0.548525  0.771319  0.774248  0.548702  \n",
      "117  0.597723  0.597817  0.598829  0.597721  0.878702  0.882351  0.597866  \n",
      "47   0.541895  0.541967  0.542778  0.541820  0.787893  0.791090  0.541813  \n",
      "172  0.579167  0.579043  0.580233  0.579034  0.801684  0.804446  0.578967  \n",
      "\n",
      "[130 rows x 46 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...      1266    1270.3    1266.6  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549714  0.549856  0.549688   \n",
      "51   0.472565  0.478432  0.426489  ...  0.546949  0.547168  0.546906   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681339  0.681728  0.681204   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660298  0.660342  0.660230   \n",
      "40   0.410373  0.415739  0.370542  ...  0.473720  0.473737  0.473768   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.621922  0.622036  0.621869   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548689  0.548802  0.548729   \n",
      "117  0.513116  0.519892  0.462708  ...  0.597846  0.597966  0.597723   \n",
      "47   0.468146  0.474372  0.421441  ...  0.541904  0.542047  0.541895   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579144  0.579241  0.579167   \n",
      "\n",
      "       1265.4    1274.7    1269.1    1369.6    1370.4    1263.5    1271.6  \n",
      "16   0.549712  0.550611  0.549698  0.713986  0.715977  0.549830  0.550016  \n",
      "51   0.546984  0.547844  0.547007  0.788922  0.792114  0.547065  0.547453  \n",
      "183  0.681295  0.682764  0.681515  0.920578  0.923226  0.681209  0.681871  \n",
      "145  0.660226  0.661221  0.660256  0.881742  0.884380  0.660099  0.660589  \n",
      "40   0.473781  0.474471  0.473737  0.686927  0.689682  0.473992  0.474036  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.621790  0.623076  0.621776  0.829722  0.832299  0.621702  0.622204  \n",
      "67   0.548642  0.549506  0.548525  0.771319  0.774248  0.548702  0.549109  \n",
      "117  0.597817  0.598829  0.597721  0.878702  0.882351  0.597866  0.598332  \n",
      "47   0.541967  0.542778  0.541820  0.787893  0.791090  0.541813  0.542462  \n",
      "172  0.579043  0.580233  0.579034  0.801684  0.804446  0.578967  0.579635  \n",
      "\n",
      "[130 rows x 47 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1270.3    1266.6    1265.4  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549856  0.549688  0.549712   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547168  0.546906  0.546984   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681728  0.681204  0.681295   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660342  0.660230  0.660226   \n",
      "40   0.410373  0.415739  0.370542  ...  0.473737  0.473768  0.473781   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.622036  0.621869  0.621790   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548802  0.548729  0.548642   \n",
      "117  0.513116  0.519892  0.462708  ...  0.597966  0.597723  0.597817   \n",
      "47   0.468146  0.474372  0.421441  ...  0.542047  0.541895  0.541967   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579241  0.579167  0.579043   \n",
      "\n",
      "       1274.7    1269.1    1369.6    1370.4    1263.5    1271.6    1361.7  \n",
      "16   0.550611  0.549698  0.713986  0.715977  0.549830  0.550016  0.693790  \n",
      "51   0.547844  0.547007  0.788922  0.792114  0.547065  0.547453  0.758795  \n",
      "183  0.682764  0.681515  0.920578  0.923226  0.681209  0.681871  0.893843  \n",
      "145  0.661221  0.660256  0.881742  0.884380  0.660099  0.660589  0.856380  \n",
      "40   0.474471  0.473737  0.686927  0.689682  0.473992  0.474036  0.660193  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.623076  0.621776  0.829722  0.832299  0.621702  0.622204  0.804826  \n",
      "67   0.549506  0.548525  0.771319  0.774248  0.548702  0.549109  0.743937  \n",
      "117  0.598829  0.597721  0.878702  0.882351  0.597866  0.598332  0.844885  \n",
      "47   0.542778  0.541820  0.787893  0.791090  0.541813  0.542462  0.757242  \n",
      "172  0.580233  0.579034  0.801684  0.804446  0.578967  0.579635  0.775057  \n",
      "\n",
      "[130 rows x 48 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1266.6    1265.4    1274.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549688  0.549712  0.550611   \n",
      "51   0.472565  0.478432  0.426489  ...  0.546906  0.546984  0.547844   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681204  0.681295  0.682764   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660230  0.660226  0.661221   \n",
      "40   0.410373  0.415739  0.370542  ...  0.473768  0.473781  0.474471   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.621869  0.621790  0.623076   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548729  0.548642  0.549506   \n",
      "117  0.513116  0.519892  0.462708  ...  0.597723  0.597817  0.598829   \n",
      "47   0.468146  0.474372  0.421441  ...  0.541895  0.541967  0.542778   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579167  0.579043  0.580233   \n",
      "\n",
      "       1269.1    1369.6    1370.4    1263.5    1271.6    1361.7    1368.9  \n",
      "16   0.549698  0.713986  0.715977  0.549830  0.550016  0.693790  0.711526  \n",
      "51   0.547007  0.788922  0.792114  0.547065  0.547453  0.758795  0.785335  \n",
      "183  0.681515  0.920578  0.923226  0.681209  0.681871  0.893843  0.917398  \n",
      "145  0.660256  0.881742  0.884380  0.660099  0.660589  0.856380  0.878686  \n",
      "40   0.473737  0.686927  0.689682  0.473992  0.474036  0.660193  0.683761  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.621776  0.829722  0.832299  0.621702  0.622204  0.804826  0.826854  \n",
      "67   0.548525  0.771319  0.774248  0.548702  0.549109  0.743937  0.768024  \n",
      "117  0.597721  0.878702  0.882351  0.597866  0.598332  0.844885  0.874686  \n",
      "47   0.541820  0.787893  0.791090  0.541813  0.542462  0.757242  0.784283  \n",
      "172  0.579034  0.801684  0.804446  0.578967  0.579635  0.775057  0.798647  \n",
      "\n",
      "[130 rows x 49 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1265.4    1274.7    1269.1  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549712  0.550611  0.549698   \n",
      "51   0.472565  0.478432  0.426489  ...  0.546984  0.547844  0.547007   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681295  0.682764  0.681515   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660226  0.661221  0.660256   \n",
      "40   0.410373  0.415739  0.370542  ...  0.473781  0.474471  0.473737   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.621790  0.623076  0.621776   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548642  0.549506  0.548525   \n",
      "117  0.513116  0.519892  0.462708  ...  0.597817  0.598829  0.597721   \n",
      "47   0.468146  0.474372  0.421441  ...  0.541967  0.542778  0.541820   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579043  0.580233  0.579034   \n",
      "\n",
      "       1369.6    1370.4    1263.5    1271.6    1361.7    1368.9    1135.7  \n",
      "16   0.713986  0.715977  0.549830  0.550016  0.693790  0.711526  0.475591  \n",
      "51   0.788922  0.792114  0.547065  0.547453  0.758795  0.785335  0.441615  \n",
      "183  0.920578  0.923226  0.681209  0.681871  0.893843  0.917398  0.559448  \n",
      "145  0.881742  0.884380  0.660099  0.660589  0.856380  0.878686  0.554381  \n",
      "40   0.686927  0.689682  0.473992  0.474036  0.660193  0.683761  0.383583  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.829722  0.832299  0.621702  0.622204  0.804826  0.826854  0.523149  \n",
      "67   0.771319  0.774248  0.548702  0.549109  0.743937  0.768024  0.453115  \n",
      "117  0.878702  0.882351  0.597866  0.598332  0.844885  0.874686  0.478717  \n",
      "47   0.787893  0.791090  0.541813  0.542462  0.757242  0.784283  0.436769  \n",
      "172  0.801684  0.804446  0.578967  0.579635  0.775057  0.798647  0.476436  \n",
      "\n",
      "[130 rows x 50 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1274.7    1269.1    1369.6  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.550611  0.549698  0.713986   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547844  0.547007  0.788922   \n",
      "183  0.596755  0.603650  0.540704  ...  0.682764  0.681515  0.920578   \n",
      "145  0.586613  0.592968  0.538299  ...  0.661221  0.660256  0.881742   \n",
      "40   0.410373  0.415739  0.370542  ...  0.474471  0.473737  0.686927   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.623076  0.621776  0.829722   \n",
      "67   0.481982  0.487460  0.439093  ...  0.549506  0.548525  0.771319   \n",
      "117  0.513116  0.519892  0.462708  ...  0.598829  0.597721  0.878702   \n",
      "47   0.468146  0.474372  0.421441  ...  0.542778  0.541820  0.787893   \n",
      "172  0.507965  0.513851  0.460731  ...  0.580233  0.579034  0.801684   \n",
      "\n",
      "       1370.4    1263.5    1271.6    1361.7    1368.9    1135.7    1276.6  \n",
      "16   0.715977  0.549830  0.550016  0.693790  0.711526  0.475591  0.551102  \n",
      "51   0.792114  0.547065  0.547453  0.758795  0.785335  0.441615  0.548631  \n",
      "183  0.923226  0.681209  0.681871  0.893843  0.917398  0.559448  0.683768  \n",
      "145  0.884380  0.660099  0.660589  0.856380  0.878686  0.554381  0.662029  \n",
      "40   0.689682  0.473992  0.474036  0.660193  0.683761  0.383583  0.475075  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.832299  0.621702  0.622204  0.804826  0.826854  0.523149  0.623623  \n",
      "67   0.774248  0.548702  0.549109  0.743937  0.768024  0.453115  0.550270  \n",
      "117  0.882351  0.597866  0.598332  0.844885  0.874686  0.478717  0.599833  \n",
      "47   0.791090  0.541813  0.542462  0.757242  0.784283  0.436769  0.543664  \n",
      "172  0.804446  0.578967  0.579635  0.775057  0.798647  0.476436  0.581031  \n",
      "\n",
      "[130 rows x 51 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1269.1    1369.6    1370.4  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549698  0.713986  0.715977   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547007  0.788922  0.792114   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681515  0.920578  0.923226   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660256  0.881742  0.884380   \n",
      "40   0.410373  0.415739  0.370542  ...  0.473737  0.686927  0.689682   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.621776  0.829722  0.832299   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548525  0.771319  0.774248   \n",
      "117  0.513116  0.519892  0.462708  ...  0.597721  0.878702  0.882351   \n",
      "47   0.468146  0.474372  0.421441  ...  0.541820  0.787893  0.791090   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579034  0.801684  0.804446   \n",
      "\n",
      "       1263.5    1271.6    1361.7    1368.9    1135.7    1276.6    1268.5  \n",
      "16   0.549830  0.550016  0.693790  0.711526  0.475591  0.551102  0.549582  \n",
      "51   0.547065  0.547453  0.758795  0.785335  0.441615  0.548631  0.546941  \n",
      "183  0.681209  0.681871  0.893843  0.917398  0.559448  0.683768  0.681472  \n",
      "145  0.660099  0.660589  0.856380  0.878686  0.554381  0.662029  0.660156  \n",
      "40   0.473992  0.474036  0.660193  0.683761  0.383583  0.475075  0.473719  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.621702  0.622204  0.804826  0.826854  0.523149  0.623623  0.621813  \n",
      "67   0.548702  0.549109  0.743937  0.768024  0.453115  0.550270  0.548529  \n",
      "117  0.597866  0.598332  0.844885  0.874686  0.478717  0.599833  0.597667  \n",
      "47   0.541813  0.542462  0.757242  0.784283  0.436769  0.543664  0.541811  \n",
      "172  0.578967  0.579635  0.775057  0.798647  0.476436  0.581031  0.579060  \n",
      "\n",
      "[130 rows x 52 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1369.6    1370.4    1263.5  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.713986  0.715977  0.549830   \n",
      "51   0.472565  0.478432  0.426489  ...  0.788922  0.792114  0.547065   \n",
      "183  0.596755  0.603650  0.540704  ...  0.920578  0.923226  0.681209   \n",
      "145  0.586613  0.592968  0.538299  ...  0.881742  0.884380  0.660099   \n",
      "40   0.410373  0.415739  0.370542  ...  0.686927  0.689682  0.473992   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.829722  0.832299  0.621702   \n",
      "67   0.481982  0.487460  0.439093  ...  0.771319  0.774248  0.548702   \n",
      "117  0.513116  0.519892  0.462708  ...  0.878702  0.882351  0.597866   \n",
      "47   0.468146  0.474372  0.421441  ...  0.787893  0.791090  0.541813   \n",
      "172  0.507965  0.513851  0.460731  ...  0.801684  0.804446  0.578967   \n",
      "\n",
      "       1271.6    1361.7    1368.9    1135.7    1276.6    1268.5    1269.7  \n",
      "16   0.550016  0.693790  0.711526  0.475591  0.551102  0.549582  0.549844  \n",
      "51   0.547453  0.758795  0.785335  0.441615  0.548631  0.546941  0.547136  \n",
      "183  0.681871  0.893843  0.917398  0.559448  0.683768  0.681472  0.681520  \n",
      "145  0.660589  0.856380  0.878686  0.554381  0.662029  0.660156  0.660257  \n",
      "40   0.474036  0.660193  0.683761  0.383583  0.475075  0.473719  0.473731  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.622204  0.804826  0.826854  0.523149  0.623623  0.621813  0.621919  \n",
      "67   0.549109  0.743937  0.768024  0.453115  0.550270  0.548529  0.548668  \n",
      "117  0.598332  0.844885  0.874686  0.478717  0.599833  0.597667  0.597910  \n",
      "47   0.542462  0.757242  0.784283  0.436769  0.543664  0.541811  0.541994  \n",
      "172  0.579635  0.775057  0.798647  0.476436  0.581031  0.579060  0.579085  \n",
      "\n",
      "[130 rows x 53 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1370.4    1263.5    1271.6  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.715977  0.549830  0.550016   \n",
      "51   0.472565  0.478432  0.426489  ...  0.792114  0.547065  0.547453   \n",
      "183  0.596755  0.603650  0.540704  ...  0.923226  0.681209  0.681871   \n",
      "145  0.586613  0.592968  0.538299  ...  0.884380  0.660099  0.660589   \n",
      "40   0.410373  0.415739  0.370542  ...  0.689682  0.473992  0.474036   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.832299  0.621702  0.622204   \n",
      "67   0.481982  0.487460  0.439093  ...  0.774248  0.548702  0.549109   \n",
      "117  0.513116  0.519892  0.462708  ...  0.882351  0.597866  0.598332   \n",
      "47   0.468146  0.474372  0.421441  ...  0.791090  0.541813  0.542462   \n",
      "172  0.507965  0.513851  0.460731  ...  0.804446  0.578967  0.579635   \n",
      "\n",
      "       1361.7    1368.9    1135.7    1276.6    1268.5    1269.7    1264.7  \n",
      "16   0.693790  0.711526  0.475591  0.551102  0.549582  0.549844  0.549565  \n",
      "51   0.758795  0.785335  0.441615  0.548631  0.546941  0.547136  0.546914  \n",
      "183  0.893843  0.917398  0.559448  0.683768  0.681472  0.681520  0.681116  \n",
      "145  0.856380  0.878686  0.554381  0.662029  0.660156  0.660257  0.660021  \n",
      "40   0.660193  0.683761  0.383583  0.475075  0.473719  0.473731  0.473723  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.804826  0.826854  0.523149  0.623623  0.621813  0.621919  0.621675  \n",
      "67   0.743937  0.768024  0.453115  0.550270  0.548529  0.548668  0.548603  \n",
      "117  0.844885  0.874686  0.478717  0.599833  0.597667  0.597910  0.597707  \n",
      "47   0.757242  0.784283  0.436769  0.543664  0.541811  0.541994  0.541856  \n",
      "172  0.775057  0.798647  0.476436  0.581031  0.579060  0.579085  0.578964  \n",
      "\n",
      "[130 rows x 54 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1263.5    1271.6    1361.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549830  0.550016  0.693790   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547065  0.547453  0.758795   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681209  0.681871  0.893843   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660099  0.660589  0.856380   \n",
      "40   0.410373  0.415739  0.370542  ...  0.473992  0.474036  0.660193   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.621702  0.622204  0.804826   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548702  0.549109  0.743937   \n",
      "117  0.513116  0.519892  0.462708  ...  0.597866  0.598332  0.844885   \n",
      "47   0.468146  0.474372  0.421441  ...  0.541813  0.542462  0.757242   \n",
      "172  0.507965  0.513851  0.460731  ...  0.578967  0.579635  0.775057   \n",
      "\n",
      "       1368.9    1135.7    1276.6    1268.5    1269.7    1264.7    1261.7  \n",
      "16   0.711526  0.475591  0.551102  0.549582  0.549844  0.549565  0.549947  \n",
      "51   0.785335  0.441615  0.548631  0.546941  0.547136  0.546914  0.547226  \n",
      "183  0.917398  0.559448  0.683768  0.681472  0.681520  0.681116  0.681493  \n",
      "145  0.878686  0.554381  0.662029  0.660156  0.660257  0.660021  0.660537  \n",
      "40   0.683761  0.383583  0.475075  0.473719  0.473731  0.473723  0.474150  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.826854  0.523149  0.623623  0.621813  0.621919  0.621675  0.622045  \n",
      "67   0.768024  0.453115  0.550270  0.548529  0.548668  0.548603  0.548968  \n",
      "117  0.874686  0.478717  0.599833  0.597667  0.597910  0.597707  0.598110  \n",
      "47   0.784283  0.436769  0.543664  0.541811  0.541994  0.541856  0.542155  \n",
      "172  0.798647  0.476436  0.581031  0.579060  0.579085  0.578964  0.579211  \n",
      "\n",
      "[130 rows x 55 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1271.6    1361.7    1368.9  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.550016  0.693790  0.711526   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547453  0.758795  0.785335   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681871  0.893843  0.917398   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660589  0.856380  0.878686   \n",
      "40   0.410373  0.415739  0.370542  ...  0.474036  0.660193  0.683761   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.622204  0.804826  0.826854   \n",
      "67   0.481982  0.487460  0.439093  ...  0.549109  0.743937  0.768024   \n",
      "117  0.513116  0.519892  0.462708  ...  0.598332  0.844885  0.874686   \n",
      "47   0.468146  0.474372  0.421441  ...  0.542462  0.757242  0.784283   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579635  0.775057  0.798647   \n",
      "\n",
      "       1135.7    1276.6    1268.5    1269.7    1264.7    1261.7    1134.7  \n",
      "16   0.475591  0.551102  0.549582  0.549844  0.549565  0.549947  0.472241  \n",
      "51   0.441615  0.548631  0.546941  0.547136  0.546914  0.547226  0.436671  \n",
      "183  0.559448  0.683768  0.681472  0.681520  0.681116  0.681493  0.553431  \n",
      "145  0.554381  0.662029  0.660156  0.660257  0.660021  0.660537  0.549406  \n",
      "40   0.383583  0.475075  0.473719  0.473731  0.473723  0.474150  0.379466  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.523149  0.623623  0.621813  0.621919  0.621675  0.622045  0.518372  \n",
      "67   0.453115  0.550270  0.548529  0.548668  0.548603  0.548968  0.448533  \n",
      "117  0.478717  0.599833  0.597667  0.597910  0.597707  0.598110  0.473634  \n",
      "47   0.436769  0.543664  0.541811  0.541994  0.541856  0.542155  0.431762  \n",
      "172  0.476436  0.581031  0.579060  0.579085  0.578964  0.579211  0.471407  \n",
      "\n",
      "[130 rows x 56 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1361.7    1368.9    1135.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.693790  0.711526  0.475591   \n",
      "51   0.472565  0.478432  0.426489  ...  0.758795  0.785335  0.441615   \n",
      "183  0.596755  0.603650  0.540704  ...  0.893843  0.917398  0.559448   \n",
      "145  0.586613  0.592968  0.538299  ...  0.856380  0.878686  0.554381   \n",
      "40   0.410373  0.415739  0.370542  ...  0.660193  0.683761  0.383583   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.804826  0.826854  0.523149   \n",
      "67   0.481982  0.487460  0.439093  ...  0.743937  0.768024  0.453115   \n",
      "117  0.513116  0.519892  0.462708  ...  0.844885  0.874686  0.478717   \n",
      "47   0.468146  0.474372  0.421441  ...  0.757242  0.784283  0.436769   \n",
      "172  0.507965  0.513851  0.460731  ...  0.775057  0.798647  0.476436   \n",
      "\n",
      "       1276.6    1268.5    1269.7    1264.7    1261.7    1134.7    1277.2  \n",
      "16   0.551102  0.549582  0.549844  0.549565  0.549947  0.472241  0.551170  \n",
      "51   0.548631  0.546941  0.547136  0.546914  0.547226  0.436671  0.548770  \n",
      "183  0.683768  0.681472  0.681520  0.681116  0.681493  0.553431  0.683853  \n",
      "145  0.662029  0.660156  0.660257  0.660021  0.660537  0.549406  0.662134  \n",
      "40   0.475075  0.473719  0.473731  0.473723  0.474150  0.379466  0.475181  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.623623  0.621813  0.621919  0.621675  0.622045  0.518372  0.623773  \n",
      "67   0.550270  0.548529  0.548668  0.548603  0.548968  0.448533  0.550447  \n",
      "117  0.599833  0.597667  0.597910  0.597707  0.598110  0.473634  0.599997  \n",
      "47   0.543664  0.541811  0.541994  0.541856  0.542155  0.431762  0.543901  \n",
      "172  0.581031  0.579060  0.579085  0.578964  0.579211  0.471407  0.581116  \n",
      "\n",
      "[130 rows x 57 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1368.9    1135.7    1276.6  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.711526  0.475591  0.551102   \n",
      "51   0.472565  0.478432  0.426489  ...  0.785335  0.441615  0.548631   \n",
      "183  0.596755  0.603650  0.540704  ...  0.917398  0.559448  0.683768   \n",
      "145  0.586613  0.592968  0.538299  ...  0.878686  0.554381  0.662029   \n",
      "40   0.410373  0.415739  0.370542  ...  0.683761  0.383583  0.475075   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.826854  0.523149  0.623623   \n",
      "67   0.481982  0.487460  0.439093  ...  0.768024  0.453115  0.550270   \n",
      "117  0.513116  0.519892  0.462708  ...  0.874686  0.478717  0.599833   \n",
      "47   0.468146  0.474372  0.421441  ...  0.784283  0.436769  0.543664   \n",
      "172  0.507965  0.513851  0.460731  ...  0.798647  0.476436  0.581031   \n",
      "\n",
      "       1268.5    1269.7    1264.7    1261.7    1134.7    1277.2      1662  \n",
      "16   0.549582  0.549844  0.549565  0.549947  0.472241  0.551170  0.816127  \n",
      "51   0.546941  0.547136  0.546914  0.547226  0.436671  0.548770  0.938917  \n",
      "183  0.681472  0.681520  0.681116  0.681493  0.553431  0.683853  1.039270  \n",
      "145  0.660156  0.660257  0.660021  0.660537  0.549406  0.662134  0.997032  \n",
      "40   0.473719  0.473731  0.473723  0.474150  0.379466  0.475181  0.831448  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.621813  0.621919  0.621675  0.622045  0.518372  0.623773  0.947411  \n",
      "67   0.548529  0.548668  0.548603  0.548968  0.448533  0.550447  0.897319  \n",
      "117  0.597667  0.597910  0.597707  0.598110  0.473634  0.599997  1.046166  \n",
      "47   0.541811  0.541994  0.541856  0.542155  0.431762  0.543901  0.934122  \n",
      "172  0.579060  0.579085  0.578964  0.579211  0.471407  0.581116  0.917807  \n",
      "\n",
      "[130 rows x 58 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1135.7    1276.6    1268.5  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.475591  0.551102  0.549582   \n",
      "51   0.472565  0.478432  0.426489  ...  0.441615  0.548631  0.546941   \n",
      "183  0.596755  0.603650  0.540704  ...  0.559448  0.683768  0.681472   \n",
      "145  0.586613  0.592968  0.538299  ...  0.554381  0.662029  0.660156   \n",
      "40   0.410373  0.415739  0.370542  ...  0.383583  0.475075  0.473719   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.523149  0.623623  0.621813   \n",
      "67   0.481982  0.487460  0.439093  ...  0.453115  0.550270  0.548529   \n",
      "117  0.513116  0.519892  0.462708  ...  0.478717  0.599833  0.597667   \n",
      "47   0.468146  0.474372  0.421441  ...  0.436769  0.543664  0.541811   \n",
      "172  0.507965  0.513851  0.460731  ...  0.476436  0.581031  0.579060   \n",
      "\n",
      "       1269.7    1264.7    1261.7    1134.7    1277.2      1662    1274.1  \n",
      "16   0.549844  0.549565  0.549947  0.472241  0.551170  0.816127  0.550584  \n",
      "51   0.547136  0.546914  0.547226  0.436671  0.548770  0.938917  0.548039  \n",
      "183  0.681520  0.681116  0.681493  0.553431  0.683853  1.039270  0.682709  \n",
      "145  0.660257  0.660021  0.660537  0.549406  0.662134  0.997032  0.661193  \n",
      "40   0.473731  0.473723  0.474150  0.379466  0.475181  0.831448  0.474422  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.621919  0.621675  0.622045  0.518372  0.623773  0.947411  0.623078  \n",
      "67   0.548668  0.548603  0.548968  0.448533  0.550447  0.897319  0.549624  \n",
      "117  0.597910  0.597707  0.598110  0.473634  0.599997  1.046166  0.599021  \n",
      "47   0.541994  0.541856  0.542155  0.431762  0.543901  0.934122  0.542917  \n",
      "172  0.579085  0.578964  0.579211  0.471407  0.581116  0.917807  0.580161  \n",
      "\n",
      "[130 rows x 59 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1276.6    1268.5    1269.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.551102  0.549582  0.549844   \n",
      "51   0.472565  0.478432  0.426489  ...  0.548631  0.546941  0.547136   \n",
      "183  0.596755  0.603650  0.540704  ...  0.683768  0.681472  0.681520   \n",
      "145  0.586613  0.592968  0.538299  ...  0.662029  0.660156  0.660257   \n",
      "40   0.410373  0.415739  0.370542  ...  0.475075  0.473719  0.473731   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.623623  0.621813  0.621919   \n",
      "67   0.481982  0.487460  0.439093  ...  0.550270  0.548529  0.548668   \n",
      "117  0.513116  0.519892  0.462708  ...  0.599833  0.597667  0.597910   \n",
      "47   0.468146  0.474372  0.421441  ...  0.543664  0.541811  0.541994   \n",
      "172  0.507965  0.513851  0.460731  ...  0.581031  0.579060  0.579085   \n",
      "\n",
      "       1264.7    1261.7    1134.7    1277.2      1662    1274.1    1264.1  \n",
      "16   0.549565  0.549947  0.472241  0.551170  0.816127  0.550584  0.549701  \n",
      "51   0.546914  0.547226  0.436671  0.548770  0.938917  0.548039  0.546952  \n",
      "183  0.681116  0.681493  0.553431  0.683853  1.039270  0.682709  0.681100  \n",
      "145  0.660021  0.660537  0.549406  0.662134  0.997032  0.661193  0.660056  \n",
      "40   0.473723  0.474150  0.379466  0.475181  0.831448  0.474422  0.473854  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.621675  0.622045  0.518372  0.623773  0.947411  0.623078  0.621735  \n",
      "67   0.548603  0.548968  0.448533  0.550447  0.897319  0.549624  0.548633  \n",
      "117  0.597707  0.598110  0.473634  0.599997  1.046166  0.599021  0.597819  \n",
      "47   0.541856  0.542155  0.431762  0.543901  0.934122  0.542917  0.541870  \n",
      "172  0.578964  0.579211  0.471407  0.581116  0.917807  0.580161  0.579067  \n",
      "\n",
      "[130 rows x 60 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1268.5    1269.7    1264.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549582  0.549844  0.549565   \n",
      "51   0.472565  0.478432  0.426489  ...  0.546941  0.547136  0.546914   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681472  0.681520  0.681116   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660156  0.660257  0.660021   \n",
      "40   0.410373  0.415739  0.370542  ...  0.473719  0.473731  0.473723   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.621813  0.621919  0.621675   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548529  0.548668  0.548603   \n",
      "117  0.513116  0.519892  0.462708  ...  0.597667  0.597910  0.597707   \n",
      "47   0.468146  0.474372  0.421441  ...  0.541811  0.541994  0.541856   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579060  0.579085  0.578964   \n",
      "\n",
      "       1261.7    1134.7    1277.2      1662    1274.1    1264.1    1279.7  \n",
      "16   0.549947  0.472241  0.551170  0.816127  0.550584  0.549701  0.551993  \n",
      "51   0.547226  0.436671  0.548770  0.938917  0.548039  0.546952  0.550015  \n",
      "183  0.681493  0.553431  0.683853  1.039270  0.682709  0.681100  0.685392  \n",
      "145  0.660537  0.549406  0.662134  0.997032  0.661193  0.660056  0.663382  \n",
      "40   0.474150  0.379466  0.475181  0.831448  0.474422  0.473854  0.476117  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.622045  0.518372  0.623773  0.947411  0.623078  0.621735  0.624964  \n",
      "67   0.548968  0.448533  0.550447  0.897319  0.549624  0.548633  0.551406  \n",
      "117  0.598110  0.473634  0.599997  1.046166  0.599021  0.597819  0.601445  \n",
      "47   0.542155  0.431762  0.543901  0.934122  0.542917  0.541870  0.545073  \n",
      "172  0.579211  0.471407  0.581116  0.917807  0.580161  0.579067  0.582375  \n",
      "\n",
      "[130 rows x 61 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1269.7    1264.7    1261.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549844  0.549565  0.549947   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547136  0.546914  0.547226   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681520  0.681116  0.681493   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660257  0.660021  0.660537   \n",
      "40   0.410373  0.415739  0.370542  ...  0.473731  0.473723  0.474150   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.621919  0.621675  0.622045   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548668  0.548603  0.548968   \n",
      "117  0.513116  0.519892  0.462708  ...  0.597910  0.597707  0.598110   \n",
      "47   0.468146  0.474372  0.421441  ...  0.541994  0.541856  0.542155   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579085  0.578964  0.579211   \n",
      "\n",
      "       1134.7    1277.2      1662    1274.1    1264.1    1279.7    1262.9  \n",
      "16   0.472241  0.551170  0.816127  0.550584  0.549701  0.551993  0.549946  \n",
      "51   0.436671  0.548770  0.938917  0.548039  0.546952  0.550015  0.547346  \n",
      "183  0.553431  0.683853  1.039270  0.682709  0.681100  0.685392  0.681444  \n",
      "145  0.549406  0.662134  0.997032  0.661193  0.660056  0.663382  0.660292  \n",
      "40   0.379466  0.475181  0.831448  0.474422  0.473854  0.476117  0.474131  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.518372  0.623773  0.947411  0.623078  0.621735  0.624964  0.621965  \n",
      "67   0.448533  0.550447  0.897319  0.549624  0.548633  0.551406  0.549034  \n",
      "117  0.473634  0.599997  1.046166  0.599021  0.597819  0.601445  0.598131  \n",
      "47   0.431762  0.543901  0.934122  0.542917  0.541870  0.545073  0.542109  \n",
      "172  0.471407  0.581116  0.917807  0.580161  0.579067  0.582375  0.579150  \n",
      "\n",
      "[130 rows x 62 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1264.7    1261.7    1134.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549565  0.549947  0.472241   \n",
      "51   0.472565  0.478432  0.426489  ...  0.546914  0.547226  0.436671   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681116  0.681493  0.553431   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660021  0.660537  0.549406   \n",
      "40   0.410373  0.415739  0.370542  ...  0.473723  0.474150  0.379466   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.621675  0.622045  0.518372   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548603  0.548968  0.448533   \n",
      "117  0.513116  0.519892  0.462708  ...  0.597707  0.598110  0.473634   \n",
      "47   0.468146  0.474372  0.421441  ...  0.541856  0.542155  0.431762   \n",
      "172  0.507965  0.513851  0.460731  ...  0.578964  0.579211  0.471407   \n",
      "\n",
      "       1277.2      1662    1274.1    1264.1    1279.7    1262.9    1135.2  \n",
      "16   0.551170  0.816127  0.550584  0.549701  0.551993  0.549946  0.473717  \n",
      "51   0.548770  0.938917  0.548039  0.546952  0.550015  0.547346  0.438979  \n",
      "183  0.683853  1.039270  0.682709  0.681100  0.685392  0.681444  0.556434  \n",
      "145  0.662134  0.997032  0.661193  0.660056  0.663382  0.660292  0.551777  \n",
      "40   0.475181  0.831448  0.474422  0.473854  0.476117  0.474131  0.381374  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.623773  0.947411  0.623078  0.621735  0.624964  0.621965  0.520782  \n",
      "67   0.550447  0.897319  0.549624  0.548633  0.551406  0.549034  0.450746  \n",
      "117  0.599997  1.046166  0.599021  0.597819  0.601445  0.598131  0.476062  \n",
      "47   0.543901  0.934122  0.542917  0.541870  0.545073  0.542109  0.434197  \n",
      "172  0.581116  0.917807  0.580161  0.579067  0.582375  0.579150  0.474023  \n",
      "\n",
      "[130 rows x 63 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1261.7    1134.7    1277.2  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549947  0.472241  0.551170   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547226  0.436671  0.548770   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681493  0.553431  0.683853   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660537  0.549406  0.662134   \n",
      "40   0.410373  0.415739  0.370542  ...  0.474150  0.379466  0.475181   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.622045  0.518372  0.623773   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548968  0.448533  0.550447   \n",
      "117  0.513116  0.519892  0.462708  ...  0.598110  0.473634  0.599997   \n",
      "47   0.468146  0.474372  0.421441  ...  0.542155  0.431762  0.543901   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579211  0.471407  0.581116   \n",
      "\n",
      "         1662    1274.1    1264.1    1279.7    1262.9    1135.2    1659.9  \n",
      "16   0.816127  0.550584  0.549701  0.551993  0.549946  0.473717  0.816565  \n",
      "51   0.938917  0.548039  0.546952  0.550015  0.547346  0.438979  0.939844  \n",
      "183  1.039270  0.682709  0.681100  0.685392  0.681444  0.556434  1.039853  \n",
      "145  0.997032  0.661193  0.660056  0.663382  0.660292  0.551777  0.997713  \n",
      "40   0.831448  0.474422  0.473854  0.476117  0.474131  0.381374  0.831430  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.947411  0.623078  0.621735  0.624964  0.621965  0.520782  0.948192  \n",
      "67   0.897319  0.549624  0.548633  0.551406  0.549034  0.450746  0.898096  \n",
      "117  1.046166  0.599021  0.597819  0.601445  0.598131  0.476062  1.047181  \n",
      "47   0.934122  0.542917  0.541870  0.545073  0.542109  0.434197  0.935377  \n",
      "172  0.917807  0.580161  0.579067  0.582375  0.579150  0.474023  0.918415  \n",
      "\n",
      "[130 rows x 64 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1134.7    1277.2      1662  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.472241  0.551170  0.816127   \n",
      "51   0.472565  0.478432  0.426489  ...  0.436671  0.548770  0.938917   \n",
      "183  0.596755  0.603650  0.540704  ...  0.553431  0.683853  1.039270   \n",
      "145  0.586613  0.592968  0.538299  ...  0.549406  0.662134  0.997032   \n",
      "40   0.410373  0.415739  0.370542  ...  0.379466  0.475181  0.831448   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.518372  0.623773  0.947411   \n",
      "67   0.481982  0.487460  0.439093  ...  0.448533  0.550447  0.897319   \n",
      "117  0.513116  0.519892  0.462708  ...  0.473634  0.599997  1.046166   \n",
      "47   0.468146  0.474372  0.421441  ...  0.431762  0.543901  0.934122   \n",
      "172  0.507965  0.513851  0.460731  ...  0.471407  0.581116  0.917807   \n",
      "\n",
      "       1274.1    1264.1    1279.7    1262.9    1135.2    1659.9    1660.9  \n",
      "16   0.550584  0.549701  0.551993  0.549946  0.473717  0.816565  0.816455  \n",
      "51   0.548039  0.546952  0.550015  0.547346  0.438979  0.939844  0.939426  \n",
      "183  0.682709  0.681100  0.685392  0.681444  0.556434  1.039853  1.039518  \n",
      "145  0.661193  0.660056  0.663382  0.660292  0.551777  0.997713  0.997298  \n",
      "40   0.474422  0.473854  0.476117  0.474131  0.381374  0.831430  0.831462  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.623078  0.621735  0.624964  0.621965  0.520782  0.948192  0.947737  \n",
      "67   0.549624  0.548633  0.551406  0.549034  0.450746  0.898096  0.897640  \n",
      "117  0.599021  0.597819  0.601445  0.598131  0.476062  1.047181  1.046762  \n",
      "47   0.542917  0.541870  0.545073  0.542109  0.434197  0.935377  0.934732  \n",
      "172  0.580161  0.579067  0.582375  0.579150  0.474023  0.918415  0.918124  \n",
      "\n",
      "[130 rows x 65 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1277.2      1662    1274.1  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.551170  0.816127  0.550584   \n",
      "51   0.472565  0.478432  0.426489  ...  0.548770  0.938917  0.548039   \n",
      "183  0.596755  0.603650  0.540704  ...  0.683853  1.039270  0.682709   \n",
      "145  0.586613  0.592968  0.538299  ...  0.662134  0.997032  0.661193   \n",
      "40   0.410373  0.415739  0.370542  ...  0.475181  0.831448  0.474422   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.623773  0.947411  0.623078   \n",
      "67   0.481982  0.487460  0.439093  ...  0.550447  0.897319  0.549624   \n",
      "117  0.513116  0.519892  0.462708  ...  0.599997  1.046166  0.599021   \n",
      "47   0.468146  0.474372  0.421441  ...  0.543901  0.934122  0.542917   \n",
      "172  0.507965  0.513851  0.460731  ...  0.581116  0.917807  0.580161   \n",
      "\n",
      "       1264.1    1279.7    1262.9    1135.2    1659.9    1660.9    1130.7  \n",
      "16   0.549701  0.551993  0.549946  0.473717  0.816565  0.816455  0.460977  \n",
      "51   0.546952  0.550015  0.547346  0.438979  0.939844  0.939426  0.420861  \n",
      "183  0.681100  0.685392  0.681444  0.556434  1.039853  1.039518  0.534176  \n",
      "145  0.660056  0.663382  0.660292  0.551777  0.997713  0.997298  0.532737  \n",
      "40   0.473854  0.476117  0.474131  0.381374  0.831430  0.831462  0.365912  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.621735  0.624964  0.621965  0.520782  0.948192  0.947737  0.503916  \n",
      "67   0.548633  0.551406  0.549034  0.450746  0.898096  0.897640  0.434060  \n",
      "117  0.597819  0.601445  0.598131  0.476062  1.047181  1.046762  0.456727  \n",
      "47   0.541870  0.545073  0.542109  0.434197  0.935377  0.934732  0.416148  \n",
      "172  0.579067  0.582375  0.579150  0.474023  0.918415  0.918124  0.455169  \n",
      "\n",
      "[130 rows x 66 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...      1662    1274.1    1264.1  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.816127  0.550584  0.549701   \n",
      "51   0.472565  0.478432  0.426489  ...  0.938917  0.548039  0.546952   \n",
      "183  0.596755  0.603650  0.540704  ...  1.039270  0.682709  0.681100   \n",
      "145  0.586613  0.592968  0.538299  ...  0.997032  0.661193  0.660056   \n",
      "40   0.410373  0.415739  0.370542  ...  0.831448  0.474422  0.473854   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.947411  0.623078  0.621735   \n",
      "67   0.481982  0.487460  0.439093  ...  0.897319  0.549624  0.548633   \n",
      "117  0.513116  0.519892  0.462708  ...  1.046166  0.599021  0.597819   \n",
      "47   0.468146  0.474372  0.421441  ...  0.934122  0.542917  0.541870   \n",
      "172  0.507965  0.513851  0.460731  ...  0.917807  0.580161  0.579067   \n",
      "\n",
      "       1279.7    1262.9    1135.2    1659.9    1660.9    1130.7    1136.2  \n",
      "16   0.551993  0.549946  0.473717  0.816565  0.816455  0.460977  0.477509  \n",
      "51   0.550015  0.547346  0.438979  0.939844  0.939426  0.420861  0.444114  \n",
      "183  0.685392  0.681444  0.556434  1.039853  1.039518  0.534176  0.562348  \n",
      "145  0.663382  0.660292  0.551777  0.997713  0.997298  0.532737  0.556938  \n",
      "40   0.476117  0.474131  0.381374  0.831430  0.831462  0.365912  0.385705  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.624964  0.621965  0.520782  0.948192  0.947737  0.503916  0.525388  \n",
      "67   0.551406  0.549034  0.450746  0.898096  0.897640  0.434060  0.455317  \n",
      "117  0.601445  0.598131  0.476062  1.047181  1.046762  0.456727  0.481483  \n",
      "47   0.545073  0.542109  0.434197  0.935377  0.934732  0.416148  0.439357  \n",
      "172  0.582375  0.579150  0.474023  0.918415  0.918124  0.455169  0.478807  \n",
      "\n",
      "[130 rows x 67 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1274.1    1264.1    1279.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.550584  0.549701  0.551993   \n",
      "51   0.472565  0.478432  0.426489  ...  0.548039  0.546952  0.550015   \n",
      "183  0.596755  0.603650  0.540704  ...  0.682709  0.681100  0.685392   \n",
      "145  0.586613  0.592968  0.538299  ...  0.661193  0.660056  0.663382   \n",
      "40   0.410373  0.415739  0.370542  ...  0.474422  0.473854  0.476117   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.623078  0.621735  0.624964   \n",
      "67   0.481982  0.487460  0.439093  ...  0.549624  0.548633  0.551406   \n",
      "117  0.513116  0.519892  0.462708  ...  0.599021  0.597819  0.601445   \n",
      "47   0.468146  0.474372  0.421441  ...  0.542917  0.541870  0.545073   \n",
      "172  0.507965  0.513851  0.460731  ...  0.580161  0.579067  0.582375   \n",
      "\n",
      "       1262.9    1135.2    1659.9    1660.9    1130.7    1136.2    1632.7  \n",
      "16   0.549946  0.473717  0.816565  0.816455  0.460977  0.477509  0.825643  \n",
      "51   0.547346  0.438979  0.939844  0.939426  0.420861  0.444114  0.957516  \n",
      "183  0.681444  0.556434  1.039853  1.039518  0.534176  0.562348  1.053193  \n",
      "145  0.660292  0.551777  0.997713  0.997298  0.532737  0.556938  1.010642  \n",
      "40   0.474131  0.381374  0.831430  0.831462  0.365912  0.385705  0.844430  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.621965  0.520782  0.948192  0.947737  0.503916  0.525388  0.961921  \n",
      "67   0.549034  0.450746  0.898096  0.897640  0.434060  0.455317  0.914183  \n",
      "117  0.598131  0.476062  1.047181  1.046762  0.456727  0.481483  1.065781  \n",
      "47   0.542109  0.434197  0.935377  0.934732  0.416148  0.439357  0.953086  \n",
      "172  0.579150  0.474023  0.918415  0.918124  0.455169  0.478807  0.933367  \n",
      "\n",
      "[130 rows x 68 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1264.1    1279.7    1262.9  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549701  0.551993  0.549946   \n",
      "51   0.472565  0.478432  0.426489  ...  0.546952  0.550015  0.547346   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681100  0.685392  0.681444   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660056  0.663382  0.660292   \n",
      "40   0.410373  0.415739  0.370542  ...  0.473854  0.476117  0.474131   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.621735  0.624964  0.621965   \n",
      "67   0.481982  0.487460  0.439093  ...  0.548633  0.551406  0.549034   \n",
      "117  0.513116  0.519892  0.462708  ...  0.597819  0.601445  0.598131   \n",
      "47   0.468146  0.474372  0.421441  ...  0.541870  0.545073  0.542109   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579067  0.582375  0.579150   \n",
      "\n",
      "       1135.2    1659.9    1660.9    1130.7    1136.2    1632.7    1663.1  \n",
      "16   0.473717  0.816565  0.816455  0.460977  0.477509  0.825643  0.815775  \n",
      "51   0.438979  0.939844  0.939426  0.420861  0.444114  0.957516  0.938268  \n",
      "183  0.556434  1.039853  1.039518  0.534176  0.562348  1.053193  1.039016  \n",
      "145  0.551777  0.997713  0.997298  0.532737  0.556938  1.010642  0.996776  \n",
      "40   0.381374  0.831430  0.831462  0.365912  0.385705  0.844430  0.831332  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.520782  0.948192  0.947737  0.503916  0.525388  0.961921  0.947063  \n",
      "67   0.450746  0.898096  0.897640  0.434060  0.455317  0.914183  0.896946  \n",
      "117  0.476062  1.047181  1.046762  0.456727  0.481483  1.065781  1.045577  \n",
      "47   0.434197  0.935377  0.934732  0.416148  0.439357  0.953086  0.933673  \n",
      "172  0.474023  0.918415  0.918124  0.455169  0.478807  0.933367  0.917556  \n",
      "\n",
      "[130 rows x 69 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1279.7    1262.9    1135.2  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.551993  0.549946  0.473717   \n",
      "51   0.472565  0.478432  0.426489  ...  0.550015  0.547346  0.438979   \n",
      "183  0.596755  0.603650  0.540704  ...  0.685392  0.681444  0.556434   \n",
      "145  0.586613  0.592968  0.538299  ...  0.663382  0.660292  0.551777   \n",
      "40   0.410373  0.415739  0.370542  ...  0.476117  0.474131  0.381374   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.624964  0.621965  0.520782   \n",
      "67   0.481982  0.487460  0.439093  ...  0.551406  0.549034  0.450746   \n",
      "117  0.513116  0.519892  0.462708  ...  0.601445  0.598131  0.476062   \n",
      "47   0.468146  0.474372  0.421441  ...  0.545073  0.542109  0.434197   \n",
      "172  0.507965  0.513851  0.460731  ...  0.582375  0.579150  0.474023   \n",
      "\n",
      "       1659.9    1660.9    1130.7    1136.2    1632.7    1663.1    1368.2  \n",
      "16   0.816565  0.816455  0.460977  0.477509  0.825643  0.815775  0.709213  \n",
      "51   0.939844  0.939426  0.420861  0.444114  0.957516  0.938268  0.781804  \n",
      "183  1.039853  1.039518  0.534176  0.562348  1.053193  1.039016  0.914292  \n",
      "145  0.997713  0.997298  0.532737  0.556938  1.010642  0.996776  0.875728  \n",
      "40   0.831430  0.831462  0.365912  0.385705  0.844430  0.831332  0.680796  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.948192  0.947737  0.503916  0.525388  0.961921  0.947063  0.824004  \n",
      "67   0.898096  0.897640  0.434060  0.455317  0.914183  0.896946  0.765013  \n",
      "117  1.047181  1.046762  0.456727  0.481483  1.065781  1.045577  0.870814  \n",
      "47   0.935377  0.934732  0.416148  0.439357  0.953086  0.933673  0.780815  \n",
      "172  0.918415  0.918124  0.455169  0.478807  0.933367  0.917556  0.795560  \n",
      "\n",
      "[130 rows x 70 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1262.9    1135.2    1659.9  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549946  0.473717  0.816565   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547346  0.438979  0.939844   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681444  0.556434  1.039853   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660292  0.551777  0.997713   \n",
      "40   0.410373  0.415739  0.370542  ...  0.474131  0.381374  0.831430   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.621965  0.520782  0.948192   \n",
      "67   0.481982  0.487460  0.439093  ...  0.549034  0.450746  0.898096   \n",
      "117  0.513116  0.519892  0.462708  ...  0.598131  0.476062  1.047181   \n",
      "47   0.468146  0.474372  0.421441  ...  0.542109  0.434197  0.935377   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579150  0.474023  0.918415   \n",
      "\n",
      "       1660.9    1130.7    1136.2    1632.7    1663.1    1368.2    1279.1  \n",
      "16   0.816455  0.460977  0.477509  0.825643  0.815775  0.709213  0.551628  \n",
      "51   0.939426  0.420861  0.444114  0.957516  0.938268  0.781804  0.549543  \n",
      "183  1.039518  0.534176  0.562348  1.053193  1.039016  0.914292  0.684944  \n",
      "145  0.997298  0.532737  0.556938  1.010642  0.996776  0.875728  0.662925  \n",
      "40   0.831462  0.365912  0.385705  0.844430  0.831332  0.680796  0.475828  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.947737  0.503916  0.525388  0.961921  0.947063  0.824004  0.624473  \n",
      "67   0.897640  0.434060  0.455317  0.914183  0.896946  0.765013  0.550994  \n",
      "117  1.046762  0.456727  0.481483  1.065781  1.045577  0.870814  0.600912  \n",
      "47   0.934732  0.416148  0.439357  0.953086  0.933673  0.780815  0.544576  \n",
      "172  0.918124  0.455169  0.478807  0.933367  0.917556  0.795560  0.581963  \n",
      "\n",
      "[130 rows x 71 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1135.2    1659.9    1660.9  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.473717  0.816565  0.816455   \n",
      "51   0.472565  0.478432  0.426489  ...  0.438979  0.939844  0.939426   \n",
      "183  0.596755  0.603650  0.540704  ...  0.556434  1.039853  1.039518   \n",
      "145  0.586613  0.592968  0.538299  ...  0.551777  0.997713  0.997298   \n",
      "40   0.410373  0.415739  0.370542  ...  0.381374  0.831430  0.831462   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.520782  0.948192  0.947737   \n",
      "67   0.481982  0.487460  0.439093  ...  0.450746  0.898096  0.897640   \n",
      "117  0.513116  0.519892  0.462708  ...  0.476062  1.047181  1.046762   \n",
      "47   0.468146  0.474372  0.421441  ...  0.434197  0.935377  0.934732   \n",
      "172  0.507965  0.513851  0.460731  ...  0.474023  0.918415  0.918124   \n",
      "\n",
      "       1130.7    1136.2    1632.7    1663.1    1368.2    1279.1    1654.6  \n",
      "16   0.460977  0.477509  0.825643  0.815775  0.709213  0.551628  0.817738  \n",
      "51   0.420861  0.444114  0.957516  0.938268  0.781804  0.549543  0.942733  \n",
      "183  0.534176  0.562348  1.053193  1.039016  0.914292  0.684944  1.041901  \n",
      "145  0.532737  0.556938  1.010642  0.996776  0.875728  0.662925  0.999814  \n",
      "40   0.365912  0.385705  0.844430  0.831332  0.680796  0.475828  0.832277  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.503916  0.525388  0.961921  0.947063  0.824004  0.624473  0.950371  \n",
      "67   0.434060  0.455317  0.914183  0.896946  0.765013  0.550994  0.900577  \n",
      "117  0.456727  0.481483  1.065781  1.045577  0.870814  0.600912  1.050402  \n",
      "47   0.416148  0.439357  0.953086  0.933673  0.780815  0.544576  0.938201  \n",
      "172  0.455169  0.478807  0.933367  0.917556  0.795560  0.581963  0.920667  \n",
      "\n",
      "[130 rows x 72 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1659.9    1660.9    1130.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.816565  0.816455  0.460977   \n",
      "51   0.472565  0.478432  0.426489  ...  0.939844  0.939426  0.420861   \n",
      "183  0.596755  0.603650  0.540704  ...  1.039853  1.039518  0.534176   \n",
      "145  0.586613  0.592968  0.538299  ...  0.997713  0.997298  0.532737   \n",
      "40   0.410373  0.415739  0.370542  ...  0.831430  0.831462  0.365912   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.948192  0.947737  0.503916   \n",
      "67   0.481982  0.487460  0.439093  ...  0.898096  0.897640  0.434060   \n",
      "117  0.513116  0.519892  0.462708  ...  1.047181  1.046762  0.456727   \n",
      "47   0.468146  0.474372  0.421441  ...  0.935377  0.934732  0.416148   \n",
      "172  0.507965  0.513851  0.460731  ...  0.918415  0.918124  0.455169   \n",
      "\n",
      "       1136.2    1632.7    1663.1    1368.2    1279.1    1654.6    1349.7  \n",
      "16   0.477509  0.825643  0.815775  0.709213  0.551628  0.817738  0.672579  \n",
      "51   0.444114  0.957516  0.938268  0.781804  0.549543  0.942733  0.726582  \n",
      "183  0.562348  1.053193  1.039016  0.914292  0.684944  1.041901  0.864974  \n",
      "145  0.556938  1.010642  0.996776  0.875728  0.662925  0.999814  0.828170  \n",
      "40   0.385705  0.844430  0.831332  0.680796  0.475828  0.832277  0.630870  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.525388  0.961921  0.947063  0.824004  0.624473  0.950371  0.778571  \n",
      "67   0.455317  0.914183  0.896946  0.765013  0.550994  0.900577  0.714306  \n",
      "117  0.481483  1.065781  1.045577  0.870814  0.600912  1.050402  0.807537  \n",
      "47   0.439357  0.953086  0.933673  0.780815  0.544576  0.938201  0.724638  \n",
      "172  0.478807  0.933367  0.917556  0.795560  0.581963  0.920667  0.746815  \n",
      "\n",
      "[130 rows x 73 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1660.9    1130.7    1136.2  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.816455  0.460977  0.477509   \n",
      "51   0.472565  0.478432  0.426489  ...  0.939426  0.420861  0.444114   \n",
      "183  0.596755  0.603650  0.540704  ...  1.039518  0.534176  0.562348   \n",
      "145  0.586613  0.592968  0.538299  ...  0.997298  0.532737  0.556938   \n",
      "40   0.410373  0.415739  0.370542  ...  0.831462  0.365912  0.385705   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.947737  0.503916  0.525388   \n",
      "67   0.481982  0.487460  0.439093  ...  0.897640  0.434060  0.455317   \n",
      "117  0.513116  0.519892  0.462708  ...  1.046762  0.456727  0.481483   \n",
      "47   0.468146  0.474372  0.421441  ...  0.934732  0.416148  0.439357   \n",
      "172  0.507965  0.513851  0.460731  ...  0.918124  0.455169  0.478807   \n",
      "\n",
      "       1632.7    1663.1    1368.2    1279.1    1654.6    1349.7    1651.4  \n",
      "16   0.825643  0.815775  0.709213  0.551628  0.817738  0.672579  0.818291  \n",
      "51   0.957516  0.938268  0.781804  0.549543  0.942733  0.726582  0.944379  \n",
      "183  1.053193  1.039016  0.914292  0.684944  1.041901  0.864974  1.043173  \n",
      "145  1.010642  0.996776  0.875728  0.662925  0.999814  0.828170  1.000898  \n",
      "40   0.844430  0.831332  0.680796  0.475828  0.832277  0.630870  0.833061  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.961921  0.947063  0.824004  0.624473  0.950371  0.778571  0.951654  \n",
      "67   0.914183  0.896946  0.765013  0.550994  0.900577  0.714306  0.902195  \n",
      "117  1.065781  1.045577  0.870814  0.600912  1.050402  0.807537  1.052046  \n",
      "47   0.953086  0.933673  0.780815  0.544576  0.938201  0.724638  0.939872  \n",
      "172  0.933367  0.917556  0.795560  0.581963  0.920667  0.746815  0.922238  \n",
      "\n",
      "[130 rows x 74 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1130.7    1136.2    1632.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.460977  0.477509  0.825643   \n",
      "51   0.472565  0.478432  0.426489  ...  0.420861  0.444114  0.957516   \n",
      "183  0.596755  0.603650  0.540704  ...  0.534176  0.562348  1.053193   \n",
      "145  0.586613  0.592968  0.538299  ...  0.532737  0.556938  1.010642   \n",
      "40   0.410373  0.415739  0.370542  ...  0.365912  0.385705  0.844430   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.503916  0.525388  0.961921   \n",
      "67   0.481982  0.487460  0.439093  ...  0.434060  0.455317  0.914183   \n",
      "117  0.513116  0.519892  0.462708  ...  0.456727  0.481483  1.065781   \n",
      "47   0.468146  0.474372  0.421441  ...  0.416148  0.439357  0.953086   \n",
      "172  0.507965  0.513851  0.460731  ...  0.455169  0.478807  0.933367   \n",
      "\n",
      "       1663.1    1368.2    1279.1    1654.6    1349.7    1651.4    1664.1  \n",
      "16   0.815775  0.709213  0.551628  0.817738  0.672579  0.818291  0.815499  \n",
      "51   0.938268  0.781804  0.549543  0.942733  0.726582  0.944379  0.937618  \n",
      "183  1.039016  0.914292  0.684944  1.041901  0.864974  1.043173  1.038752  \n",
      "145  0.996776  0.875728  0.662925  0.999814  0.828170  1.000898  0.996390  \n",
      "40   0.831332  0.680796  0.475828  0.832277  0.630870  0.833061  0.831374  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.947063  0.824004  0.624473  0.950371  0.778571  0.951654  0.946719  \n",
      "67   0.896946  0.765013  0.550994  0.900577  0.714306  0.902195  0.896501  \n",
      "117  1.045577  0.870814  0.600912  1.050402  0.807537  1.052046  1.045060  \n",
      "47   0.933673  0.780815  0.544576  0.938201  0.724638  0.939872  0.933124  \n",
      "172  0.917556  0.795560  0.581963  0.920667  0.746815  0.922238  0.917195  \n",
      "\n",
      "[130 rows x 75 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1136.2    1632.7    1663.1  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.477509  0.825643  0.815775   \n",
      "51   0.472565  0.478432  0.426489  ...  0.444114  0.957516  0.938268   \n",
      "183  0.596755  0.603650  0.540704  ...  0.562348  1.053193  1.039016   \n",
      "145  0.586613  0.592968  0.538299  ...  0.556938  1.010642  0.996776   \n",
      "40   0.410373  0.415739  0.370542  ...  0.385705  0.844430  0.831332   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.525388  0.961921  0.947063   \n",
      "67   0.481982  0.487460  0.439093  ...  0.455317  0.914183  0.896946   \n",
      "117  0.513116  0.519892  0.462708  ...  0.481483  1.065781  1.045577   \n",
      "47   0.468146  0.474372  0.421441  ...  0.439357  0.953086  0.933673   \n",
      "172  0.507965  0.513851  0.460731  ...  0.478807  0.933367  0.917556   \n",
      "\n",
      "       1368.2    1279.1    1654.6    1349.7    1651.4    1664.1    1631.7  \n",
      "16   0.709213  0.551628  0.817738  0.672579  0.818291  0.815499  0.826209  \n",
      "51   0.781804  0.549543  0.942733  0.726582  0.944379  0.937618  0.958695  \n",
      "183  0.914292  0.684944  1.041901  0.864974  1.043173  1.038752  1.053865  \n",
      "145  0.875728  0.662925  0.999814  0.828170  1.000898  0.996390  1.011491  \n",
      "40   0.680796  0.475828  0.832277  0.630870  0.833061  0.831374  0.845410  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.824004  0.624473  0.950371  0.778571  0.951654  0.946719  0.962782  \n",
      "67   0.765013  0.550994  0.900577  0.714306  0.902195  0.896501  0.915029  \n",
      "117  0.870814  0.600912  1.050402  0.807537  1.052046  1.045060  1.066848  \n",
      "47   0.780815  0.544576  0.938201  0.724638  0.939872  0.933124  0.954226  \n",
      "172  0.795560  0.581963  0.920667  0.746815  0.922238  0.917195  0.934193  \n",
      "\n",
      "[130 rows x 76 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1632.7    1663.1    1368.2  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.825643  0.815775  0.709213   \n",
      "51   0.472565  0.478432  0.426489  ...  0.957516  0.938268  0.781804   \n",
      "183  0.596755  0.603650  0.540704  ...  1.053193  1.039016  0.914292   \n",
      "145  0.586613  0.592968  0.538299  ...  1.010642  0.996776  0.875728   \n",
      "40   0.410373  0.415739  0.370542  ...  0.844430  0.831332  0.680796   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.961921  0.947063  0.824004   \n",
      "67   0.481982  0.487460  0.439093  ...  0.914183  0.896946  0.765013   \n",
      "117  0.513116  0.519892  0.462708  ...  1.065781  1.045577  0.870814   \n",
      "47   0.468146  0.474372  0.421441  ...  0.953086  0.933673  0.780815   \n",
      "172  0.507965  0.513851  0.460731  ...  0.933367  0.917556  0.795560   \n",
      "\n",
      "       1279.1    1654.6    1349.7    1651.4    1664.1    1631.7    1657.8  \n",
      "16   0.551628  0.817738  0.672579  0.818291  0.815499  0.826209  0.816918  \n",
      "51   0.549543  0.942733  0.726582  0.944379  0.937618  0.958695  0.941025  \n",
      "183  0.684944  1.041901  0.864974  1.043173  1.038752  1.053865  1.040785  \n",
      "145  0.662925  0.999814  0.828170  1.000898  0.996390  1.011491  0.998529  \n",
      "40   0.475828  0.832277  0.630870  0.833061  0.831374  0.845410  0.831643  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.624473  0.950371  0.778571  0.951654  0.946719  0.962782  0.948999  \n",
      "67   0.550994  0.900577  0.714306  0.902195  0.896501  0.915029  0.899177  \n",
      "117  0.600912  1.050402  0.807537  1.052046  1.045060  1.066848  1.048548  \n",
      "47   0.544576  0.938201  0.724638  0.939872  0.933124  0.954226  0.936495  \n",
      "172  0.581963  0.920667  0.746815  0.922238  0.917195  0.934193  0.919344  \n",
      "\n",
      "[130 rows x 77 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1663.1    1368.2    1279.1  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.815775  0.709213  0.551628   \n",
      "51   0.472565  0.478432  0.426489  ...  0.938268  0.781804  0.549543   \n",
      "183  0.596755  0.603650  0.540704  ...  1.039016  0.914292  0.684944   \n",
      "145  0.586613  0.592968  0.538299  ...  0.996776  0.875728  0.662925   \n",
      "40   0.410373  0.415739  0.370542  ...  0.831332  0.680796  0.475828   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.947063  0.824004  0.624473   \n",
      "67   0.481982  0.487460  0.439093  ...  0.896946  0.765013  0.550994   \n",
      "117  0.513116  0.519892  0.462708  ...  1.045577  0.870814  0.600912   \n",
      "47   0.468146  0.474372  0.421441  ...  0.933673  0.780815  0.544576   \n",
      "172  0.507965  0.513851  0.460731  ...  0.917556  0.795560  0.581963   \n",
      "\n",
      "       1654.6    1349.7    1651.4    1664.1    1631.7    1657.8    1652.5  \n",
      "16   0.817738  0.672579  0.818291  0.815499  0.826209  0.816918  0.818068  \n",
      "51   0.942733  0.726582  0.944379  0.937618  0.958695  0.941025  0.943825  \n",
      "183  1.041901  0.864974  1.043173  1.038752  1.053865  1.040785  1.042806  \n",
      "145  0.999814  0.828170  1.000898  0.996390  1.011491  0.998529  1.000560  \n",
      "40   0.832277  0.630870  0.833061  0.831374  0.845410  0.831643  0.832835  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.950371  0.778571  0.951654  0.946719  0.962782  0.948999  0.951232  \n",
      "67   0.900577  0.714306  0.902195  0.896501  0.915029  0.899177  0.901635  \n",
      "117  1.050402  0.807537  1.052046  1.045060  1.066848  1.048548  1.051460  \n",
      "47   0.938201  0.724638  0.939872  0.933124  0.954226  0.936495  0.939293  \n",
      "172  0.920667  0.746815  0.922238  0.917195  0.934193  0.919344  0.921686  \n",
      "\n",
      "[130 rows x 78 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1368.2    1279.1    1654.6  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.709213  0.551628  0.817738   \n",
      "51   0.472565  0.478432  0.426489  ...  0.781804  0.549543  0.942733   \n",
      "183  0.596755  0.603650  0.540704  ...  0.914292  0.684944  1.041901   \n",
      "145  0.586613  0.592968  0.538299  ...  0.875728  0.662925  0.999814   \n",
      "40   0.410373  0.415739  0.370542  ...  0.680796  0.475828  0.832277   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.824004  0.624473  0.950371   \n",
      "67   0.481982  0.487460  0.439093  ...  0.765013  0.550994  0.900577   \n",
      "117  0.513116  0.519892  0.462708  ...  0.870814  0.600912  1.050402   \n",
      "47   0.468146  0.474372  0.421441  ...  0.780815  0.544576  0.938201   \n",
      "172  0.507965  0.513851  0.460731  ...  0.795560  0.581963  0.920667   \n",
      "\n",
      "       1349.7    1651.4    1664.1    1631.7    1657.8    1652.5    1658.8  \n",
      "16   0.672579  0.818291  0.815499  0.826209  0.816918  0.818068  0.816691  \n",
      "51   0.726582  0.944379  0.937618  0.958695  0.941025  0.943825  0.940328  \n",
      "183  0.864974  1.043173  1.038752  1.053865  1.040785  1.042806  1.040278  \n",
      "145  0.828170  1.000898  0.996390  1.011491  0.998529  1.000560  0.998091  \n",
      "40   0.630870  0.833061  0.831374  0.845410  0.831643  0.832835  0.831504  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.778571  0.951654  0.946719  0.962782  0.948999  0.951232  0.948635  \n",
      "67   0.714306  0.902195  0.896501  0.915029  0.899177  0.901635  0.898689  \n",
      "117  0.807537  1.052046  1.045060  1.066848  1.048548  1.051460  1.047815  \n",
      "47   0.724638  0.939872  0.933124  0.954226  0.936495  0.939293  0.935926  \n",
      "172  0.746815  0.922238  0.917195  0.934193  0.919344  0.921686  0.918728  \n",
      "\n",
      "[130 rows x 79 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1279.1    1654.6    1349.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.551628  0.817738  0.672579   \n",
      "51   0.472565  0.478432  0.426489  ...  0.549543  0.942733  0.726582   \n",
      "183  0.596755  0.603650  0.540704  ...  0.684944  1.041901  0.864974   \n",
      "145  0.586613  0.592968  0.538299  ...  0.662925  0.999814  0.828170   \n",
      "40   0.410373  0.415739  0.370542  ...  0.475828  0.832277  0.630870   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.624473  0.950371  0.778571   \n",
      "67   0.481982  0.487460  0.439093  ...  0.550994  0.900577  0.714306   \n",
      "117  0.513116  0.519892  0.462708  ...  0.600912  1.050402  0.807537   \n",
      "47   0.468146  0.474372  0.421441  ...  0.544576  0.938201  0.724638   \n",
      "172  0.507965  0.513851  0.460731  ...  0.581963  0.920667  0.746815   \n",
      "\n",
      "       1651.4    1664.1    1631.7    1657.8    1652.5    1658.8    1000.3  \n",
      "16   0.818291  0.815499  0.826209  0.816918  0.818068  0.816691  0.471074  \n",
      "51   0.944379  0.937618  0.958695  0.941025  0.943825  0.940328  0.432622  \n",
      "183  1.043173  1.038752  1.053865  1.040785  1.042806  1.040278  0.544204  \n",
      "145  1.000898  0.996390  1.011491  0.998529  1.000560  0.998091  0.544815  \n",
      "40   0.833061  0.831374  0.845410  0.831643  0.832835  0.831504  0.380483  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.951654  0.946719  0.962782  0.948999  0.951232  0.948635  0.511287  \n",
      "67   0.902195  0.896501  0.915029  0.899177  0.901635  0.898689  0.446172  \n",
      "117  1.052046  1.045060  1.066848  1.048548  1.051460  1.047815  0.469285  \n",
      "47   0.939872  0.933124  0.954226  0.936495  0.939293  0.935926  0.427882  \n",
      "172  0.922238  0.917195  0.934193  0.919344  0.921686  0.918728  0.465592  \n",
      "\n",
      "[130 rows x 80 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1654.6    1349.7    1651.4  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.817738  0.672579  0.818291   \n",
      "51   0.472565  0.478432  0.426489  ...  0.942733  0.726582  0.944379   \n",
      "183  0.596755  0.603650  0.540704  ...  1.041901  0.864974  1.043173   \n",
      "145  0.586613  0.592968  0.538299  ...  0.999814  0.828170  1.000898   \n",
      "40   0.410373  0.415739  0.370542  ...  0.832277  0.630870  0.833061   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.950371  0.778571  0.951654   \n",
      "67   0.481982  0.487460  0.439093  ...  0.900577  0.714306  0.902195   \n",
      "117  0.513116  0.519892  0.462708  ...  1.050402  0.807537  1.052046   \n",
      "47   0.468146  0.474372  0.421441  ...  0.938201  0.724638  0.939872   \n",
      "172  0.507965  0.513851  0.460731  ...  0.920667  0.746815  0.922238   \n",
      "\n",
      "       1664.1    1631.7    1657.8    1652.5    1658.8    1000.3    1628.6  \n",
      "16   0.815499  0.826209  0.816918  0.818068  0.816691  0.471074  0.828415  \n",
      "51   0.937618  0.958695  0.941025  0.943825  0.940328  0.432622  0.962064  \n",
      "183  1.038752  1.053865  1.040785  1.042806  1.040278  0.544204  1.056418  \n",
      "145  0.996390  1.011491  0.998529  1.000560  0.998091  0.544815  1.013938  \n",
      "40   0.831374  0.845410  0.831643  0.832835  0.831504  0.380483  0.848572  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.946719  0.962782  0.948999  0.951232  0.948635  0.511287  0.965366  \n",
      "67   0.896501  0.915029  0.899177  0.901635  0.898689  0.446172  0.918111  \n",
      "117  1.045060  1.066848  1.048548  1.051460  1.047815  0.469285  1.070440  \n",
      "47   0.933124  0.954226  0.936495  0.939293  0.935926  0.427882  0.957631  \n",
      "172  0.917195  0.934193  0.919344  0.921686  0.918728  0.465592  0.936986  \n",
      "\n",
      "[130 rows x 81 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1349.7    1651.4    1664.1  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.672579  0.818291  0.815499   \n",
      "51   0.472565  0.478432  0.426489  ...  0.726582  0.944379  0.937618   \n",
      "183  0.596755  0.603650  0.540704  ...  0.864974  1.043173  1.038752   \n",
      "145  0.586613  0.592968  0.538299  ...  0.828170  1.000898  0.996390   \n",
      "40   0.410373  0.415739  0.370542  ...  0.630870  0.833061  0.831374   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.778571  0.951654  0.946719   \n",
      "67   0.481982  0.487460  0.439093  ...  0.714306  0.902195  0.896501   \n",
      "117  0.513116  0.519892  0.462708  ...  0.807537  1.052046  1.045060   \n",
      "47   0.468146  0.474372  0.421441  ...  0.724638  0.939872  0.933124   \n",
      "172  0.507965  0.513851  0.460731  ...  0.746815  0.922238  0.917195   \n",
      "\n",
      "       1631.7    1657.8    1652.5    1658.8    1000.3    1628.6    1653.5  \n",
      "16   0.826209  0.816918  0.818068  0.816691  0.471074  0.828415  0.817875  \n",
      "51   0.958695  0.941025  0.943825  0.940328  0.432622  0.962064  0.943239  \n",
      "183  1.053865  1.040785  1.042806  1.040278  0.544204  1.056418  1.042428  \n",
      "145  1.011491  0.998529  1.000560  0.998091  0.544815  1.013938  1.000246  \n",
      "40   0.845410  0.831643  0.832835  0.831504  0.380483  0.848572  0.832598  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.962782  0.948999  0.951232  0.948635  0.511287  0.965366  0.950859  \n",
      "67   0.915029  0.899177  0.901635  0.898689  0.446172  0.918111  0.901095  \n",
      "117  1.066848  1.048548  1.051460  1.047815  0.469285  1.070440  1.050966  \n",
      "47   0.954226  0.936495  0.939293  0.935926  0.427882  0.957631  0.938742  \n",
      "172  0.934193  0.919344  0.921686  0.918728  0.465592  0.936986  0.921118  \n",
      "\n",
      "[130 rows x 82 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1651.4    1664.1    1631.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.818291  0.815499  0.826209   \n",
      "51   0.472565  0.478432  0.426489  ...  0.944379  0.937618  0.958695   \n",
      "183  0.596755  0.603650  0.540704  ...  1.043173  1.038752  1.053865   \n",
      "145  0.586613  0.592968  0.538299  ...  1.000898  0.996390  1.011491   \n",
      "40   0.410373  0.415739  0.370542  ...  0.833061  0.831374  0.845410   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.951654  0.946719  0.962782   \n",
      "67   0.481982  0.487460  0.439093  ...  0.902195  0.896501  0.915029   \n",
      "117  0.513116  0.519892  0.462708  ...  1.052046  1.045060  1.066848   \n",
      "47   0.468146  0.474372  0.421441  ...  0.939872  0.933124  0.954226   \n",
      "172  0.507965  0.513851  0.460731  ...  0.922238  0.917195  0.934193   \n",
      "\n",
      "       1657.8    1652.5    1658.8    1000.3    1628.6    1653.5    1297.7  \n",
      "16   0.816918  0.818068  0.816691  0.471074  0.828415  0.817875  0.565460  \n",
      "51   0.941025  0.943825  0.940328  0.432622  0.962064  0.943239  0.568649  \n",
      "183  1.040785  1.042806  1.040278  0.544204  1.056418  1.042428  0.706880  \n",
      "145  0.998529  1.000560  0.998091  0.544815  1.013938  1.000246  0.681547  \n",
      "40   0.831643  0.832835  0.831504  0.380483  0.848572  0.832598  0.491905  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.948999  0.951232  0.948635  0.511287  0.965366  0.950859  0.642325  \n",
      "67   0.899177  0.901635  0.898689  0.446172  0.918111  0.901095  0.568902  \n",
      "117  1.048548  1.051460  1.047815  0.469285  1.070440  1.050966  0.622864  \n",
      "47   0.936495  0.939293  0.935926  0.427882  0.957631  0.938742  0.564337  \n",
      "172  0.919344  0.921686  0.918728  0.465592  0.936986  0.921118  0.601065  \n",
      "\n",
      "[130 rows x 83 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1664.1    1631.7    1657.8  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.815499  0.826209  0.816918   \n",
      "51   0.472565  0.478432  0.426489  ...  0.937618  0.958695  0.941025   \n",
      "183  0.596755  0.603650  0.540704  ...  1.038752  1.053865  1.040785   \n",
      "145  0.586613  0.592968  0.538299  ...  0.996390  1.011491  0.998529   \n",
      "40   0.410373  0.415739  0.370542  ...  0.831374  0.845410  0.831643   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.946719  0.962782  0.948999   \n",
      "67   0.481982  0.487460  0.439093  ...  0.896501  0.915029  0.899177   \n",
      "117  0.513116  0.519892  0.462708  ...  1.045060  1.066848  1.048548   \n",
      "47   0.468146  0.474372  0.421441  ...  0.933124  0.954226  0.936495   \n",
      "172  0.507965  0.513851  0.460731  ...  0.917195  0.934193  0.919344   \n",
      "\n",
      "       1652.5    1658.8    1000.3    1628.6    1653.5    1297.7    1277.8  \n",
      "16   0.818068  0.816691  0.471074  0.828415  0.817875  0.565460  0.551303  \n",
      "51   0.943825  0.940328  0.432622  0.962064  0.943239  0.568649  0.549068  \n",
      "183  1.042806  1.040278  0.544204  1.056418  1.042428  0.706880  0.684308  \n",
      "145  1.000560  0.998091  0.544815  1.013938  1.000246  0.681547  0.662441  \n",
      "40   0.832835  0.831504  0.380483  0.848572  0.832598  0.491905  0.475356  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.951232  0.948635  0.511287  0.965366  0.950859  0.642325  0.624179  \n",
      "67   0.901635  0.898689  0.446172  0.918111  0.901095  0.568902  0.550704  \n",
      "117  1.051460  1.047815  0.469285  1.070440  1.050966  0.622864  0.600274  \n",
      "47   0.939293  0.935926  0.427882  0.957631  0.938742  0.564337  0.544241  \n",
      "172  0.921686  0.918728  0.465592  0.936986  0.921118  0.601065  0.581502  \n",
      "\n",
      "[130 rows x 84 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1631.7    1657.8    1652.5  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.826209  0.816918  0.818068   \n",
      "51   0.472565  0.478432  0.426489  ...  0.958695  0.941025  0.943825   \n",
      "183  0.596755  0.603650  0.540704  ...  1.053865  1.040785  1.042806   \n",
      "145  0.586613  0.592968  0.538299  ...  1.011491  0.998529  1.000560   \n",
      "40   0.410373  0.415739  0.370542  ...  0.845410  0.831643  0.832835   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.962782  0.948999  0.951232   \n",
      "67   0.481982  0.487460  0.439093  ...  0.915029  0.899177  0.901635   \n",
      "117  0.513116  0.519892  0.462708  ...  1.066848  1.048548  1.051460   \n",
      "47   0.468146  0.474372  0.421441  ...  0.954226  0.936495  0.939293   \n",
      "172  0.507965  0.513851  0.460731  ...  0.934193  0.919344  0.921686   \n",
      "\n",
      "       1658.8    1000.3    1628.6    1653.5    1297.7    1277.8    1630.6  \n",
      "16   0.816691  0.471074  0.828415  0.817875  0.565460  0.551303  0.826817  \n",
      "51   0.940328  0.432622  0.962064  0.943239  0.568649  0.549068  0.959616  \n",
      "183  1.040278  0.544204  1.056418  1.042428  0.706880  0.684308  1.054530  \n",
      "145  0.998091  0.544815  1.013938  1.000246  0.681547  0.662441  1.012306  \n",
      "40   0.831504  0.380483  0.848572  0.832598  0.491905  0.475356  0.846423  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.948635  0.511287  0.965366  0.950859  0.642325  0.624179  0.963653  \n",
      "67   0.898689  0.446172  0.918111  0.901095  0.568902  0.550704  0.915981  \n",
      "117  1.047815  0.469285  1.070440  1.050966  0.622864  0.600274  1.068031  \n",
      "47   0.935926  0.427882  0.957631  0.938742  0.564337  0.544241  0.955252  \n",
      "172  0.918728  0.465592  0.936986  0.921118  0.601065  0.581502  0.935157  \n",
      "\n",
      "[130 rows x 85 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1657.8    1652.5    1658.8  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.816918  0.818068  0.816691   \n",
      "51   0.472565  0.478432  0.426489  ...  0.941025  0.943825  0.940328   \n",
      "183  0.596755  0.603650  0.540704  ...  1.040785  1.042806  1.040278   \n",
      "145  0.586613  0.592968  0.538299  ...  0.998529  1.000560  0.998091   \n",
      "40   0.410373  0.415739  0.370542  ...  0.831643  0.832835  0.831504   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.948999  0.951232  0.948635   \n",
      "67   0.481982  0.487460  0.439093  ...  0.899177  0.901635  0.898689   \n",
      "117  0.513116  0.519892  0.462708  ...  1.048548  1.051460  1.047815   \n",
      "47   0.468146  0.474372  0.421441  ...  0.936495  0.939293  0.935926   \n",
      "172  0.507965  0.513851  0.460731  ...  0.919344  0.921686  0.918728   \n",
      "\n",
      "       1000.3    1628.6    1653.5    1297.7    1277.8    1630.6    1280.4  \n",
      "16   0.471074  0.828415  0.817875  0.565460  0.551303  0.826817  0.552443  \n",
      "51   0.432622  0.962064  0.943239  0.568649  0.549068  0.959616  0.550674  \n",
      "183  0.544204  1.056418  1.042428  0.706880  0.684308  1.054530  0.686004  \n",
      "145  0.544815  1.013938  1.000246  0.681547  0.662441  1.012306  0.663980  \n",
      "40   0.380483  0.848572  0.832598  0.491905  0.475356  0.846423  0.476645  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.511287  0.965366  0.950859  0.642325  0.624179  0.963653  0.625709  \n",
      "67   0.446172  0.918111  0.901095  0.568902  0.550704  0.915981  0.552075  \n",
      "117  0.469285  1.070440  1.050966  0.622864  0.600274  1.068031  0.602213  \n",
      "47   0.427882  0.957631  0.938742  0.564337  0.544241  0.955252  0.545832  \n",
      "172  0.465592  0.936986  0.921118  0.601065  0.581502  0.935157  0.583081  \n",
      "\n",
      "[130 rows x 86 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1652.5    1658.8    1000.3  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.818068  0.816691  0.471074   \n",
      "51   0.472565  0.478432  0.426489  ...  0.943825  0.940328  0.432622   \n",
      "183  0.596755  0.603650  0.540704  ...  1.042806  1.040278  0.544204   \n",
      "145  0.586613  0.592968  0.538299  ...  1.000560  0.998091  0.544815   \n",
      "40   0.410373  0.415739  0.370542  ...  0.832835  0.831504  0.380483   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.951232  0.948635  0.511287   \n",
      "67   0.481982  0.487460  0.439093  ...  0.901635  0.898689  0.446172   \n",
      "117  0.513116  0.519892  0.462708  ...  1.051460  1.047815  0.469285   \n",
      "47   0.468146  0.474372  0.421441  ...  0.939293  0.935926  0.427882   \n",
      "172  0.507965  0.513851  0.460731  ...  0.921686  0.918728  0.465592   \n",
      "\n",
      "       1628.6    1653.5    1297.7    1277.8    1630.6    1280.4    1655.6  \n",
      "16   0.828415  0.817875  0.565460  0.551303  0.826817  0.552443  0.817486  \n",
      "51   0.962064  0.943239  0.568649  0.549068  0.959616  0.550674  0.942340  \n",
      "183  1.056418  1.042428  0.706880  0.684308  1.054530  0.686004  1.041398  \n",
      "145  1.013938  1.000246  0.681547  0.662441  1.012306  0.663980  0.999369  \n",
      "40   0.848572  0.832598  0.491905  0.475356  0.846423  0.476645  0.831962  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.965366  0.950859  0.642325  0.624179  0.963653  0.625709  0.949906  \n",
      "67   0.918111  0.901095  0.568902  0.550704  0.915981  0.552075  0.900241  \n",
      "117  1.070440  1.050966  0.622864  0.600274  1.068031  0.602213  1.049863  \n",
      "47   0.957631  0.938742  0.564337  0.544241  0.955252  0.545832  0.937663  \n",
      "172  0.936986  0.921118  0.601065  0.581502  0.935157  0.583081  0.920301  \n",
      "\n",
      "[130 rows x 87 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1658.8    1000.3    1628.6  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.816691  0.471074  0.828415   \n",
      "51   0.472565  0.478432  0.426489  ...  0.940328  0.432622  0.962064   \n",
      "183  0.596755  0.603650  0.540704  ...  1.040278  0.544204  1.056418   \n",
      "145  0.586613  0.592968  0.538299  ...  0.998091  0.544815  1.013938   \n",
      "40   0.410373  0.415739  0.370542  ...  0.831504  0.380483  0.848572   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.948635  0.511287  0.965366   \n",
      "67   0.481982  0.487460  0.439093  ...  0.898689  0.446172  0.918111   \n",
      "117  0.513116  0.519892  0.462708  ...  1.047815  0.469285  1.070440   \n",
      "47   0.468146  0.474372  0.421441  ...  0.935926  0.427882  0.957631   \n",
      "172  0.507965  0.513851  0.460731  ...  0.918728  0.465592  0.936986   \n",
      "\n",
      "       1653.5    1297.7    1277.8    1630.6    1280.4    1655.6    1350.4  \n",
      "16   0.817875  0.565460  0.551303  0.826817  0.552443  0.817486  0.673728  \n",
      "51   0.943239  0.568649  0.549068  0.959616  0.550674  0.942340  0.728334  \n",
      "183  1.042428  0.706880  0.684308  1.054530  0.686004  1.041398  0.866598  \n",
      "145  1.000246  0.681547  0.662441  1.012306  0.663980  0.999369  0.829840  \n",
      "40   0.832598  0.491905  0.475356  0.846423  0.476645  0.831962  0.632526  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.950859  0.642325  0.624179  0.963653  0.625709  0.949906  0.779885  \n",
      "67   0.901095  0.568902  0.550704  0.915981  0.552075  0.900241  0.716015  \n",
      "117  1.050966  0.622864  0.600274  1.068031  0.602213  1.049863  0.809791  \n",
      "47   0.938742  0.564337  0.544241  0.955252  0.545832  0.937663  0.726461  \n",
      "172  0.921118  0.601065  0.581502  0.935157  0.583081  0.920301  0.748447  \n",
      "\n",
      "[130 rows x 88 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1000.3    1628.6    1653.5  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.471074  0.828415  0.817875   \n",
      "51   0.472565  0.478432  0.426489  ...  0.432622  0.962064  0.943239   \n",
      "183  0.596755  0.603650  0.540704  ...  0.544204  1.056418  1.042428   \n",
      "145  0.586613  0.592968  0.538299  ...  0.544815  1.013938  1.000246   \n",
      "40   0.410373  0.415739  0.370542  ...  0.380483  0.848572  0.832598   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.511287  0.965366  0.950859   \n",
      "67   0.481982  0.487460  0.439093  ...  0.446172  0.918111  0.901095   \n",
      "117  0.513116  0.519892  0.462708  ...  0.469285  1.070440  1.050966   \n",
      "47   0.468146  0.474372  0.421441  ...  0.427882  0.957631  0.938742   \n",
      "172  0.507965  0.513851  0.460731  ...  0.465592  0.936986  0.921118   \n",
      "\n",
      "       1297.7    1277.8    1630.6    1280.4    1655.6    1350.4    1353.9  \n",
      "16   0.565460  0.551303  0.826817  0.552443  0.817486  0.673728  0.680314  \n",
      "51   0.568649  0.549068  0.959616  0.550674  0.942340  0.728334  0.738115  \n",
      "183  0.706880  0.684308  1.054530  0.686004  1.041398  0.866598  0.875535  \n",
      "145  0.681547  0.662441  1.012306  0.663980  0.999369  0.829840  0.838350  \n",
      "40   0.491905  0.475356  0.846423  0.476645  0.831962  0.632526  0.641674  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.642325  0.624179  0.963653  0.625709  0.949906  0.779885  0.788012  \n",
      "67   0.568902  0.550704  0.915981  0.552075  0.900241  0.716015  0.724904  \n",
      "117  0.622864  0.600274  1.068031  0.602213  1.049863  0.809791  0.820938  \n",
      "47   0.564337  0.544241  0.955252  0.545832  0.937663  0.726461  0.736236  \n",
      "172  0.601065  0.581502  0.935157  0.583081  0.920301  0.748447  0.756980  \n",
      "\n",
      "[130 rows x 89 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1628.6    1653.5    1297.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.828415  0.817875  0.565460   \n",
      "51   0.472565  0.478432  0.426489  ...  0.962064  0.943239  0.568649   \n",
      "183  0.596755  0.603650  0.540704  ...  1.056418  1.042428  0.706880   \n",
      "145  0.586613  0.592968  0.538299  ...  1.013938  1.000246  0.681547   \n",
      "40   0.410373  0.415739  0.370542  ...  0.848572  0.832598  0.491905   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.965366  0.950859  0.642325   \n",
      "67   0.481982  0.487460  0.439093  ...  0.918111  0.901095  0.568902   \n",
      "117  0.513116  0.519892  0.462708  ...  1.070440  1.050966  0.622864   \n",
      "47   0.468146  0.474372  0.421441  ...  0.957631  0.938742  0.564337   \n",
      "172  0.507965  0.513851  0.460731  ...  0.936986  0.921118  0.601065   \n",
      "\n",
      "       1277.8    1630.6    1280.4    1655.6    1350.4    1353.9    1259.8  \n",
      "16   0.551303  0.826817  0.552443  0.817486  0.673728  0.680314  0.549991  \n",
      "51   0.549068  0.959616  0.550674  0.942340  0.728334  0.738115  0.547713  \n",
      "183  0.684308  1.054530  0.686004  1.041398  0.866598  0.875535  0.681788  \n",
      "145  0.662441  1.012306  0.663980  0.999369  0.829840  0.838350  0.660779  \n",
      "40   0.475356  0.846423  0.476645  0.831962  0.632526  0.641674  0.474447  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.624179  0.963653  0.625709  0.949906  0.779885  0.788012  0.622385  \n",
      "67   0.550704  0.915981  0.552075  0.900241  0.716015  0.724904  0.549372  \n",
      "117  0.600274  1.068031  0.602213  1.049863  0.809791  0.820938  0.598709  \n",
      "47   0.544241  0.955252  0.545832  0.937663  0.726461  0.736236  0.542685  \n",
      "172  0.581502  0.935157  0.583081  0.920301  0.748447  0.756980  0.579502  \n",
      "\n",
      "[130 rows x 90 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1653.5    1297.7    1277.8  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.817875  0.565460  0.551303   \n",
      "51   0.472565  0.478432  0.426489  ...  0.943239  0.568649  0.549068   \n",
      "183  0.596755  0.603650  0.540704  ...  1.042428  0.706880  0.684308   \n",
      "145  0.586613  0.592968  0.538299  ...  1.000246  0.681547  0.662441   \n",
      "40   0.410373  0.415739  0.370542  ...  0.832598  0.491905  0.475356   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.950859  0.642325  0.624179   \n",
      "67   0.481982  0.487460  0.439093  ...  0.901095  0.568902  0.550704   \n",
      "117  0.513116  0.519892  0.462708  ...  1.050966  0.622864  0.600274   \n",
      "47   0.468146  0.474372  0.421441  ...  0.938742  0.564337  0.544241   \n",
      "172  0.507965  0.513851  0.460731  ...  0.921118  0.601065  0.581502   \n",
      "\n",
      "       1630.6    1280.4    1655.6    1350.4    1353.9    1259.8    1000.7  \n",
      "16   0.826817  0.552443  0.817486  0.673728  0.680314  0.549991  0.470934  \n",
      "51   0.959616  0.550674  0.942340  0.728334  0.738115  0.547713  0.432626  \n",
      "183  1.054530  0.686004  1.041398  0.866598  0.875535  0.681788  0.543792  \n",
      "145  1.012306  0.663980  0.999369  0.829840  0.838350  0.660779  0.544524  \n",
      "40   0.846423  0.476645  0.831962  0.632526  0.641674  0.474447  0.380541  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.963653  0.625709  0.949906  0.779885  0.788012  0.622385  0.511098  \n",
      "67   0.915981  0.552075  0.900241  0.716015  0.724904  0.549372  0.446178  \n",
      "117  1.068031  0.602213  1.049863  0.809791  0.820938  0.598709  0.468909  \n",
      "47   0.955252  0.545832  0.937663  0.726461  0.736236  0.542685  0.427481  \n",
      "172  0.935157  0.583081  0.920301  0.748447  0.756980  0.579502  0.465206  \n",
      "\n",
      "[130 rows x 91 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1297.7    1277.8    1630.6  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.565460  0.551303  0.826817   \n",
      "51   0.472565  0.478432  0.426489  ...  0.568649  0.549068  0.959616   \n",
      "183  0.596755  0.603650  0.540704  ...  0.706880  0.684308  1.054530   \n",
      "145  0.586613  0.592968  0.538299  ...  0.681547  0.662441  1.012306   \n",
      "40   0.410373  0.415739  0.370542  ...  0.491905  0.475356  0.846423   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.642325  0.624179  0.963653   \n",
      "67   0.481982  0.487460  0.439093  ...  0.568902  0.550704  0.915981   \n",
      "117  0.513116  0.519892  0.462708  ...  0.622864  0.600274  1.068031   \n",
      "47   0.468146  0.474372  0.421441  ...  0.564337  0.544241  0.955252   \n",
      "172  0.507965  0.513851  0.460731  ...  0.601065  0.581502  0.935157   \n",
      "\n",
      "       1280.4    1655.6    1350.4    1353.9    1259.8    1000.7    1665.2  \n",
      "16   0.552443  0.817486  0.673728  0.680314  0.549991  0.470934  0.815275  \n",
      "51   0.550674  0.942340  0.728334  0.738115  0.547713  0.432626  0.937095  \n",
      "183  0.686004  1.041398  0.866598  0.875535  0.681788  0.543792  1.038537  \n",
      "145  0.663980  0.999369  0.829840  0.838350  0.660779  0.544524  0.995970  \n",
      "40   0.476645  0.831962  0.632526  0.641674  0.474447  0.380541  0.831384  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.625709  0.949906  0.779885  0.788012  0.622385  0.511098  0.946201  \n",
      "67   0.552075  0.900241  0.716015  0.724904  0.549372  0.446178  0.896007  \n",
      "117  0.602213  1.049863  0.809791  0.820938  0.598709  0.468909  1.044436  \n",
      "47   0.545832  0.937663  0.726461  0.736236  0.542685  0.427481  0.932502  \n",
      "172  0.583081  0.920301  0.748447  0.756980  0.579502  0.465206  0.916614  \n",
      "\n",
      "[130 rows x 92 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1277.8    1630.6    1280.4  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.551303  0.826817  0.552443   \n",
      "51   0.472565  0.478432  0.426489  ...  0.549068  0.959616  0.550674   \n",
      "183  0.596755  0.603650  0.540704  ...  0.684308  1.054530  0.686004   \n",
      "145  0.586613  0.592968  0.538299  ...  0.662441  1.012306  0.663980   \n",
      "40   0.410373  0.415739  0.370542  ...  0.475356  0.846423  0.476645   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.624179  0.963653  0.625709   \n",
      "67   0.481982  0.487460  0.439093  ...  0.550704  0.915981  0.552075   \n",
      "117  0.513116  0.519892  0.462708  ...  0.600274  1.068031  0.602213   \n",
      "47   0.468146  0.474372  0.421441  ...  0.544241  0.955252  0.545832   \n",
      "172  0.507965  0.513851  0.460731  ...  0.581502  0.935157  0.583081   \n",
      "\n",
      "       1655.6    1350.4    1353.9    1259.8    1000.7    1665.2    1282.9  \n",
      "16   0.817486  0.673728  0.680314  0.549991  0.470934  0.815275  0.553567  \n",
      "51   0.942340  0.728334  0.738115  0.547713  0.432626  0.937095  0.552194  \n",
      "183  1.041398  0.866598  0.875535  0.681788  0.543792  1.038537  0.687996  \n",
      "145  0.999369  0.829840  0.838350  0.660779  0.544524  0.995970  0.665443  \n",
      "40   0.831962  0.632526  0.641674  0.474447  0.380541  0.831384  0.477874  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.949906  0.779885  0.788012  0.622385  0.511098  0.946201  0.626963  \n",
      "67   0.900241  0.716015  0.724904  0.549372  0.446178  0.896007  0.553497  \n",
      "117  1.049863  0.809791  0.820938  0.598709  0.468909  1.044436  0.603738  \n",
      "47   0.937663  0.726461  0.736236  0.542685  0.427481  0.932502  0.547322  \n",
      "172  0.920301  0.748447  0.756980  0.579502  0.465206  0.916614  0.584843  \n",
      "\n",
      "[130 rows x 93 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1630.6    1280.4    1655.6  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.826817  0.552443  0.817486   \n",
      "51   0.472565  0.478432  0.426489  ...  0.959616  0.550674  0.942340   \n",
      "183  0.596755  0.603650  0.540704  ...  1.054530  0.686004  1.041398   \n",
      "145  0.586613  0.592968  0.538299  ...  1.012306  0.663980  0.999369   \n",
      "40   0.410373  0.415739  0.370542  ...  0.846423  0.476645  0.831962   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.963653  0.625709  0.949906   \n",
      "67   0.481982  0.487460  0.439093  ...  0.915981  0.552075  0.900241   \n",
      "117  0.513116  0.519892  0.462708  ...  1.068031  0.602213  1.049863   \n",
      "47   0.468146  0.474372  0.421441  ...  0.955252  0.545832  0.937663   \n",
      "172  0.507965  0.513851  0.460731  ...  0.935157  0.583081  0.920301   \n",
      "\n",
      "       1350.4    1353.9    1259.8    1000.7    1665.2    1282.9    1656.7  \n",
      "16   0.673728  0.680314  0.549991  0.470934  0.815275  0.553567  0.817199  \n",
      "51   0.728334  0.738115  0.547713  0.432626  0.937095  0.552194  0.941669  \n",
      "183  0.866598  0.875535  0.681788  0.543792  1.038537  0.687996  1.041071  \n",
      "145  0.829840  0.838350  0.660779  0.544524  0.995970  0.665443  0.998934  \n",
      "40   0.632526  0.641674  0.474447  0.380541  0.831384  0.477874  0.831791  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.779885  0.788012  0.622385  0.511098  0.946201  0.626963  0.949434  \n",
      "67   0.716015  0.724904  0.549372  0.446178  0.896007  0.553497  0.899725  \n",
      "117  0.809791  0.820938  0.598709  0.468909  1.044436  0.603738  1.049266  \n",
      "47   0.726461  0.736236  0.542685  0.427481  0.932502  0.547322  0.937104  \n",
      "172  0.748447  0.756980  0.579502  0.465206  0.916614  0.584843  0.919836  \n",
      "\n",
      "[130 rows x 94 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1280.4    1655.6    1350.4  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.552443  0.817486  0.673728   \n",
      "51   0.472565  0.478432  0.426489  ...  0.550674  0.942340  0.728334   \n",
      "183  0.596755  0.603650  0.540704  ...  0.686004  1.041398  0.866598   \n",
      "145  0.586613  0.592968  0.538299  ...  0.663980  0.999369  0.829840   \n",
      "40   0.410373  0.415739  0.370542  ...  0.476645  0.831962  0.632526   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.625709  0.949906  0.779885   \n",
      "67   0.481982  0.487460  0.439093  ...  0.552075  0.900241  0.716015   \n",
      "117  0.513116  0.519892  0.462708  ...  0.602213  1.049863  0.809791   \n",
      "47   0.468146  0.474372  0.421441  ...  0.545832  0.937663  0.726461   \n",
      "172  0.507965  0.513851  0.460731  ...  0.583081  0.920301  0.748447   \n",
      "\n",
      "       1353.9    1259.8    1000.7    1665.2    1282.9    1656.7    1371.1  \n",
      "16   0.680314  0.549991  0.470934  0.815275  0.553567  0.817199  0.718331  \n",
      "51   0.738115  0.547713  0.432626  0.937095  0.552194  0.941669  0.795758  \n",
      "183  0.875535  0.681788  0.543792  1.038537  0.687996  1.041071  0.926334  \n",
      "145  0.838350  0.660779  0.544524  0.995970  0.665443  0.998934  0.887319  \n",
      "40   0.641674  0.474447  0.380541  0.831384  0.477874  0.831791  0.692902  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.788012  0.622385  0.511098  0.946201  0.626963  0.949434  0.835361  \n",
      "67   0.724904  0.549372  0.446178  0.896007  0.553497  0.899725  0.777510  \n",
      "117  0.820938  0.598709  0.468909  1.044436  0.603738  1.049266  0.886576  \n",
      "47   0.736236  0.542685  0.427481  0.932502  0.547322  0.937104  0.794833  \n",
      "172  0.756980  0.579502  0.465206  0.916614  0.584843  0.919836  0.807621  \n",
      "\n",
      "[130 rows x 95 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1655.6    1350.4    1353.9  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.817486  0.673728  0.680314   \n",
      "51   0.472565  0.478432  0.426489  ...  0.942340  0.728334  0.738115   \n",
      "183  0.596755  0.603650  0.540704  ...  1.041398  0.866598  0.875535   \n",
      "145  0.586613  0.592968  0.538299  ...  0.999369  0.829840  0.838350   \n",
      "40   0.410373  0.415739  0.370542  ...  0.831962  0.632526  0.641674   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.949906  0.779885  0.788012   \n",
      "67   0.481982  0.487460  0.439093  ...  0.900241  0.716015  0.724904   \n",
      "117  0.513116  0.519892  0.462708  ...  1.049863  0.809791  0.820938   \n",
      "47   0.468146  0.474372  0.421441  ...  0.937663  0.726461  0.736236   \n",
      "172  0.507965  0.513851  0.460731  ...  0.920301  0.748447  0.756980   \n",
      "\n",
      "       1259.8    1000.7    1665.2    1282.9    1656.7    1371.1    1367.5  \n",
      "16   0.549991  0.470934  0.815275  0.553567  0.817199  0.718331  0.707554  \n",
      "51   0.547713  0.432626  0.937095  0.552194  0.941669  0.795758  0.778815  \n",
      "183  0.681788  0.543792  1.038537  0.687996  1.041071  0.926334  0.911805  \n",
      "145  0.660779  0.544524  0.995970  0.665443  0.998934  0.887319  0.873373  \n",
      "40   0.474447  0.380541  0.831384  0.477874  0.831791  0.692902  0.678247  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.622385  0.511098  0.946201  0.626963  0.949434  0.835361  0.821721  \n",
      "67   0.549372  0.446178  0.896007  0.553497  0.899725  0.777510  0.762338  \n",
      "117  0.598709  0.468909  1.044436  0.603738  1.049266  0.886576  0.867440  \n",
      "47   0.542685  0.427481  0.932502  0.547322  0.937104  0.794833  0.777917  \n",
      "172  0.579502  0.465206  0.916614  0.584843  0.919836  0.807621  0.792881  \n",
      "\n",
      "[130 rows x 96 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1350.4    1353.9    1259.8  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.673728  0.680314  0.549991   \n",
      "51   0.472565  0.478432  0.426489  ...  0.728334  0.738115  0.547713   \n",
      "183  0.596755  0.603650  0.540704  ...  0.866598  0.875535  0.681788   \n",
      "145  0.586613  0.592968  0.538299  ...  0.829840  0.838350  0.660779   \n",
      "40   0.410373  0.415739  0.370542  ...  0.632526  0.641674  0.474447   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.779885  0.788012  0.622385   \n",
      "67   0.481982  0.487460  0.439093  ...  0.716015  0.724904  0.549372   \n",
      "117  0.513116  0.519892  0.462708  ...  0.809791  0.820938  0.598709   \n",
      "47   0.468146  0.474372  0.421441  ...  0.726461  0.736236  0.542685   \n",
      "172  0.507965  0.513851  0.460731  ...  0.748447  0.756980  0.579502   \n",
      "\n",
      "       1000.7    1665.2    1282.9    1656.7    1371.1    1367.5    1260.4  \n",
      "16   0.470934  0.815275  0.553567  0.817199  0.718331  0.707554  0.549968  \n",
      "51   0.432626  0.937095  0.552194  0.941669  0.795758  0.778815  0.547477  \n",
      "183  0.543792  1.038537  0.687996  1.041071  0.926334  0.911805  0.681766  \n",
      "145  0.544524  0.995970  0.665443  0.998934  0.887319  0.873373  0.660565  \n",
      "40   0.380541  0.831384  0.477874  0.831791  0.692902  0.678247  0.474385  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.511098  0.946201  0.626963  0.949434  0.835361  0.821721  0.622106  \n",
      "67   0.446178  0.896007  0.553497  0.899725  0.777510  0.762338  0.549055  \n",
      "117  0.468909  1.044436  0.603738  1.049266  0.886576  0.867440  0.598418  \n",
      "47   0.427481  0.932502  0.547322  0.937104  0.794833  0.777917  0.542392  \n",
      "172  0.465206  0.916614  0.584843  0.919836  0.807621  0.792881  0.579340  \n",
      "\n",
      "[130 rows x 97 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1353.9    1259.8    1000.7  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.680314  0.549991  0.470934   \n",
      "51   0.472565  0.478432  0.426489  ...  0.738115  0.547713  0.432626   \n",
      "183  0.596755  0.603650  0.540704  ...  0.875535  0.681788  0.543792   \n",
      "145  0.586613  0.592968  0.538299  ...  0.838350  0.660779  0.544524   \n",
      "40   0.410373  0.415739  0.370542  ...  0.641674  0.474447  0.380541   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.788012  0.622385  0.511098   \n",
      "67   0.481982  0.487460  0.439093  ...  0.724904  0.549372  0.446178   \n",
      "117  0.513116  0.519892  0.462708  ...  0.820938  0.598709  0.468909   \n",
      "47   0.468146  0.474372  0.421441  ...  0.736236  0.542685  0.427481   \n",
      "172  0.507965  0.513851  0.460731  ...  0.756980  0.579502  0.465206   \n",
      "\n",
      "       1665.2    1282.9    1656.7    1371.1    1367.5    1260.4     999.9  \n",
      "16   0.815275  0.553567  0.817199  0.718331  0.707554  0.549968  0.471459  \n",
      "51   0.937095  0.552194  0.941669  0.795758  0.778815  0.547477  0.433239  \n",
      "183  1.038537  0.687996  1.041071  0.926334  0.911805  0.681766  0.545045  \n",
      "145  0.995970  0.665443  0.998934  0.887319  0.873373  0.660565  0.545846  \n",
      "40   0.831384  0.477874  0.831791  0.692902  0.678247  0.474385  0.381048  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.946201  0.626963  0.949434  0.835361  0.821721  0.622106  0.511888  \n",
      "67   0.896007  0.553497  0.899725  0.777510  0.762338  0.549055  0.446987  \n",
      "117  1.044436  0.603738  1.049266  0.886576  0.867440  0.598418  0.470157  \n",
      "47   0.932502  0.547322  0.937104  0.794833  0.777917  0.542392  0.428519  \n",
      "172  0.916614  0.584843  0.919836  0.807621  0.792881  0.579340  0.466400  \n",
      "\n",
      "[130 rows x 98 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1259.8    1000.7    1665.2  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.549991  0.470934  0.815275   \n",
      "51   0.472565  0.478432  0.426489  ...  0.547713  0.432626  0.937095   \n",
      "183  0.596755  0.603650  0.540704  ...  0.681788  0.543792  1.038537   \n",
      "145  0.586613  0.592968  0.538299  ...  0.660779  0.544524  0.995970   \n",
      "40   0.410373  0.415739  0.370542  ...  0.474447  0.380541  0.831384   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.622385  0.511098  0.946201   \n",
      "67   0.481982  0.487460  0.439093  ...  0.549372  0.446178  0.896007   \n",
      "117  0.513116  0.519892  0.462708  ...  0.598709  0.468909  1.044436   \n",
      "47   0.468146  0.474372  0.421441  ...  0.542685  0.427481  0.932502   \n",
      "172  0.507965  0.513851  0.460731  ...  0.579502  0.465206  0.916614   \n",
      "\n",
      "       1282.9    1656.7    1371.1    1367.5    1260.4     999.9      1281  \n",
      "16   0.553567  0.817199  0.718331  0.707554  0.549968  0.471459  0.552638  \n",
      "51   0.552194  0.941669  0.795758  0.778815  0.547477  0.433239  0.550958  \n",
      "183  0.687996  1.041071  0.926334  0.911805  0.681766  0.545045  0.686438  \n",
      "145  0.665443  0.998934  0.887319  0.873373  0.660565  0.545846  0.664311  \n",
      "40   0.477874  0.831791  0.692902  0.678247  0.474385  0.381048  0.476923  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.626963  0.949434  0.835361  0.821721  0.622106  0.511888  0.626013  \n",
      "67   0.553497  0.899725  0.777510  0.762338  0.549055  0.446987  0.552393  \n",
      "117  0.603738  1.049266  0.886576  0.867440  0.598418  0.470157  0.602534  \n",
      "47   0.547322  0.937104  0.794833  0.777917  0.542392  0.428519  0.546099  \n",
      "172  0.584843  0.919836  0.807621  0.792881  0.579340  0.466400  0.583465  \n",
      "\n",
      "[130 rows x 99 columns]\n",
      "       1139.7    1140.2    1141.7    1138.2    1138.7    1139.2    1140.7  \\\n",
      "16   0.492063  0.493875  0.500545  0.485824  0.487906  0.489890  0.496089   \n",
      "51   0.464010  0.466825  0.475396  0.454894  0.457810  0.460916  0.469729   \n",
      "183  0.586312  0.589698  0.600224  0.575629  0.579215  0.582786  0.593261   \n",
      "145  0.577506  0.580543  0.589776  0.568155  0.571318  0.574446  0.583619   \n",
      "40   0.402640  0.405209  0.413083  0.395286  0.397766  0.400044  0.407849   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "103  0.543538  0.546104  0.554676  0.535210  0.538105  0.540964  0.549027   \n",
      "67   0.473749  0.476239  0.484654  0.465488  0.468295  0.471059  0.479081   \n",
      "117  0.503392  0.506553  0.516428  0.493610  0.496839  0.500089  0.509868   \n",
      "47   0.459436  0.462290  0.471228  0.450392  0.453445  0.456400  0.465182   \n",
      "172  0.499040  0.501932  0.510931  0.489965  0.492841  0.495920  0.504904   \n",
      "\n",
      "       1141.2    1142.2    1132.2  ...    1000.7    1665.2    1282.9  \\\n",
      "16   0.498279  0.502916  0.464507  ...  0.470934  0.815275  0.553567   \n",
      "51   0.472565  0.478432  0.426489  ...  0.432626  0.937095  0.552194   \n",
      "183  0.596755  0.603650  0.540704  ...  0.543792  1.038537  0.687996   \n",
      "145  0.586613  0.592968  0.538299  ...  0.544524  0.995970  0.665443   \n",
      "40   0.410373  0.415739  0.370542  ...  0.380541  0.831384  0.477874   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "103  0.551875  0.557337  0.508815  ...  0.511098  0.946201  0.626963   \n",
      "67   0.481982  0.487460  0.439093  ...  0.446178  0.896007  0.553497   \n",
      "117  0.513116  0.519892  0.462708  ...  0.468909  1.044436  0.603738   \n",
      "47   0.468146  0.474372  0.421441  ...  0.427481  0.932502  0.547322   \n",
      "172  0.507965  0.513851  0.460731  ...  0.465206  0.916614  0.584843   \n",
      "\n",
      "       1656.7    1371.1    1367.5    1260.4     999.9      1281    1278.5  \n",
      "16   0.817199  0.718331  0.707554  0.549968  0.471459  0.552638  0.551472  \n",
      "51   0.941669  0.795758  0.778815  0.547477  0.433239  0.550958  0.549321  \n",
      "183  1.041071  0.926334  0.911805  0.681766  0.545045  0.686438  0.684643  \n",
      "145  0.998934  0.887319  0.873373  0.660565  0.545846  0.664311  0.662634  \n",
      "40   0.831791  0.692902  0.678247  0.474385  0.381048  0.476923  0.475571  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "103  0.949434  0.835361  0.821721  0.622106  0.511888  0.626013  0.624299  \n",
      "67   0.899725  0.777510  0.762338  0.549055  0.446987  0.552393  0.550897  \n",
      "117  1.049266  0.886576  0.867440  0.598418  0.470157  0.602534  0.600575  \n",
      "47   0.937104  0.794833  0.777917  0.542392  0.428519  0.546099  0.544344  \n",
      "172  0.919836  0.807621  0.792881  0.579340  0.466400  0.583465  0.581721  \n",
      "\n",
      "[130 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "for n in ab:\n",
    "    x_train_selected = x_train[features_mi.nlargest(100,columns=\"Mutual Information Value\").iloc[0:n, 0]]\n",
    "    print(x_train_selected) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Akurasi model RFE data Train dengan 1 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 1 fitur dan 100 trees:0.82\n",
      "Precision model RFE data Train dengan 1 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 1 fitur dan 100 trees:0.82\n",
      "Recall model RFE data Train dengan 1 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 1 fitur dan 100 trees:0.82\n",
      " \n",
      "total time taken this loop:  1.409531593322754\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 2 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 2 fitur dan 100 trees:0.82\n",
      "Precision model RFE data Train dengan 2 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 2 fitur dan 100 trees:0.82\n",
      "Recall model RFE data Train dengan 2 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 2 fitur dan 100 trees:0.82\n",
      " \n",
      "total time taken this loop:  1.3524270057678223\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 3 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 3 fitur dan 100 trees:0.84\n",
      "Precision model RFE data Train dengan 3 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 3 fitur dan 100 trees:0.84\n",
      "Recall model RFE data Train dengan 3 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 3 fitur dan 100 trees:0.84\n",
      " \n",
      "total time taken this loop:  1.3517112731933594\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 4 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 4 fitur dan 100 trees:0.84\n",
      "Precision model RFE data Train dengan 4 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 4 fitur dan 100 trees:0.84\n",
      "Recall model RFE data Train dengan 4 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 4 fitur dan 100 trees:0.84\n",
      " \n",
      "total time taken this loop:  1.3861949443817139\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 5 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 5 fitur dan 100 trees:0.84\n",
      "Precision model RFE data Train dengan 5 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 5 fitur dan 100 trees:0.84\n",
      "Recall model RFE data Train dengan 5 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 5 fitur dan 100 trees:0.84\n",
      " \n",
      "total time taken this loop:  1.3801240921020508\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 6 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 6 fitur dan 100 trees:0.84\n",
      "Precision model RFE data Train dengan 6 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 6 fitur dan 100 trees:0.84\n",
      "Recall model RFE data Train dengan 6 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 6 fitur dan 100 trees:0.84\n",
      " \n",
      "total time taken this loop:  1.382385015487671\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 7 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 7 fitur dan 100 trees:0.84\n",
      "Precision model RFE data Train dengan 7 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 7 fitur dan 100 trees:0.84\n",
      "Recall model RFE data Train dengan 7 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 7 fitur dan 100 trees:0.84\n",
      " \n",
      "total time taken this loop:  1.373033046722412\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 8 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 8 fitur dan 100 trees:0.84\n",
      "Precision model RFE data Train dengan 8 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 8 fitur dan 100 trees:0.84\n",
      "Recall model RFE data Train dengan 8 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 8 fitur dan 100 trees:0.84\n",
      " \n",
      "total time taken this loop:  1.3682262897491455\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 9 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 9 fitur dan 100 trees:0.84\n",
      "Precision model RFE data Train dengan 9 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 9 fitur dan 100 trees:0.84\n",
      "Recall model RFE data Train dengan 9 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 9 fitur dan 100 trees:0.84\n",
      " \n",
      "total time taken this loop:  1.389580249786377\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 10 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 10 fitur dan 100 trees:0.84\n",
      "Precision model RFE data Train dengan 10 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 10 fitur dan 100 trees:0.84\n",
      "Recall model RFE data Train dengan 10 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 10 fitur dan 100 trees:0.84\n",
      " \n",
      "total time taken this loop:  1.3564937114715576\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 11 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 11 fitur dan 100 trees:0.84\n",
      "Precision model RFE data Train dengan 11 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 11 fitur dan 100 trees:0.84\n",
      "Recall model RFE data Train dengan 11 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 11 fitur dan 100 trees:0.84\n",
      " \n",
      "total time taken this loop:  1.3463506698608398\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 12 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 12 fitur dan 100 trees:0.84\n",
      "Precision model RFE data Train dengan 12 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 12 fitur dan 100 trees:0.84\n",
      "Recall model RFE data Train dengan 12 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 12 fitur dan 100 trees:0.84\n",
      " \n",
      "total time taken this loop:  1.3478055000305176\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 13 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 13 fitur dan 100 trees:0.84\n",
      "Precision model RFE data Train dengan 13 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 13 fitur dan 100 trees:0.84\n",
      "Recall model RFE data Train dengan 13 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 13 fitur dan 100 trees:0.84\n",
      " \n",
      "total time taken this loop:  1.4238057136535645\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 14 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 14 fitur dan 100 trees:0.88\n",
      "Precision model RFE data Train dengan 14 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 14 fitur dan 100 trees:0.88\n",
      "Recall model RFE data Train dengan 14 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 14 fitur dan 100 trees:0.88\n",
      " \n",
      "total time taken this loop:  1.345942497253418\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 15 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 15 fitur dan 100 trees:0.89\n",
      "Precision model RFE data Train dengan 15 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 15 fitur dan 100 trees:0.89\n",
      "Recall model RFE data Train dengan 15 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 15 fitur dan 100 trees:0.89\n",
      " \n",
      "total time taken this loop:  1.3474559783935547\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 16 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 16 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 16 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 16 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 16 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 16 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3404104709625244\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 17 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 17 fitur dan 100 trees:0.91\n",
      "Precision model RFE data Train dengan 17 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 17 fitur dan 100 trees:0.91\n",
      "Recall model RFE data Train dengan 17 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 17 fitur dan 100 trees:0.91\n",
      " \n",
      "total time taken this loop:  1.340723991394043\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 18 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 18 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 18 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 18 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 18 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 18 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3412227630615234\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 19 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 19 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 19 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 19 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 19 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 19 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3394715785980225\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 20 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 20 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 20 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 20 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 20 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 20 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3368539810180664\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 21 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 21 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 21 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 21 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 21 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 21 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3266370296478271\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 22 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 22 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 22 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 22 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 22 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 22 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3271393775939941\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 23 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 23 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 23 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 23 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 23 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 23 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3209185600280762\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 24 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 24 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 24 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 24 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 24 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 24 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3204820156097412\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 25 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 25 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 25 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 25 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 25 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 25 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3414666652679443\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 26 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 26 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 26 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 26 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 26 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 26 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3424053192138672\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 27 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 27 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 27 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 27 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 27 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 27 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.33817458152771\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 28 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 28 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 28 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 28 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 28 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 28 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.343090295791626\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 29 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 29 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 29 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 29 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 29 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 29 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3193695545196533\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 30 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 30 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 30 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 30 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 30 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 30 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.2694926261901855\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 31 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 31 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 31 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 31 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 31 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 31 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.2811102867126465\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 32 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 32 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 32 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 32 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 32 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 32 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.2755677700042725\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 33 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 33 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 33 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 33 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 33 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 33 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.2825777530670166\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 34 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 34 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 34 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 34 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 34 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 34 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.28068208694458\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 35 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 35 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 35 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 35 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 35 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 35 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.2771880626678467\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 36 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 36 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 36 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 36 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 36 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 36 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.276094675064087\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 37 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 37 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 37 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 37 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 37 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 37 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.283771276473999\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 38 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 38 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 38 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 38 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 38 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 38 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.2994225025177002\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 39 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 39 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 39 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 39 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 39 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 39 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.2959764003753662\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 40 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 40 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 40 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 40 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 40 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 40 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.2997767925262451\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 41 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 41 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 41 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 41 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 41 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 41 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3035845756530762\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 42 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 42 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 42 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 42 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 42 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 42 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.297370195388794\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 43 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 43 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 43 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 43 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 43 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 43 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3006744384765625\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 44 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 44 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 44 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 44 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 44 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 44 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.2981288433074951\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 45 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 45 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 45 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 45 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 45 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 45 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3065273761749268\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 46 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 46 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 46 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 46 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 46 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 46 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3014636039733887\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 47 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 47 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 47 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 47 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 47 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 47 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.298828363418579\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 48 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 48 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 48 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 48 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 48 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 48 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3036155700683594\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 49 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 49 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 49 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 49 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 49 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 49 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3209562301635742\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 50 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 50 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 50 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 50 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 50 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 50 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3200531005859375\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 51 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 51 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 51 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 51 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 51 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 51 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.324317455291748\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 52 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 52 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 52 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 52 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 52 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 52 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3511390686035156\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 53 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 53 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 53 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 53 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 53 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 53 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3556418418884277\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 54 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 54 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 54 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 54 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 54 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 54 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3547923564910889\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 55 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 55 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 55 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 55 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 55 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 55 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.353942632675171\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 56 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 56 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 56 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 56 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 56 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 56 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3548088073730469\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 57 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 57 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 57 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 57 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 57 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 57 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3533637523651123\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 58 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 58 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 58 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 58 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 58 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 58 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3567323684692383\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 59 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 59 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 59 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 59 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 59 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 59 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.353097915649414\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 60 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 60 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 60 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 60 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 60 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 60 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3561124801635742\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 61 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 61 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 61 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 61 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 61 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 61 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3540902137756348\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 62 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 62 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 62 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 62 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 62 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 62 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3559529781341553\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 63 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 63 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 63 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 63 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 63 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 63 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3528470993041992\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 64 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 64 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 64 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 64 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 64 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 64 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.374603033065796\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 65 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 65 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 65 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 65 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 65 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 65 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.371861219406128\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 66 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 66 fitur dan 100 trees:0.96\n",
      "Precision model RFE data Train dengan 66 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 66 fitur dan 100 trees:0.96\n",
      "Recall model RFE data Train dengan 66 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 66 fitur dan 100 trees:0.96\n",
      " \n",
      "total time taken this loop:  1.3701353073120117\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 67 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 67 fitur dan 100 trees:0.93\n",
      "Precision model RFE data Train dengan 67 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 67 fitur dan 100 trees:0.93\n",
      "Recall model RFE data Train dengan 67 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 67 fitur dan 100 trees:0.93\n",
      " \n",
      "total time taken this loop:  1.3724844455718994\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 68 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 68 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 68 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 68 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 68 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 68 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3687975406646729\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 69 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 69 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 69 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 69 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 69 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 69 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3676321506500244\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 70 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 70 fitur dan 100 trees:0.96\n",
      "Precision model RFE data Train dengan 70 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 70 fitur dan 100 trees:0.96\n",
      "Recall model RFE data Train dengan 70 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 70 fitur dan 100 trees:0.96\n",
      " \n",
      "total time taken this loop:  1.3713796138763428\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 71 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 71 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 71 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 71 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 71 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 71 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.370511770248413\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 72 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 72 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 72 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 72 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 72 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 72 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3691277503967285\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 73 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 73 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 73 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 73 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 73 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 73 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3746337890625\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 74 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 74 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 74 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 74 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 74 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 74 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.366138219833374\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 75 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 75 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 75 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 75 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 75 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 75 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.369330883026123\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 76 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 76 fitur dan 100 trees:0.96\n",
      "Precision model RFE data Train dengan 76 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 76 fitur dan 100 trees:0.96\n",
      "Recall model RFE data Train dengan 76 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 76 fitur dan 100 trees:0.96\n",
      " \n",
      "total time taken this loop:  1.3706035614013672\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 77 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 77 fitur dan 100 trees:0.96\n",
      "Precision model RFE data Train dengan 77 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 77 fitur dan 100 trees:0.96\n",
      "Recall model RFE data Train dengan 77 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 77 fitur dan 100 trees:0.96\n",
      " \n",
      "total time taken this loop:  1.368290901184082\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 78 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 78 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 78 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 78 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 78 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 78 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3727858066558838\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 79 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 79 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 79 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 79 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 79 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 79 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3678772449493408\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 80 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 80 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 80 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 80 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 80 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 80 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3695662021636963\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 81 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 81 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 81 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 81 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 81 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 81 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.385890007019043\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 82 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 82 fitur dan 100 trees:0.96\n",
      "Precision model RFE data Train dengan 82 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 82 fitur dan 100 trees:0.96\n",
      "Recall model RFE data Train dengan 82 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 82 fitur dan 100 trees:0.96\n",
      " \n",
      "total time taken this loop:  1.3878610134124756\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 83 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 83 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 83 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 83 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 83 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 83 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3869502544403076\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 84 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 84 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 84 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 84 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 84 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 84 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3915324211120605\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 85 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 85 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 85 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 85 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 85 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 85 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3863661289215088\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 86 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 86 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 86 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 86 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 86 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 86 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3887522220611572\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 87 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 87 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 87 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 87 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 87 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 87 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.387523889541626\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 88 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 88 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 88 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 88 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 88 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 88 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3936636447906494\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 89 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 89 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 89 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 89 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 89 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 89 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3907439708709717\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 90 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 90 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 90 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 90 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 90 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 90 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3900034427642822\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 91 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 91 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 91 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 91 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 91 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 91 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.392808198928833\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 92 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 92 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 92 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 92 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 92 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 92 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.399294137954712\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 93 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 93 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 93 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 93 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 93 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 93 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.413144588470459\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 94 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 94 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 94 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 94 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 94 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 94 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3917067050933838\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 95 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 95 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 95 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 95 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 95 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 95 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3884222507476807\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 96 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 96 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 96 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 96 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 96 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 96 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3933961391448975\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 97 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 97 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 97 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 97 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 97 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 97 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3893406391143799\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 98 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 98 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 98 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 98 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 98 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 98 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.389714241027832\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 99 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 99 fitur dan 100 trees:0.95\n",
      "Precision model RFE data Train dengan 99 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 99 fitur dan 100 trees:0.95\n",
      "Recall model RFE data Train dengan 99 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 99 fitur dan 100 trees:0.95\n",
      " \n",
      "total time taken this loop:  1.3936948776245117\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 100 fitur dan 100 trees:1.0\n",
      "Akurasi model RFE data Test dengan 100 fitur dan 100 trees:0.96\n",
      "Precision model RFE data Train dengan 100 fitur dan 100 trees:1.0\n",
      "Precision model RFE data Test dengan 100 fitur dan 100 trees:0.96\n",
      "Recall model RFE data Train dengan 100 fitur dan 100 trees:1.0\n",
      "Recall model RFE data Test dengan 100 fitur dan 100 trees:0.96\n",
      " \n",
      "total time taken this loop:  1.4043917655944824\n",
      "====Nilai Performa Tertinggi====\n",
      "Nilai akurasi model tertinggi: 0.96\n",
      "Nilai presisi model tertinggi: 0.96\n",
      "Nilai Recall model tertinggi: 0.96\n",
      "CPU times: user 2min 14s, sys: 1.54 s, total: 2min 15s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#tentukan metode scoring yang digunakan\n",
    "scoring_rfe = {'acc': 'accuracy',\n",
    "               'prec_micro': 'precision_micro',\n",
    "               'rec_micro': 'recall_micro'}\n",
    "\n",
    "#tentukan total fitur dan trees yang digunakan dalam proses klasifikasi ini\n",
    "n_feat = range(1, 101)\n",
    "n_trees = [100]\n",
    "\n",
    "for nfeat in n_feat:\n",
    "    for ntrees in n_trees:\n",
    "        print(\"==================================================\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #ambil n fitur input hasil seleksi fitur MI\n",
    "        x_train_selected = x_train[features_mi.nlargest(100,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "        x_test_selected = x_test[features_mi.nlargest(100,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "\n",
    "        #Create a Gaussian Classifier\n",
    "        clf_rfe = RandomForestClassifier(n_estimators=ntrees)\n",
    "        \n",
    "        #Train the model using the training sets\n",
    "        clf_rfe.fit(x_train_selected, y_train)\n",
    "        y_pred_rfe=clf_rfe.predict(x_test_selected)\n",
    "        \n",
    "        #hitung score model dari data train\n",
    "        scores_rfe = cross_validate(clf_rfe, x_train_selected, y_train, scoring=scoring_rfe, cv=cv, return_train_score=True)\n",
    "        \n",
    "        print(\"Akurasi model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_acc'].mean(), 2)))\n",
    "        print(\"Akurasi model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.accuracy_score(y_test, y_pred_rfe), 2)))\n",
    "        print(\"Precision model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_prec_micro'].mean(), 2)))\n",
    "        print(\"Precision model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.precision_score(y_test, y_pred_rfe, average='micro'), 2)))\n",
    "        print(\"Recall model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_rec_micro'].mean(), 2)))\n",
    "        print(\"Recall model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.recall_score(y_test, y_pred_rfe, average='micro'), 2)))\n",
    "        print(\" \")\n",
    "        \n",
    "        max_acc = 0\n",
    "        max_prec = 0\n",
    "        max_rec = 0\n",
    "        \n",
    "        if(round(metrics.accuracy_score(y_test, y_pred_rfe), 2) > max_acc):\n",
    "            max_acc = round(metrics.accuracy_score(y_test, y_pred_rfe), 2)\n",
    "        \n",
    "        if(round(metrics.precision_score(y_test, y_pred_rfe, average='micro'), 2) > max_prec):\n",
    "            max_prec = round(metrics.precision_score(y_test, y_pred_rfe, average='micro'), 2)\n",
    "            \n",
    "        if(round(metrics.recall_score(y_test, y_pred_rfe, average='micro'), 2) > max_rec):\n",
    "            max_rec = round(metrics.recall_score(y_test, y_pred_rfe, average='micro'), 2)\n",
    "            \n",
    "        end_time = time.time()\n",
    "        print(\"Total waktu: \", end_time - start_time)\n",
    "\n",
    "print(\"====Nilai Performa Tertinggi====\")\n",
    "print(\"Nilai akurasi model tertinggi: \" + str(max_acc))\n",
    "print(\"Nilai presisi model tertinggi: \" + str(max_prec))\n",
    "print(\"Nilai Recall model tertinggi: \" + str(max_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Akurasi model RFE data Train dengan 1 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 1 fitur dan 150 trees:0.82\n",
      "Precision model RFE data Train dengan 1 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 1 fitur dan 150 trees:0.82\n",
      "Recall model RFE data Train dengan 1 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 1 fitur dan 150 trees:0.82\n",
      " \n",
      "Total waktu:  2.136610746383667\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 2 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 2 fitur dan 150 trees:0.82\n",
      "Precision model RFE data Train dengan 2 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 2 fitur dan 150 trees:0.82\n",
      "Recall model RFE data Train dengan 2 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 2 fitur dan 150 trees:0.82\n",
      " \n",
      "Total waktu:  1.8688099384307861\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 3 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 3 fitur dan 150 trees:0.84\n",
      "Precision model RFE data Train dengan 3 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 3 fitur dan 150 trees:0.84\n",
      "Recall model RFE data Train dengan 3 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 3 fitur dan 150 trees:0.84\n",
      " \n",
      "Total waktu:  1.8655309677124023\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 4 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 4 fitur dan 150 trees:0.84\n",
      "Precision model RFE data Train dengan 4 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 4 fitur dan 150 trees:0.84\n",
      "Recall model RFE data Train dengan 4 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 4 fitur dan 150 trees:0.84\n",
      " \n",
      "Total waktu:  1.8708844184875488\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 5 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 5 fitur dan 150 trees:0.84\n",
      "Precision model RFE data Train dengan 5 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 5 fitur dan 150 trees:0.84\n",
      "Recall model RFE data Train dengan 5 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 5 fitur dan 150 trees:0.84\n",
      " \n",
      "Total waktu:  1.8978116512298584\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 6 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 6 fitur dan 150 trees:0.84\n",
      "Precision model RFE data Train dengan 6 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 6 fitur dan 150 trees:0.84\n",
      "Recall model RFE data Train dengan 6 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 6 fitur dan 150 trees:0.84\n",
      " \n",
      "Total waktu:  1.8891000747680664\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 7 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 7 fitur dan 150 trees:0.84\n",
      "Precision model RFE data Train dengan 7 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 7 fitur dan 150 trees:0.84\n",
      "Recall model RFE data Train dengan 7 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 7 fitur dan 150 trees:0.84\n",
      " \n",
      "Total waktu:  1.8834433555603027\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 8 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 8 fitur dan 150 trees:0.84\n",
      "Precision model RFE data Train dengan 8 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 8 fitur dan 150 trees:0.84\n",
      "Recall model RFE data Train dengan 8 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 8 fitur dan 150 trees:0.84\n",
      " \n",
      "Total waktu:  1.8651514053344727\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 9 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 9 fitur dan 150 trees:0.84\n",
      "Precision model RFE data Train dengan 9 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 9 fitur dan 150 trees:0.84\n",
      "Recall model RFE data Train dengan 9 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 9 fitur dan 150 trees:0.84\n",
      " \n",
      "Total waktu:  1.893678903579712\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 10 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 10 fitur dan 150 trees:0.84\n",
      "Precision model RFE data Train dengan 10 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 10 fitur dan 150 trees:0.84\n",
      "Recall model RFE data Train dengan 10 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 10 fitur dan 150 trees:0.84\n",
      " \n",
      "Total waktu:  1.914231538772583\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 11 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 11 fitur dan 150 trees:0.84\n",
      "Precision model RFE data Train dengan 11 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 11 fitur dan 150 trees:0.84\n",
      "Recall model RFE data Train dengan 11 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 11 fitur dan 150 trees:0.84\n",
      " \n",
      "Total waktu:  1.9314725399017334\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 12 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 12 fitur dan 150 trees:0.84\n",
      "Precision model RFE data Train dengan 12 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 12 fitur dan 150 trees:0.84\n",
      "Recall model RFE data Train dengan 12 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 12 fitur dan 150 trees:0.84\n",
      " \n",
      "Total waktu:  2.013294219970703\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 13 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 13 fitur dan 150 trees:0.84\n",
      "Precision model RFE data Train dengan 13 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 13 fitur dan 150 trees:0.84\n",
      "Recall model RFE data Train dengan 13 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 13 fitur dan 150 trees:0.84\n",
      " \n",
      "Total waktu:  1.9641375541687012\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 14 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 14 fitur dan 150 trees:0.88\n",
      "Precision model RFE data Train dengan 14 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 14 fitur dan 150 trees:0.88\n",
      "Recall model RFE data Train dengan 14 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 14 fitur dan 150 trees:0.88\n",
      " \n",
      "Total waktu:  2.0254263877868652\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 15 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 15 fitur dan 150 trees:0.91\n",
      "Precision model RFE data Train dengan 15 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 15 fitur dan 150 trees:0.91\n",
      "Recall model RFE data Train dengan 15 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 15 fitur dan 150 trees:0.91\n",
      " \n",
      "Total waktu:  1.9458320140838623\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 16 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 16 fitur dan 150 trees:0.91\n",
      "Precision model RFE data Train dengan 16 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 16 fitur dan 150 trees:0.91\n",
      "Recall model RFE data Train dengan 16 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 16 fitur dan 150 trees:0.91\n",
      " \n",
      "Total waktu:  1.9605066776275635\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 17 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 17 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 17 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 17 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 17 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 17 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.001417398452759\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 18 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 18 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 18 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 18 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 18 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 18 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0677671432495117\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 19 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 19 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 19 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 19 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 19 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 19 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  1.9458847045898438\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 20 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 20 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 20 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 20 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 20 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 20 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  1.9476346969604492\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 21 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 21 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 21 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 21 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 21 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 21 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  1.9404618740081787\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 22 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 22 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 22 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 22 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 22 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 22 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  1.946704626083374\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 23 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 23 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 23 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 23 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 23 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 23 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  1.9272875785827637\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 24 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 24 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 24 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 24 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 24 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 24 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  1.9756412506103516\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 25 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 25 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 25 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 25 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 25 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 25 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  1.984426498413086\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 26 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 26 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 26 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 26 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 26 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 26 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  1.9974703788757324\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 27 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 27 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 27 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 27 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 27 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 27 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  1.9907987117767334\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 28 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 28 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 28 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 28 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 28 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 28 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0114905834198\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 29 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 29 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 29 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 29 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 29 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 29 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  1.9635982513427734\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 30 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 30 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 30 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 30 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 30 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 30 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  1.991295337677002\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 31 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 31 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 31 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 31 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 31 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 31 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0591824054718018\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 32 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 32 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 32 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 32 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 32 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 32 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0063014030456543\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 33 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 33 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 33 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 33 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 33 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 33 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.014930009841919\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 34 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 34 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 34 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 34 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 34 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 34 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  1.9889018535614014\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 35 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 35 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 35 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 35 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 35 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 35 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.009958505630493\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 36 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 36 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 36 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 36 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 36 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 36 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0317652225494385\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 37 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 37 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 37 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 37 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 37 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 37 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0341920852661133\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 38 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 38 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 38 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 38 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 38 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 38 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.030237913131714\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 39 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 39 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 39 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 39 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 39 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 39 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.063023328781128\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 40 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 40 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 40 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 40 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 40 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 40 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0178775787353516\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 41 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 41 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 41 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 41 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 41 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 41 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0277576446533203\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 42 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 42 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 42 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 42 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 42 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 42 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.02121639251709\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 43 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 43 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 43 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 43 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 43 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 43 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.036428928375244\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 44 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 44 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 44 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 44 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 44 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 44 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.004652738571167\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 45 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 45 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 45 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 45 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 45 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 45 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0103375911712646\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 46 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 46 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 46 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 46 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 46 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 46 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0275070667266846\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 47 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 47 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 47 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 47 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 47 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 47 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.1071248054504395\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 48 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 48 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 48 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 48 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 48 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 48 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0183324813842773\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 49 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 49 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 49 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 49 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 49 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 49 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0233051776885986\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 50 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 50 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 50 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 50 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 50 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 50 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0924899578094482\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 51 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 51 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 51 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 51 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 51 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 51 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.063533067703247\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 52 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 52 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 52 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 52 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 52 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 52 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0479736328125\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 53 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 53 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 53 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 53 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 53 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 53 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.062237024307251\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 54 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 54 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 54 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 54 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 54 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 54 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.038094997406006\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 55 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 55 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 55 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 55 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 55 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 55 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.07586407661438\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 56 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 56 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 56 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 56 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 56 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 56 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0450968742370605\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 57 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 57 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 57 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 57 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 57 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 57 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0387051105499268\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 58 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 58 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 58 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 58 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 58 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 58 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.063711166381836\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 59 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 59 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 59 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 59 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 59 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 59 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0379836559295654\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 60 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 60 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 60 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 60 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 60 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 60 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.064518928527832\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 61 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 61 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 61 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 61 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 61 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 61 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.047420024871826\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 62 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 62 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 62 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 62 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 62 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 62 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.083407163619995\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 63 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 63 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 63 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 63 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 63 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 63 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.033578395843506\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 64 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 64 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 64 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 64 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 64 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 64 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.1418402194976807\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 65 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 65 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 65 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 65 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 65 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 65 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0694217681884766\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 66 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 66 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 66 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 66 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 66 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 66 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0607502460479736\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 67 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 67 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 67 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 67 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 67 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 67 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.094424247741699\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 68 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 68 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 68 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 68 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 68 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 68 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.0760247707366943\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 69 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 69 fitur dan 150 trees:0.96\n",
      "Precision model RFE data Train dengan 69 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 69 fitur dan 150 trees:0.96\n",
      "Recall model RFE data Train dengan 69 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 69 fitur dan 150 trees:0.96\n",
      " \n",
      "Total waktu:  2.0586585998535156\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 70 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 70 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 70 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 70 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 70 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 70 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.0411484241485596\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 71 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 71 fitur dan 150 trees:0.96\n",
      "Precision model RFE data Train dengan 71 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 71 fitur dan 150 trees:0.96\n",
      "Recall model RFE data Train dengan 71 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 71 fitur dan 150 trees:0.96\n",
      " \n",
      "Total waktu:  2.0867512226104736\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 72 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 72 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 72 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 72 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 72 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 72 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.0926389694213867\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 73 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 73 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 73 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 73 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 73 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 73 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.025055170059204\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 74 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 74 fitur dan 150 trees:0.96\n",
      "Precision model RFE data Train dengan 74 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 74 fitur dan 150 trees:0.96\n",
      "Recall model RFE data Train dengan 74 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 74 fitur dan 150 trees:0.96\n",
      " \n",
      "Total waktu:  2.0804507732391357\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 75 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 75 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 75 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 75 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 75 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 75 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.094782829284668\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 76 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 76 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 76 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 76 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 76 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 76 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.0687596797943115\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 77 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 77 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 77 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 77 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 77 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 77 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.035118818283081\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 78 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 78 fitur dan 150 trees:0.93\n",
      "Precision model RFE data Train dengan 78 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 78 fitur dan 150 trees:0.93\n",
      "Recall model RFE data Train dengan 78 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 78 fitur dan 150 trees:0.93\n",
      " \n",
      "Total waktu:  2.048818826675415\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 79 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 79 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 79 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 79 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 79 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 79 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.097919464111328\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 80 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 80 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 80 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 80 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 80 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 80 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.1553235054016113\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 81 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 81 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 81 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 81 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 81 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 81 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.1074841022491455\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 82 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 82 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 82 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 82 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 82 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 82 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.0730557441711426\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 83 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 83 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 83 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 83 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 83 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 83 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.101076126098633\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 84 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 84 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 84 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 84 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 84 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 84 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.095517158508301\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 85 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 85 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 85 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 85 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 85 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 85 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.088505268096924\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 86 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 86 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 86 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 86 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 86 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 86 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.1109633445739746\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 87 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 87 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 87 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 87 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 87 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 87 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.0934712886810303\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 88 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 88 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 88 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 88 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 88 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 88 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.118635654449463\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 89 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 89 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 89 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 89 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 89 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 89 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.105442762374878\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 90 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 90 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 90 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 90 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 90 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 90 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.114664316177368\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 91 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 91 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 91 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 91 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 91 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 91 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.1245505809783936\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 92 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 92 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 92 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 92 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 92 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 92 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.085028648376465\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 93 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 93 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 93 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 93 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 93 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 93 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.086108446121216\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 94 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 94 fitur dan 150 trees:0.96\n",
      "Precision model RFE data Train dengan 94 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 94 fitur dan 150 trees:0.96\n",
      "Recall model RFE data Train dengan 94 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 94 fitur dan 150 trees:0.96\n",
      " \n",
      "Total waktu:  2.0959038734436035\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 95 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 95 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 95 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 95 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 95 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 95 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.111145257949829\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 96 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 96 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 96 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 96 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 96 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 96 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.163320302963257\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 97 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 97 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 97 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 97 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 97 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 97 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.090702533721924\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 98 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 98 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 98 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 98 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 98 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 98 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.1124825477600098\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 99 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 99 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 99 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 99 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 99 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 99 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.2251458168029785\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 100 fitur dan 150 trees:1.0\n",
      "Akurasi model RFE data Test dengan 100 fitur dan 150 trees:0.95\n",
      "Precision model RFE data Train dengan 100 fitur dan 150 trees:1.0\n",
      "Precision model RFE data Test dengan 100 fitur dan 150 trees:0.95\n",
      "Recall model RFE data Train dengan 100 fitur dan 150 trees:1.0\n",
      "Recall model RFE data Test dengan 100 fitur dan 150 trees:0.95\n",
      " \n",
      "Total waktu:  2.1758337020874023\n",
      "====Nilai Performa Tertinggi====\n",
      "Nilai akurasi model tertinggi: 0.95\n",
      "Nilai presisi model tertinggi: 0.95\n",
      "Nilai Recall model tertinggi: 0.95\n",
      "CPU times: user 3min 22s, sys: 1.65 s, total: 3min 24s\n",
      "Wall time: 3min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#tentukan metode scoring yang digunakan\n",
    "scoring_rfe = {'acc': 'accuracy',\n",
    "               'prec_micro': 'precision_micro',\n",
    "               'rec_micro': 'recall_micro'}\n",
    "\n",
    "#tentukan total fitur dan trees yang digunakan dalam proses klasifikasi ini\n",
    "n_feat = range(1, 101)\n",
    "n_trees = [150]\n",
    "\n",
    "for nfeat in n_feat:\n",
    "    for ntrees in n_trees:\n",
    "        print(\"==================================================\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #ambil n fitur input hasil seleksi fitur MI\n",
    "        x_train_selected = x_train[features_mi.nlargest(100,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "        x_test_selected = x_test[features_mi.nlargest(100,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "\n",
    "        #Create a Gaussian Classifier\n",
    "        clf_rfe = RandomForestClassifier(n_estimators=ntrees)\n",
    "        \n",
    "        #Train the model using the training sets\n",
    "        clf_rfe.fit(x_train_selected, y_train)\n",
    "        y_pred_rfe=clf_rfe.predict(x_test_selected)\n",
    "        \n",
    "        #hitung score model dari data train\n",
    "        scores_rfe = cross_validate(clf_rfe, x_train_selected, y_train, scoring=scoring_rfe, cv=cv, return_train_score=True)\n",
    "        \n",
    "        print(\"Akurasi model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_acc'].mean(), 2)))\n",
    "        print(\"Akurasi model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.accuracy_score(y_test, y_pred_rfe), 2)))\n",
    "        print(\"Precision model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_prec_micro'].mean(), 2)))\n",
    "        print(\"Precision model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.precision_score(y_test, y_pred_rfe, average='micro'), 2)))\n",
    "        print(\"Recall model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_rec_micro'].mean(), 2)))\n",
    "        print(\"Recall model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.recall_score(y_test, y_pred_rfe, average='micro'), 2)))\n",
    "        print(\" \")\n",
    "        \n",
    "        max_acc = 0\n",
    "        max_prec = 0\n",
    "        max_rec = 0\n",
    "        \n",
    "        if(round(metrics.accuracy_score(y_test, y_pred_rfe), 2) > max_acc):\n",
    "            max_acc = round(metrics.accuracy_score(y_test, y_pred_rfe), 2)\n",
    "        \n",
    "        if(round(metrics.precision_score(y_test, y_pred_rfe, average='micro'), 2) > max_prec):\n",
    "            max_prec = round(metrics.precision_score(y_test, y_pred_rfe, average='micro'), 2)\n",
    "            \n",
    "        if(round(metrics.recall_score(y_test, y_pred_rfe, average='micro'), 2) > max_rec):\n",
    "            max_rec = round(metrics.recall_score(y_test, y_pred_rfe, average='micro'), 2)\n",
    "            \n",
    "        end_time = time.time()\n",
    "        print(\"Total waktu: \", end_time - start_time)\n",
    "\n",
    "print(\"====Nilai Performa Tertinggi====\")\n",
    "print(\"Nilai akurasi model tertinggi: \" + str(max_acc))\n",
    "print(\"Nilai presisi model tertinggi: \" + str(max_prec))\n",
    "print(\"Nilai Recall model tertinggi: \" + str(max_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Akurasi model RFE data Train dengan 1 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 1 fitur dan 200 trees:0.82\n",
      "Precision model RFE data Train dengan 1 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 1 fitur dan 200 trees:0.82\n",
      "Recall model RFE data Train dengan 1 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 1 fitur dan 200 trees:0.82\n",
      " \n",
      "Total waktu:  2.538083076477051\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 2 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 2 fitur dan 200 trees:0.82\n",
      "Precision model RFE data Train dengan 2 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 2 fitur dan 200 trees:0.82\n",
      "Recall model RFE data Train dengan 2 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 2 fitur dan 200 trees:0.82\n",
      " \n",
      "Total waktu:  2.4159770011901855\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 3 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 3 fitur dan 200 trees:0.84\n",
      "Precision model RFE data Train dengan 3 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 3 fitur dan 200 trees:0.84\n",
      "Recall model RFE data Train dengan 3 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 3 fitur dan 200 trees:0.84\n",
      " \n",
      "Total waktu:  2.4431025981903076\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 4 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 4 fitur dan 200 trees:0.84\n",
      "Precision model RFE data Train dengan 4 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 4 fitur dan 200 trees:0.84\n",
      "Recall model RFE data Train dengan 4 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 4 fitur dan 200 trees:0.84\n",
      " \n",
      "Total waktu:  2.5195422172546387\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 5 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 5 fitur dan 200 trees:0.84\n",
      "Precision model RFE data Train dengan 5 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 5 fitur dan 200 trees:0.84\n",
      "Recall model RFE data Train dengan 5 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 5 fitur dan 200 trees:0.84\n",
      " \n",
      "Total waktu:  2.5140042304992676\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 6 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 6 fitur dan 200 trees:0.84\n",
      "Precision model RFE data Train dengan 6 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 6 fitur dan 200 trees:0.84\n",
      "Recall model RFE data Train dengan 6 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 6 fitur dan 200 trees:0.84\n",
      " \n",
      "Total waktu:  2.5306570529937744\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 7 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 7 fitur dan 200 trees:0.84\n",
      "Precision model RFE data Train dengan 7 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 7 fitur dan 200 trees:0.84\n",
      "Recall model RFE data Train dengan 7 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 7 fitur dan 200 trees:0.84\n",
      " \n",
      "Total waktu:  2.486301898956299\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 8 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 8 fitur dan 200 trees:0.84\n",
      "Precision model RFE data Train dengan 8 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 8 fitur dan 200 trees:0.84\n",
      "Recall model RFE data Train dengan 8 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 8 fitur dan 200 trees:0.84\n",
      " \n",
      "Total waktu:  2.585958242416382\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 9 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 9 fitur dan 200 trees:0.84\n",
      "Precision model RFE data Train dengan 9 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 9 fitur dan 200 trees:0.84\n",
      "Recall model RFE data Train dengan 9 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 9 fitur dan 200 trees:0.84\n",
      " \n",
      "Total waktu:  2.5533974170684814\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 10 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 10 fitur dan 200 trees:0.84\n",
      "Precision model RFE data Train dengan 10 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 10 fitur dan 200 trees:0.84\n",
      "Recall model RFE data Train dengan 10 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 10 fitur dan 200 trees:0.84\n",
      " \n",
      "Total waktu:  2.5485916137695312\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 11 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 11 fitur dan 200 trees:0.84\n",
      "Precision model RFE data Train dengan 11 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 11 fitur dan 200 trees:0.84\n",
      "Recall model RFE data Train dengan 11 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 11 fitur dan 200 trees:0.84\n",
      " \n",
      "Total waktu:  2.6286733150482178\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 12 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 12 fitur dan 200 trees:0.84\n",
      "Precision model RFE data Train dengan 12 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 12 fitur dan 200 trees:0.84\n",
      "Recall model RFE data Train dengan 12 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 12 fitur dan 200 trees:0.84\n",
      " \n",
      "Total waktu:  2.5932815074920654\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 13 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 13 fitur dan 200 trees:0.84\n",
      "Precision model RFE data Train dengan 13 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 13 fitur dan 200 trees:0.84\n",
      "Recall model RFE data Train dengan 13 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 13 fitur dan 200 trees:0.84\n",
      " \n",
      "Total waktu:  2.5654332637786865\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 14 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 14 fitur dan 200 trees:0.88\n",
      "Precision model RFE data Train dengan 14 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 14 fitur dan 200 trees:0.88\n",
      "Recall model RFE data Train dengan 14 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 14 fitur dan 200 trees:0.88\n",
      " \n",
      "Total waktu:  2.5628397464752197\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 15 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 15 fitur dan 200 trees:0.91\n",
      "Precision model RFE data Train dengan 15 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 15 fitur dan 200 trees:0.91\n",
      "Recall model RFE data Train dengan 15 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 15 fitur dan 200 trees:0.91\n",
      " \n",
      "Total waktu:  2.5859756469726562\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 16 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 16 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 16 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 16 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 16 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 16 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.5862693786621094\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 17 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 17 fitur dan 200 trees:0.91\n",
      "Precision model RFE data Train dengan 17 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 17 fitur dan 200 trees:0.91\n",
      "Recall model RFE data Train dengan 17 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 17 fitur dan 200 trees:0.91\n",
      " \n",
      "Total waktu:  2.609013319015503\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 18 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 18 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 18 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 18 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 18 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 18 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.5632877349853516\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 19 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 19 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 19 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 19 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 19 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 19 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.588836908340454\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 20 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 20 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 20 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 20 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 20 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 20 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.6040196418762207\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 21 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 21 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 21 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 21 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 21 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 21 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.5817031860351562\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 22 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 22 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 22 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 22 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 22 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 22 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.57011342048645\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 23 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 23 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 23 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 23 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 23 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 23 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.5817573070526123\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 24 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 24 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 24 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 24 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 24 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 24 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.588728904724121\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 25 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 25 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 25 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 25 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 25 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 25 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.7228803634643555\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 26 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 26 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 26 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 26 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 26 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 26 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.6377124786376953\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 27 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 27 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 27 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 27 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 27 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 27 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.643751382827759\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 28 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 28 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 28 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 28 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 28 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 28 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.7072367668151855\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 29 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 29 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 29 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 29 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 29 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 29 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.6169021129608154\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 30 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 30 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 30 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 30 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 30 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 30 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.6398353576660156\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 31 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 31 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 31 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 31 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 31 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 31 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.64931058883667\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 32 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 32 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 32 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 32 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 32 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 32 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.637748956680298\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 33 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 33 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 33 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 33 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 33 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 33 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.671614170074463\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 34 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 34 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 34 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 34 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 34 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 34 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.628295660018921\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 35 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 35 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 35 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 35 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 35 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 35 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.6298158168792725\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 36 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 36 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 36 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 36 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 36 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 36 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.714364528656006\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 37 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 37 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 37 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 37 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 37 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 37 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.6679811477661133\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 38 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 38 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 38 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 38 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 38 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 38 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.6552281379699707\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 39 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 39 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 39 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 39 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 39 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 39 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.6926121711730957\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 40 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 40 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 40 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 40 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 40 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 40 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.666170120239258\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 41 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 41 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 41 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 41 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 41 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 41 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.672929286956787\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 42 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 42 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 42 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 42 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 42 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 42 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.678570032119751\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 43 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 43 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 43 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 43 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 43 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 43 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.6587865352630615\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 44 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 44 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 44 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 44 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 44 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 44 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.7323575019836426\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 45 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 45 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 45 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 45 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 45 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 45 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.6318976879119873\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 46 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 46 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 46 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 46 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 46 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 46 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.6585404872894287\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 47 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 47 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 47 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 47 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 47 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 47 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.6591107845306396\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 48 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 48 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 48 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 48 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 48 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 48 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.698194742202759\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 49 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 49 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 49 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 49 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 49 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 49 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.7224433422088623\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 50 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 50 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 50 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 50 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 50 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 50 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.7236530780792236\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 51 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 51 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 51 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 51 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 51 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 51 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.7952234745025635\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 52 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 52 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 52 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 52 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 52 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 52 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.722276210784912\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 53 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 53 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 53 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 53 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 53 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 53 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.704972982406616\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 54 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 54 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 54 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 54 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 54 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 54 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.704446792602539\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 55 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 55 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 55 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 55 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 55 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 55 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.719602584838867\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 56 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 56 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 56 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 56 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 56 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 56 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.7221615314483643\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 57 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 57 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 57 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 57 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 57 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 57 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.7209174633026123\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 58 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 58 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 58 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 58 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 58 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 58 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.633568286895752\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 59 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 59 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 59 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 59 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 59 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 59 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.7150909900665283\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 60 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 60 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 60 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 60 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 60 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 60 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.7250003814697266\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 61 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 61 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 61 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 61 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 61 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 61 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.7024106979370117\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 62 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 62 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 62 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 62 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 62 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 62 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.7114782333374023\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 63 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 63 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 63 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 63 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 63 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 63 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.738409996032715\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 64 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 64 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 64 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 64 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 64 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 64 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.757828950881958\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 65 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 65 fitur dan 200 trees:0.93\n",
      "Precision model RFE data Train dengan 65 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 65 fitur dan 200 trees:0.93\n",
      "Recall model RFE data Train dengan 65 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 65 fitur dan 200 trees:0.93\n",
      " \n",
      "Total waktu:  2.7659075260162354\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 66 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 66 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 66 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 66 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 66 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 66 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.774747371673584\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 67 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 67 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 67 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 67 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 67 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 67 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.761953592300415\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 68 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 68 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 68 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 68 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 68 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 68 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.758472204208374\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 69 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 69 fitur dan 200 trees:0.96\n",
      "Precision model RFE data Train dengan 69 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 69 fitur dan 200 trees:0.96\n",
      "Recall model RFE data Train dengan 69 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 69 fitur dan 200 trees:0.96\n",
      " \n",
      "Total waktu:  2.7141847610473633\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 70 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 70 fitur dan 200 trees:0.96\n",
      "Precision model RFE data Train dengan 70 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 70 fitur dan 200 trees:0.96\n",
      "Recall model RFE data Train dengan 70 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 70 fitur dan 200 trees:0.96\n",
      " \n",
      "Total waktu:  2.727193593978882\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 71 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 71 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 71 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 71 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 71 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 71 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.750326633453369\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 72 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 72 fitur dan 200 trees:0.96\n",
      "Precision model RFE data Train dengan 72 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 72 fitur dan 200 trees:0.96\n",
      "Recall model RFE data Train dengan 72 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 72 fitur dan 200 trees:0.96\n",
      " \n",
      "Total waktu:  2.7889199256896973\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 73 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 73 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 73 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 73 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 73 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 73 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.8112399578094482\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 74 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 74 fitur dan 200 trees:0.96\n",
      "Precision model RFE data Train dengan 74 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 74 fitur dan 200 trees:0.96\n",
      "Recall model RFE data Train dengan 74 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 74 fitur dan 200 trees:0.96\n",
      " \n",
      "Total waktu:  2.715848684310913\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 75 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 75 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 75 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 75 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 75 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 75 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.7123184204101562\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 76 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 76 fitur dan 200 trees:0.96\n",
      "Precision model RFE data Train dengan 76 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 76 fitur dan 200 trees:0.96\n",
      "Recall model RFE data Train dengan 76 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 76 fitur dan 200 trees:0.96\n",
      " \n",
      "Total waktu:  2.7498199939727783\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 77 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 77 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 77 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 77 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 77 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 77 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.769507884979248\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 78 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 78 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 78 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 78 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 78 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 78 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.7287745475769043\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 79 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 79 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 79 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 79 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 79 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 79 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.768249273300171\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 80 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 80 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 80 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 80 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 80 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 80 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.7314980030059814\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 81 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 81 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 81 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 81 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 81 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 81 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.7820730209350586\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 82 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 82 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 82 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 82 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 82 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 82 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.797175168991089\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 83 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 83 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 83 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 83 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 83 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 83 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.768000841140747\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 84 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 84 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 84 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 84 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 84 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 84 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.765920877456665\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 85 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 85 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 85 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 85 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 85 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 85 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.7802188396453857\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 86 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 86 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 86 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 86 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 86 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 86 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.752568483352661\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 87 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 87 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 87 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 87 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 87 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 87 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.7876596450805664\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 88 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 88 fitur dan 200 trees:0.96\n",
      "Precision model RFE data Train dengan 88 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 88 fitur dan 200 trees:0.96\n",
      "Recall model RFE data Train dengan 88 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 88 fitur dan 200 trees:0.96\n",
      " \n",
      "Total waktu:  2.8017683029174805\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 89 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 89 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 89 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 89 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 89 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 89 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.7967984676361084\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 90 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 90 fitur dan 200 trees:0.96\n",
      "Precision model RFE data Train dengan 90 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 90 fitur dan 200 trees:0.96\n",
      "Recall model RFE data Train dengan 90 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 90 fitur dan 200 trees:0.96\n",
      " \n",
      "Total waktu:  2.793750762939453\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi model RFE data Train dengan 91 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 91 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 91 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 91 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 91 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 91 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.7816498279571533\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 92 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 92 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 92 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 92 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 92 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 92 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.8505048751831055\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 93 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 93 fitur dan 200 trees:0.96\n",
      "Precision model RFE data Train dengan 93 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 93 fitur dan 200 trees:0.96\n",
      "Recall model RFE data Train dengan 93 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 93 fitur dan 200 trees:0.96\n",
      " \n",
      "Total waktu:  2.802985668182373\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 94 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 94 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 94 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 94 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 94 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 94 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.776228904724121\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 95 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 95 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 95 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 95 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 95 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 95 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.882293462753296\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 96 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 96 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 96 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 96 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 96 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 96 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.836559772491455\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 97 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 97 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 97 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 97 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 97 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 97 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.8096022605895996\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 98 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 98 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 98 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 98 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 98 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 98 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.8316633701324463\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 99 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 99 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 99 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 99 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 99 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 99 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.7926106452941895\n",
      "==================================================\n",
      "Akurasi model RFE data Train dengan 100 fitur dan 200 trees:1.0\n",
      "Akurasi model RFE data Test dengan 100 fitur dan 200 trees:0.95\n",
      "Precision model RFE data Train dengan 100 fitur dan 200 trees:1.0\n",
      "Precision model RFE data Test dengan 100 fitur dan 200 trees:0.95\n",
      "Recall model RFE data Train dengan 100 fitur dan 200 trees:1.0\n",
      "Recall model RFE data Test dengan 100 fitur dan 200 trees:0.95\n",
      " \n",
      "Total waktu:  2.827913284301758\n",
      "====Nilai Performa Tertinggi====\n",
      "Nilai akurasi model tertinggi: 0.95\n",
      "Nilai presisi model tertinggi: 0.95\n",
      "Nilai Recall model tertinggi: 0.95\n",
      "CPU times: user 4min 26s, sys: 3.06 s, total: 4min 29s\n",
      "Wall time: 4min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#tentukan metode scoring yang digunakan\n",
    "scoring_rfe = {'acc': 'accuracy',\n",
    "               'prec_micro': 'precision_micro',\n",
    "               'rec_micro': 'recall_micro'}\n",
    "\n",
    "#tentukan total fitur dan trees yang digunakan dalam proses klasifikasi ini\n",
    "n_feat = range(1, 101)\n",
    "n_trees = [200]\n",
    "\n",
    "for nfeat in n_feat:\n",
    "    for ntrees in n_trees:\n",
    "        print(\"==================================================\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #ambil n fitur input hasil seleksi fitur MI\n",
    "        x_train_selected = x_train[features_mi.nlargest(100,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "        x_test_selected = x_test[features_mi.nlargest(100,columns=\"Mutual Information Value\").iloc[0:nfeat, 0]]\n",
    "\n",
    "        #Create a Gaussian Classifier\n",
    "        clf_rfe = RandomForestClassifier(n_estimators=ntrees)\n",
    "        \n",
    "        #Train the model using the training sets\n",
    "        clf_rfe.fit(x_train_selected, y_train)\n",
    "        y_pred_rfe=clf_rfe.predict(x_test_selected)\n",
    "        \n",
    "        #hitung score model dari data train\n",
    "        scores_rfe = cross_validate(clf_rfe, x_train_selected, y_train, scoring=scoring_rfe, cv=cv, return_train_score=True)\n",
    "        \n",
    "        print(\"Akurasi model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_acc'].mean(), 2)))\n",
    "        print(\"Akurasi model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.accuracy_score(y_test, y_pred_rfe), 2)))\n",
    "        print(\"Precision model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_prec_micro'].mean(), 2)))\n",
    "        print(\"Precision model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.precision_score(y_test, y_pred_rfe, average='micro'), 2)))\n",
    "        print(\"Recall model RFE data Train dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(scores_rfe['train_rec_micro'].mean(), 2)))\n",
    "        print(\"Recall model RFE data Test dengan \" + str(nfeat) + \" fitur dan \" + str(ntrees) + \" trees:\" \n",
    "              + str(round(metrics.recall_score(y_test, y_pred_rfe, average='micro'), 2)))\n",
    "        print(\" \")\n",
    "        \n",
    "        max_acc = 0\n",
    "        max_prec = 0\n",
    "        max_rec = 0\n",
    "        \n",
    "        if(round(metrics.accuracy_score(y_test, y_pred_rfe), 2) > max_acc):\n",
    "            max_acc = round(metrics.accuracy_score(y_test, y_pred_rfe), 2)\n",
    "        \n",
    "        if(round(metrics.precision_score(y_test, y_pred_rfe, average='micro'), 2) > max_prec):\n",
    "            max_prec = round(metrics.precision_score(y_test, y_pred_rfe, average='micro'), 2)\n",
    "            \n",
    "        if(round(metrics.recall_score(y_test, y_pred_rfe, average='micro'), 2) > max_rec):\n",
    "            max_rec = round(metrics.recall_score(y_test, y_pred_rfe, average='micro'), 2)\n",
    "            \n",
    "        end_time = time.time()\n",
    "        print(\"Total waktu: \", end_time - start_time)\n",
    "\n",
    "print(\"====Nilai Performa Tertinggi====\")\n",
    "print(\"Nilai akurasi model tertinggi: \" + str(max_acc))\n",
    "print(\"Nilai presisi model tertinggi: \" + str(max_prec))\n",
    "print(\"Nilai Recall model tertinggi: \" + str(max_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
